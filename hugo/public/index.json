[
{
	"uri": "https://gardener.cloud/",
	"title": "Gardener",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gardener.cloud/news/002/00/",
	"title": "Gardener Website 2.0",
	"tags": [],
	"description": "",
	"content": "\r.lity-content .news-item {\rbackground: #fff;\rpadding: 30px 20px;\r}\r.news-item .green-power {\rcolor: #0f674e;\rfont-weight: 700;\r}\r.news-item .green-power img {\rwidth: 32px;\rvertical-align: middle;\rmargin-right: 5px;\r}\r@media (min-width: 750px) {\r.news-item .green-power img {\rwidth: 48px;\r} }\r.news-item .title-2 {\rfont-weight: 700;\r}\r\rNew Website, Same Green Power!\rThe Gardener Project just got a brand new website with multiple improvements and we are all very excited!\n\rGo ahead and explore around now and help us spread the word: https://gardener.cloud\rTweet\r\rRead more about changes and plans in the blog post.\n"
},
{
	"uri": "https://gardener.cloud/news/",
	"title": "News",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gardener.cloud/news/003/00/",
	"title": "Garden Linux",
	"tags": [],
	"description": "",
	"content": "\r\rGarden Linux\r"
},
{
	"uri": "https://gardener.cloud/news/001/release/",
	"title": "Gardener 1.3.0 Released",
	"tags": [],
	"description": "",
	"content": "Gardener 1.3.0 Released See what's new and noteworthy presented by the team on the regular community meeting recording.\nBookmark the agenda for upcomming community meetings and stay up to date with what's going on in the project.\nEnjoy!\n"
},
{
	"uri": "https://gardener.cloud/installer/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "gardener-installerWe're sorry but gardener-installer doesn't work properly without JavaScript enabled. Please enable it to continue."
},
{
	"uri": "https://gardener.cloud/readme/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "This folder contains the content of the public facing gardener landing page http://gardener.cloud\n"
},
{
	"uri": "https://gardener.cloud/documentation/050-tutorials/content/howto/access_pod_from_local/",
	"title": "Access a port of a pod locally",
	"tags": [],
	"description": "",
	"content": "Question You deployed an application with a web UI or an internal endpoint in your Kubernetes (K8s) cluster. How can I access this endpoint without an external load balancer (e.g. Ingress)? This tutorial presents two options:\n Using Kubernetes port forward Using Kubernetes apiserver proxy  Please note that the options described here are mostly for quick testing or troubleshooting your application. For enabling access to your application for productive environment, please refer to Access my service\nSolution 1: Using Kubernetes port forward You could use the port forwarding functionality of kubectl to access the pods from your local host without involving a service.\nTo access any pod follow these steps:\n Run kubectl get pods Note down the name of the pod in question as \u0026lt;your-pod-name\u0026gt; Run kubectl port-forward \u0026lt;your-pod-name\u0026gt; \u0026lt;local-port\u0026gt;:\u0026lt;your-app-port\u0026gt; Run a web browser or curl locally and enter the URL http(s)://localhost:\u0026lt;local-port\u0026gt;  In addition, kubectl port-forward allows to use a resource name. such as a deployment or service name, to select a matching pod to port forward. Find more details in the Kubernetes documentation.\nThe main drawback of this approach is that the pod's name will change as soon as it is restarted. Moreover, you need to have a web browser on your client and you need to make sure that the local port is not already used by an application running on your system. Finally, sometimes port forwarding is canceled due to non obvious reasons. This leads to a kind of shaky approach. A more robust approach is to access the application using kube-proxy.\nSolution 2: Using apiserver proxy There are several different proxies used with Kubernetes, the official documentation provides a good overview.\nIn this tutorial we are using apiserver proxy to enable access to services running in Kubernetes without using an Ingress. Different from the first solution, a service is required for this solution .\nUse the following URL to access a service via apiserver proxy. For details about apiserver proxy URLs read Discovering builtin services\nhttps://\u0026lt;cluster-master\u0026gt;/api/v1/namespace/\u0026lt;namespace\u0026gt;/services/\u0026lt;service\u0026gt;:\u0026lt;service-port\u0026gt;/proxy/\u0026lt;service-endpoint\u0026gt;\nExample:\n   cluster-master namespace service yservice-port service-endpoint url to access service     api.testclstr.cpet.k8s.sapcloud.io default nginx-svc 80 / url   api.testclstr.cpet.k8s.sapcloud.io default docker-nodejs-svc 4500 /cpu?baseNumber=4 url    There are applications, which do not yet support relative URLs like Prometheus (as of end of November, 2017). This typically leads to missing JavaScript objects when trying to open the URL in a browser. In this case use the port-forward approach described above.\n"
},
{
	"uri": "https://gardener.cloud/documentation/050-tutorials/content/howto/service-access/",
	"title": "Access service from outside Kubernetes cluster",
	"tags": [],
	"description": "Is there an ingress deployed and how is it configured",
	"content": "TL;DR To expose your application / service for access from outside the cluster, following options exist:\n Kubernetes Service of type LoadBalancer Kubernetes Service of type \u0026lsquo;NodePort\u0026rsquo; + Ingress  This tutorial discusses how to enable access to your application from outside the Kubernetes cluster (sometimes called North-South traffic). For internal communication amongst pods and services (sometimes called East-West traffic) there are many examples, here is one brief example.\nService Types A Service in Kubernetes is an abstraction defining a logical set of Pods and an access policy.\nServices can be exposed in different ways by specifying a type in the service spec, and different types determine accessibility from inside and outside of cluster.\n ClusterIP NodePort LoadBalancer  Type ExternalName is a special case of service and not discussed here.\nType ClusterIP A service of type ClusterIP exposes a service on an internal IP in the cluster, which makes the service only reachable from within the cluster. This is the default value if no type is specified.\napiVersion: apps/v1\rkind: Deployment\rmetadata:\rname: nginx-deployment\rspec:\rselector:\rmatchLabels:\rapp: nginx-app\rreplicas: 1\rtemplate:\rmetadata:\rlabels:\rapp: nginx-app\rspec:\rcontainers:\r- name: nginx\rimage: nginx:1.13.12\rports:\r- containerPort: 80\r---\rapiVersion: v1\r kind: Service\rmetadata:\rlabels:\rapp: nginx-app\rname: nginx-svc\rnamespace: default\rspec:\rtype: ClusterIP # use ClusterIP as type here\r ports:\r- port: 80\rselector:\rapp: nginx-app\rExecute following commands to create deployment and service\nkubectl create -f \u0026lt;Your yaml file name\u0026gt;\rChecking the service status\n$ kubectl get svc nginx-svc\rNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE\rnginx-svc ClusterIP 100.66.125.61 \u0026lt;none\u0026gt; 80/TCP 45m\rAs shown above, the service is assigned with a cluster ip address and port 80 as defined in configuration file. You can test the service like this:\n# list all existing pods in cluster\r$ kubectl get pods\rNAME READY STATUS RESTARTS AGE\rdocker-nodejs-app-76b77494-vwv4d 1/1 Running 0 11d\rnginx-deployment-74d949bf69-nvdzs 1/1 Running 0 1h\rprivileged-pod 1/1 Running 0 11d\r# test service from within the cluster on the same pod\r$ kubectl exec -it nginx-deployment-74d949bf69-nvdzs curl 100.66.125.61:80\r% Total % Received % Xferd Average Speed Time Time Time Current\rDload Upload Total Spent Left Speed\r100 612 100 612 0 0 1006k 0 --:--:-- --:--:-- --:--:-- 597k\r\u0026lt;!DOCTYPE html\u0026gt;\r\u0026lt;html\u0026gt;\r\u0026lt;head\u0026gt;\r\u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt;\r\u0026lt;style\u0026gt;\rbody {\rwidth: 35em;\rmargin: 0 auto;\rfont-family: Tahoma, Verdana, Arial, sans-serif;\r}\r\u0026lt;/style\u0026gt;\r\u0026lt;/head\u0026gt;\r\u0026lt;body\u0026gt;\r\u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt;\r\u0026lt;p\u0026gt;If you see this page, the nginx web server is successfully installed and\rworking. Further configuration is required.\u0026lt;/p\u0026gt;\r...\r  Tip\n The service is also accessible from any other container (even from different pods) within the same cluster, e.g. kubectl -it exec \u0026lt;another POD_NAME\u0026gt; curl \u0026lt;YourServiceClusterIP:YourPort\u0026gt;. You need to make sure command curl is installed in the container. You can also find out the dns name of the ClusterIP by command kubectl exec -it \u0026lt;POD_NAME\u0026gt; nslookup \u0026lt;ClusterIP\u0026gt;, replace the IP address with the resolved name in your test. The resolved name typically looks like nginx-svc.default.svc.cluster.local where nginx-svc is the name of your service defined in the configuration file.   Type NodePort Follow the previous example, just replace the type with NodePort\n...\rspec:\rtype: NodePort\rports:\r- port: 80\r...\rA service of type NodePort is a ClusterIP service with an additional capability: it is reachable at the IP address of the node as well as at the assigned cluster IP on the services network. The way this is accomplished is pretty straightforward: when Kubernetes creates a NodePort service kube-proxy allocates a port in the range 30000–32767 and opens this port on every node (thus the name “NodePort”). Connections to this port are forwarded to the service’s cluster IP. If we create the service above and run kubectl get svc \u0026lt;your-service\u0026gt;, we can see the NodePort that has been allocated for it.\nNote that in the in following example, in addition to port 80, port 32521 has been opened as well on the node, in contrast to the output of \u0026ldquo;ClusterIP\u0026rdquo; case where only port 80 is opened.\n$ kubectl get svc nginx-svc\rNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE\rnginx-svc NodePort 100.70.105.182 \u0026lt;none\u0026gt; 80:32521/TCP 16m\rTherefore you can access the service from within the cluster in two ways:\n Access via ClusterIP:port  #via ClusterIP\rkubectl exec -it nginx-deployment-74d949bf69-7n6bs curl 100.70.105.182:80\r#via internal name of ClusterIP\rkubectl exec -it nginx-deployment-74d949bf69-7n6bs curl nginx-svc.default.svc.cluster.local:80\r Access via NodeIP:NodePort  # First find out the Node IP address\r$ kubectl describe node\rName: ip-10-250-20-203.eu-central-1.compute.internal\rRoles: node\rAddresses:\rInternalIP: 10.250.20.203\rInternalDNS: ip-10-250-20-203.eu-central-1.compute.internal\rHostname: ip-10-250-20-203.eu-central-1.compute.internal\r...\r#via NodeIP:NodePort\rkubectl exec -it nginx-deployment-74d949bf69-7n6bs curl 10.250.20.203:32521\r#via internal name of NodeIP\rkubectl exec -it nginx-deployment-74d949bf69-7n6bs curl ip-10-250-20-203.eu-central-1.compute.internal:32521\rType LoadBalancer The LoadBalancer type is the simplest approach, which is created by specifying type as LoadBalancer.\napiVersion: apps/v1\rkind: Deployment\rmetadata:\rname: nginx-deployment\rspec:\rselector:\rmatchLabels:\rapp: nginx-app\rreplicas: 1\rtemplate:\rmetadata:\rlabels:\rapp: nginx-app\rspec:\rcontainers:\r- name: nginx\rimage: nginx:1.13.12\rports:\r- containerPort: 80\r---\rapiVersion: v1\r kind: Service\rmetadata:\rlabels:\rapp: nginx-app\rname: nginx-svc\rnamespace: default\rspec:\rtype: LoadBalancer # use LoadBalancer as type here\r ports:\r- port: 80\rselector:\rapp: nginx-app\rOnce the service is created, it has an external IP address as shown here:\n$ kubectl get services -l app=nginx-app -o wide\rNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR\rnginx-svc LoadBalancer 100.67.182.148 a54a62300696611e88ba00af02406931-1787163476.eu-central-1.elb.amazonaws.com 80:31196/TCP 9m app=nginx-app\rA service of type LoadBalancer combines the capabilities of a NodePort with the ability to setup a complete ingress path.\nHence the service can be accessible from outside the cluster without the need for additional components like an Ingress.\nTo test the external IP run this curl command from your local machine:\n$ curl http://a54a62300696611e88ba00af02406931-1787163476.eu-central-1.elb.amazonaws.com\rStatusCode : 200\rStatusDescription : OK\rContent : \u0026lt;!DOCTYPE html\u0026gt;\r\u0026lt;html\u0026gt;\r\u0026lt;head\u0026gt;\r\u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt;\r\u0026lt;style\u0026gt;\rbody {\rwidth: 35em;\rmargin: 0 auto;\rfont-family: Tahoma, Verdana, Arial, sans-serif;\r}\r\u0026lt;/style\u0026gt;\r\u0026lt;...\rRawContent : HTTP/1.1 200 OK\r...\rObviously the service can also is accessed from within the cluster. You can test this in the same way as described in section NodePort.\nLoadBalancer vs. Ingress As presented in the previous section, only the service type LoadBalancer enables access from outside the cluster. However this approach has its own limitation. You cannot configure a LoadBalancer to terminate HTTPS traffic, virtual hosts or path-based routing. In Kubernetes 1.2 a separate resource called Ingress is introduced for this purpose.\nWhy an Ingress LoadBalancer services are all about extending a service to support external clients. By contrast an Ingress is a a separate resource that configures a LoadBalancer in a more flexible way. The Ingress API supports TLS termination, virtual hosts, and path-based routing. It can easily set up a load balancer to handle multiple backend services. In addition routing traffic is realised in a different way. In the case of the LoadBalancer service, the traffic entering through the external load balancer is forwarded to the kube-proxy that in turn forwards the traffic to the selected pods. In contrast, the Ingress LoadBalancer forwards the traffic straight to the selected pods which is more efficient.\nTypically a service of type LoadBalancer costs at least 40$ per month. This means if your applications needs 10 of them you already pay 400$ per month just for load balancing.\nHow to use the ingress? In the cluster, a nginx-ingress controller has been deployed for you as an LoadBalancer and also registered the DNS record. Depending on how your cluster is defined, the DNS registration is performed under following conventions:\n k8s-hana.ondemand.com  \u0026lt;gardener_cluster_name\u0026gt;.\u0026lt;gardener_project_name\u0026gt;.shoot.canary.k8s-hana.ondemand.com.\nBoth \u0026lt;gardener_cluster_name\u0026gt; and \u0026lt;gardener_project_name\u0026gt; are defined in Gardener which can be determined on Gardener dashboard.\nThis results in the following default DNS endpoints:\n api.\u0026lt;cluster_domain\u0026gt; Kubernetes API *.ingress.\u0026lt;cluster_domain\u0026gt; Internal nginx ingress  Example: Configure an Ingress resource with Service type: NodePort With the configuration below you can reach your service nginx-svc with:\nhttp://test.ingress.\u0026amp;lt;GARDENER-CLUSTER-NAME\u0026amp;gt;.\u0026amp;lt;GARDENER-PROJECT-NAME\u0026amp;gt;.shoot.canary.k8s-hana.ondemand.com\napiVersion: apps/v1\rkind: Deployment\rmetadata:\rname: nginx-deployment\rspec:\rselector:\rmatchLabels:\rapp: nginx-app\rreplicas: 1\rtemplate:\rmetadata:\rlabels:\rapp: nginx-app\rspec:\rcontainers:\r- name: nginx\rimage: nginx:1.13.12\rports:\r- containerPort: 80\r---\rapiVersion: v1\r kind: Service\rmetadata:\rlabels:\rapp: nginx-app\rname: nginx-svc\rnamespace: default\rspec:\rtype: NodePort\rports:\r- port: 80\rselector:\rapp: nginx-app\r---\rapiVersion: networking.k8s.io/v1beta1\r kind: Ingress\rmetadata:\rname: nginxsvc-ingress\rspec:\rrules:\r- host: nginxsvc.ingress.eecluster.cpet.shoot.canary.k8s-hana.ondemand.com\rhttp:\rpaths:\r- backend:\rserviceName: nginx-svc\rservicePort: 80\rShow the newly created ingress and test it :\n$ kubectl get ingress\rNAME HOSTS ADDRESS PORTS AGE\rnginxsvc-ingress nginxsvc.ingress.eecluster.cpet.shoot.canary.k8s-hana.ondemand.com 10.250.20.203 80 29s\r$ curl nginxsvc.ingress.eecluster.cpet.shoot.canary.k8s-hana.ondemand.com\rStatusCode : 200\rStatusDescription : OK\rContent : \u0026lt;!DOCTYPE html\u0026gt;\r\u0026lt;html\u0026gt;\r\u0026lt;head\u0026gt;\r\u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt;\r\u0026lt;style\u0026gt;\rbody {\rwidth: 35em;\rmargin: 0 auto;\rfont-family: Tahoma, Verdana, Arial, sans-serif;\r...\rReference:  Concepts: Kubernetes Service Concepts: Connecting Applications with Services Tutorial: Using a Service to Expose Your App Tutorial: Using Source IP Kubernetes Networking Accessing Kubernetes Pods from Outside of the Cluster  "
},
{
	"uri": "https://gardener.cloud/using-gardener/administrator/",
	"title": "Administrator",
	"tags": [],
	"description": "",
	"content": "Learning Material\rEverything you need to know to operate gardener.\r\r\r\r"
},
{
	"uri": "https://gardener.cloud/adopter/",
	"title": "Adopters",
	"tags": [],
	"description": "",
	"content": "See who is using Gardener\r\r\r\r\r\rThis is a list of adopters of Gardener in production environments that have publicly shared details of their usage.\r\r\rb’nerd uses Gardener as the core technology for its own managed Kubernetes as a Service\rsolution and operates multiple Gardener installations for several cloud hosting service providers.\r\r\r\r\r\r\rSAP uses Gardener to deploy and manage Kubernetes clusters at scale in a uniform way\racross infrastructures (AWS, Azure, GCP, Alicloud, OpenStack). Workloads include databases (SAP Hana),\rBig Data (SAP Data Hub), IoT, AI, and Machine Learning (SAP Leonardo), Serverless and diverse\rbusiness workloads.\r\r\r\rScaleUp Technologies runs Gardener within\rtheir public Openstack Clouds (Hamburg, Berlin, Düsseldorf). Their clients run all kinds of workloads\ron top of Gardener maintained Kubernetes clusters ranging from databases to Software as a Service\rapplications.\r\r\r\rFinanz Informatik Technologie Services GmbH\ruses Gardener to offer k8s as a service for customers in the financial industry in Germany. It is\rbuilt on top of a \"metal as a service\" infrastructure implemented from scratch for k8s workloads\rin mind. The result is k8s on top of bare metal in minutes.\r\r\rIf you’re using Gardener and aren’t on this list, feel free to submit a pull request!\r\r"
},
{
	"uri": "https://gardener.cloud/blog/2018_week_22/",
	"title": "Anti Patterns",
	"tags": [],
	"description": "",
	"content": "\rRunning as root user Whenever possible, do not run containers as root users. One could be tempted to say that Kubernetes Pods and Node are well separated. The host and the container share the same kernel. If the container is compromised, a root user can damage the underlying node. Use RUN groupadd -r anygroup \u0026amp;\u0026amp; useradd -r -g anygroup myuser to create a group and a user in it. Use the USER command to switch to this user.\nStoring data or logs in containers Containers are ideal for stateless applications and should be transient. This means that no data or logs should be stored in the container, as they are lost when the container is closed. If absolutely necessary, you can use persistence volumes instead to persist them outside the containers. However, an ELK stack is preferred for storing and processing log files.\n..read some more on Common Kubernetes Antipattern.\n"
},
{
	"uri": "https://gardener.cloud/api-reference/",
	"title": "API Reference",
	"tags": [],
	"description": "",
	"content": "API Reference Documentation Bellow you can find reference documentation of the API for Gardener.\nGardener Gardener APIs are Kubernetes-style APIs for the life-cycle of a Kubernetes Cluster.\nThose APIs are divided into different groups:\n Core Extensions Settings  "
},
{
	"uri": "https://gardener.cloud/using-gardener/developer/topic/",
	"title": "App Developer",
	"tags": [],
	"description": "",
	"content": "Learning Material\rEverything you need to know about running your software.\r\r\r\r\r\rby Topic\r\r\r \rby Experience Level \r\r\r"
},
{
	"uri": "https://gardener.cloud/documentation/030-architecture/",
	"title": "Architecture",
	"tags": [],
	"description": "",
	"content": "Official Definition - What is Kubernetes?  \u0026ldquo;Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.\u0026rdquo;\n Introduction - Basic Principle The foundation of the Gardener (providing Kubernetes Clusters as a Service) is Kubernetes itself, because Kubernetes is the go-to solution to manage software in the Cloud, even when it's Kubernetes itself (see also OpenStack which is provisioned more and more on top of Kubernetes as well).\nWhile self-hosting, meaning to run Kubernetes components inside Kubernetes, is a popular topic in the community, we apply a special pattern catering to the needs of our cloud platform to provision hundreds or even thousands of clusters. We take a so-called \u0026ldquo;seed\u0026rdquo; cluster and seed the control plane (such as the API server, scheduler, controllers, etcd persistence and others) of an end-user cluster, which we call \u0026ldquo;shoot\u0026rdquo; cluster, as pods into the \u0026ldquo;seed\u0026rdquo; cluster. That means one \u0026ldquo;seed\u0026rdquo; cluster, of which we will have one per IaaS and region, hosts the control planes of multiple \u0026ldquo;shoot\u0026rdquo; clusters. That allows us to avoid dedicated hardware/virtual machines for the \u0026ldquo;shoot\u0026rdquo; cluster control planes. We simply put the control plane into pods/containers and since the \u0026ldquo;seed\u0026rdquo; cluster watches them, they can be deployed with a replica count of 1 and only need to be scaled out when the control plane gets under pressure, but no longer for HA reasons. At the same time, the deployments get simpler (standard Kubernetes deployment) and easier to update (standard Kubernetes rolling update). The actual \u0026ldquo;shoot\u0026rdquo; cluster consists only out of the worker nodes (no control plane) and therefore the users may get full administrative access to their clusters.\nSetting The Scene - Components and Procedure We provide a central operator UI, which we call the \u0026ldquo;Gardener Dashboard\u0026rdquo;. It talks to a dedicated cluster, which we call the \u0026ldquo;Garden\u0026rdquo; cluster and uses custom resources managed by an aggregated API server, one of the general extension concepts of Kubernetes) to represent \u0026ldquo;shoot\u0026rdquo; clusters. In this \u0026ldquo;Garden\u0026rdquo; cluster runs the \u0026ldquo;Gardener\u0026rdquo;, which is basically a Kubernetes controller that watches the custom resources and acts upon them, i.e. creates, updates/modifies, or deletes \u0026ldquo;shoot\u0026rdquo; clusters. The creation follows basically these steps:\n Create a namespace in the \u0026ldquo;seed\u0026rdquo; cluster for the \u0026ldquo;shoot\u0026rdquo; cluster which will host the \u0026ldquo;shoot\u0026rdquo; cluster control plane Generate secrets and credentials which the worker nodes will need to talk to the control plane Create the infrastructure (using Terraform), which basically consists out of the network setup) Deploy the \u0026ldquo;shoot\u0026rdquo; cluster control plane into the \u0026ldquo;shoot\u0026rdquo; namespace in the \u0026ldquo;seed\u0026rdquo; cluster, containing the \u0026ldquo;machine-controller-manager\u0026rdquo; pod Create machine CRDs in the \u0026ldquo;seed\u0026rdquo; cluster, describing the configuration and the number of worker machines for the \u0026ldquo;shoot\u0026rdquo; (the machine-controller-manager watches the CRDs and creates virtual machines out of it) Wait for the \u0026ldquo;shoot\u0026rdquo; cluster API server to become responsive (pods will be scheduled, persistent volumes and load balancers are created by Kubernetes via the respective cloud provider) Finally we deploy kube-system daemons like kube-proxy and further add-ons like the dashboard into the \u0026ldquo;shoot\u0026rdquo; cluster and the cluster becomes active  Overview Architecture Diagram Note: The kubelet as well as the pods inside the \u0026ldquo;shoot\u0026rdquo; cluster talk through the front-door (load balancer IP; public Internet) to its \u0026ldquo;shoot\u0026rdquo; cluster API server running in the \u0026ldquo;seed\u0026rdquo; cluster. The reverse communication from the API server to the pod, service, and node networks happens through a VPN connection that we deploy into \u0026ldquo;seed\u0026rdquo; and \u0026ldquo;shoot\u0026rdquo; clusters.\n"
},
{
	"uri": "https://gardener.cloud/documentation/045_contribute/20_documentation/25_markup/attachments/",
	"title": "Attachments",
	"tags": [],
	"description": "The Attachments shortcode displays a list of files attached to a page.",
	"content": "The Attachments shortcode displays a list of files attached to a page.\n\r\rAttachments\r\r\u0026lt;div class=\u0026quot;attachments-files\u0026quot;\u0026gt;\r\u0026lt;li\u0026gt;\r\u0026lt;a href=\u0026quot;/documentation/045_contribute/20_documentation/25_markup/attachments.files/BachGavotteShort.mp3\u0026quot; \u0026gt;\rBachGavotteShort.mp3\r\u0026lt;/a\u0026gt;\r(0 ko)\r\u0026lt;/li\u0026gt;\r\u0026lt;li\u0026gt;\r\u0026lt;a href=\u0026quot;/documentation/045_contribute/20_documentation/25_markup/attachments.files/Carroll_AliceAuPaysDesMerveilles.pdf\u0026quot; \u0026gt;\rCarroll_AliceAuPaysDesMerveilles.pdf\r\u0026lt;/a\u0026gt;\r(0 ko)\r\u0026lt;/li\u0026gt;\r\u0026lt;li\u0026gt;\r\u0026lt;a href=\u0026quot;/documentation/045_contribute/20_documentation/25_markup/attachments.files/adivorciarsetoca00cape.pdf\u0026quot; \u0026gt;\radivorciarsetoca00cape.pdf\r\u0026lt;/a\u0026gt;\r(0 ko)\r\u0026lt;/li\u0026gt;\r\u0026lt;li\u0026gt;\r\u0026lt;a href=\u0026quot;/documentation/045_contribute/20_documentation/25_markup/attachments.files/hugo.png\u0026quot; \u0026gt;\rhugo.png\r\u0026lt;/a\u0026gt;\r(17 ko)\r\u0026lt;/li\u0026gt;\r\u0026lt;li\u0026gt;\r\u0026lt;a href=\u0026quot;/documentation/045_contribute/20_documentation/25_markup/attachments.files/movieselectricsheep-flock-244-32500-2.mp4\u0026quot; \u0026gt;\rmovieselectricsheep-flock-244-32500-2.mp4\r\u0026lt;/a\u0026gt;\r(0 ko)\r\u0026lt;/li\u0026gt;\r\u0026lt;div\u0026gt;\r \rUsage The shortcurt lists files found in a specific folder. Currently, it support two implementations for pages\n  If your page is a markdown file, attachements must be place in a folder named like your page and ending with .files.\n  content  _index.md page.files  attachment.pdf   page.md       If your page is a folder, attachements must be place in a nested \u0026lsquo;files\u0026rsquo; folder.\n  content  _index.md page  index.md files  attachment.pdf           Be aware that if you use a multilingual website, you will need to have as many folders as languages.\nThat's all !\nParameters    Parameter Default Description     title \u0026ldquo;Attachments\u0026rdquo; List's title   style \u0026quot;\u0026rdquo; Choose between \u0026ldquo;orange\u0026rdquo;, \u0026ldquo;grey\u0026rdquo;, \u0026ldquo;blue\u0026rdquo; and \u0026ldquo;green\u0026rdquo; for nice style   pattern \u0026ldquo;.*\u0026rdquo; A regular expressions, used to filter the attachments by file name. The pattern parameter value must be regular expressions.    For example:\n To match a file suffix of \u0026lsquo;jpg\u0026rsquo;, use *.jpg (not *.jpg). To match file names ending in \u0026lsquo;jpg\u0026rsquo; or \u0026lsquo;png\u0026rsquo;, use .*(jpg|png)  Examples List of attachments ending in pdf or mp4 {{%attachments title=\u0026quot;Related files\u0026quot; pattern=\u0026quot;.*(pdf|mp4)\u0026quot;/%}}\r renders as\n\r\rRelated files\r\r\u0026lt;div class=\u0026quot;attachments-files\u0026quot;\u0026gt;\r\u0026lt;li\u0026gt;\r\u0026lt;a href=\u0026quot;/documentation/045_contribute/20_documentation/25_markup/attachments.files/Carroll_AliceAuPaysDesMerveilles.pdf\u0026quot; \u0026gt;\rCarroll_AliceAuPaysDesMerveilles.pdf\r\u0026lt;/a\u0026gt;\r(0 ko)\r\u0026lt;/li\u0026gt;\r\u0026lt;li\u0026gt;\r\u0026lt;a href=\u0026quot;/documentation/045_contribute/20_documentation/25_markup/attachments.files/adivorciarsetoca00cape.pdf\u0026quot; \u0026gt;\radivorciarsetoca00cape.pdf\r\u0026lt;/a\u0026gt;\r(0 ko)\r\u0026lt;/li\u0026gt;\r\u0026lt;li\u0026gt;\r\u0026lt;a href=\u0026quot;/documentation/045_contribute/20_documentation/25_markup/attachments.files/movieselectricsheep-flock-244-32500-2.mp4\u0026quot; \u0026gt;\rmovieselectricsheep-flock-244-32500-2.mp4\r\u0026lt;/a\u0026gt;\r(0 ko)\r\u0026lt;/li\u0026gt;\r\u0026lt;div\u0026gt;\r \rColored styled box {{%attachments style=\u0026quot;orange\u0026quot; /%}}\r renders as\n\r\rAttachments\r\r\u0026lt;div class=\u0026quot;attachments-files\u0026quot;\u0026gt;\r\u0026lt;li\u0026gt;\r\u0026lt;a href=\u0026quot;/documentation/045_contribute/20_documentation/25_markup/attachments.files/BachGavotteShort.mp3\u0026quot; \u0026gt;\rBachGavotteShort.mp3\r\u0026lt;/a\u0026gt;\r(0 ko)\r\u0026lt;/li\u0026gt;\r\u0026lt;li\u0026gt;\r\u0026lt;a href=\u0026quot;/documentation/045_contribute/20_documentation/25_markup/attachments.files/Carroll_AliceAuPaysDesMerveilles.pdf\u0026quot; \u0026gt;\rCarroll_AliceAuPaysDesMerveilles.pdf\r\u0026lt;/a\u0026gt;\r(0 ko)\r\u0026lt;/li\u0026gt;\r\u0026lt;li\u0026gt;\r\u0026lt;a href=\u0026quot;/documentation/045_contribute/20_documentation/25_markup/attachments.files/adivorciarsetoca00cape.pdf\u0026quot; \u0026gt;\radivorciarsetoca00cape.pdf\r\u0026lt;/a\u0026gt;\r(0 ko)\r\u0026lt;/li\u0026gt;\r\u0026lt;li\u0026gt;\r\u0026lt;a href=\u0026quot;/documentation/045_contribute/20_documentation/25_markup/attachments.files/hugo.png\u0026quot; \u0026gt;\rhugo.png\r\u0026lt;/a\u0026gt;\r(17 ko)\r\u0026lt;/li\u0026gt;\r\u0026lt;li\u0026gt;\r\u0026lt;a href=\u0026quot;/documentation/045_contribute/20_documentation/25_markup/attachments.files/movieselectricsheep-flock-244-32500-2.mp4\u0026quot; \u0026gt;\rmovieselectricsheep-flock-244-32500-2.mp4\r\u0026lt;/a\u0026gt;\r(0 ko)\r\u0026lt;/li\u0026gt;\r\u0026lt;div\u0026gt;\r \r{{%attachments style=\u0026quot;grey\u0026quot; /%}}\r renders as\n\r\rAttachments\r\r\u0026lt;div class=\u0026quot;attachments-files\u0026quot;\u0026gt;\r\u0026lt;li\u0026gt;\r\u0026lt;a href=\u0026quot;/documentation/045_contribute/20_documentation/25_markup/attachments.files/BachGavotteShort.mp3\u0026quot; \u0026gt;\rBachGavotteShort.mp3\r\u0026lt;/a\u0026gt;\r(0 ko)\r\u0026lt;/li\u0026gt;\r\u0026lt;li\u0026gt;\r\u0026lt;a href=\u0026quot;/documentation/045_contribute/20_documentation/25_markup/attachments.files/Carroll_AliceAuPaysDesMerveilles.pdf\u0026quot; \u0026gt;\rCarroll_AliceAuPaysDesMerveilles.pdf\r\u0026lt;/a\u0026gt;\r(0 ko)\r\u0026lt;/li\u0026gt;\r\u0026lt;li\u0026gt;\r\u0026lt;a href=\u0026quot;/documentation/045_contribute/20_documentation/25_markup/attachments.files/adivorciarsetoca00cape.pdf\u0026quot; \u0026gt;\radivorciarsetoca00cape.pdf\r\u0026lt;/a\u0026gt;\r(0 ko)\r\u0026lt;/li\u0026gt;\r\u0026lt;li\u0026gt;\r\u0026lt;a href=\u0026quot;/documentation/045_contribute/20_documentation/25_markup/attachments.files/hugo.png\u0026quot; \u0026gt;\rhugo.png\r\u0026lt;/a\u0026gt;\r(17 ko)\r\u0026lt;/li\u0026gt;\r\u0026lt;li\u0026gt;\r\u0026lt;a href=\u0026quot;/documentation/045_contribute/20_documentation/25_markup/attachments.files/movieselectricsheep-flock-244-32500-2.mp4\u0026quot; \u0026gt;\rmovieselectricsheep-flock-244-32500-2.mp4\r\u0026lt;/a\u0026gt;\r(0 ko)\r\u0026lt;/li\u0026gt;\r\u0026lt;div\u0026gt;\r \r{{%attachments style=\u0026quot;blue\u0026quot; /%}}\r renders as\n\r\rAttachments\r\r\u0026lt;div class=\u0026quot;attachments-files\u0026quot;\u0026gt;\r\u0026lt;li\u0026gt;\r\u0026lt;a href=\u0026quot;/documentation/045_contribute/20_documentation/25_markup/attachments.files/BachGavotteShort.mp3\u0026quot; \u0026gt;\rBachGavotteShort.mp3\r\u0026lt;/a\u0026gt;\r(0 ko)\r\u0026lt;/li\u0026gt;\r\u0026lt;li\u0026gt;\r\u0026lt;a href=\u0026quot;/documentation/045_contribute/20_documentation/25_markup/attachments.files/Carroll_AliceAuPaysDesMerveilles.pdf\u0026quot; \u0026gt;\rCarroll_AliceAuPaysDesMerveilles.pdf\r\u0026lt;/a\u0026gt;\r(0 ko)\r\u0026lt;/li\u0026gt;\r\u0026lt;li\u0026gt;\r\u0026lt;a href=\u0026quot;/documentation/045_contribute/20_documentation/25_markup/attachments.files/adivorciarsetoca00cape.pdf\u0026quot; \u0026gt;\radivorciarsetoca00cape.pdf\r\u0026lt;/a\u0026gt;\r(0 ko)\r\u0026lt;/li\u0026gt;\r\u0026lt;li\u0026gt;\r\u0026lt;a href=\u0026quot;/documentation/045_contribute/20_documentation/25_markup/attachments.files/hugo.png\u0026quot; \u0026gt;\rhugo.png\r\u0026lt;/a\u0026gt;\r(17 ko)\r\u0026lt;/li\u0026gt;\r\u0026lt;li\u0026gt;\r\u0026lt;a href=\u0026quot;/documentation/045_contribute/20_documentation/25_markup/attachments.files/movieselectricsheep-flock-244-32500-2.mp4\u0026quot; \u0026gt;\rmovieselectricsheep-flock-244-32500-2.mp4\r\u0026lt;/a\u0026gt;\r(0 ko)\r\u0026lt;/li\u0026gt;\r\u0026lt;div\u0026gt;\r \r{{%attachments style=\u0026quot;green\u0026quot; /%}}\r renders as\n\r\rAttachments\r\r\u0026lt;div class=\u0026quot;attachments-files\u0026quot;\u0026gt;\r\u0026lt;li\u0026gt;\r\u0026lt;a href=\u0026quot;/documentation/045_contribute/20_documentation/25_markup/attachments.files/BachGavotteShort.mp3\u0026quot; \u0026gt;\rBachGavotteShort.mp3\r\u0026lt;/a\u0026gt;\r(0 ko)\r\u0026lt;/li\u0026gt;\r\u0026lt;li\u0026gt;\r\u0026lt;a href=\u0026quot;/documentation/045_contribute/20_documentation/25_markup/attachments.files/Carroll_AliceAuPaysDesMerveilles.pdf\u0026quot; \u0026gt;\rCarroll_AliceAuPaysDesMerveilles.pdf\r\u0026lt;/a\u0026gt;\r(0 ko)\r\u0026lt;/li\u0026gt;\r\u0026lt;li\u0026gt;\r\u0026lt;a href=\u0026quot;/documentation/045_contribute/20_documentation/25_markup/attachments.files/adivorciarsetoca00cape.pdf\u0026quot; \u0026gt;\radivorciarsetoca00cape.pdf\r\u0026lt;/a\u0026gt;\r(0 ko)\r\u0026lt;/li\u0026gt;\r\u0026lt;li\u0026gt;\r\u0026lt;a href=\u0026quot;/documentation/045_contribute/20_documentation/25_markup/attachments.files/hugo.png\u0026quot; \u0026gt;\rhugo.png\r\u0026lt;/a\u0026gt;\r(17 ko)\r\u0026lt;/li\u0026gt;\r\u0026lt;li\u0026gt;\r\u0026lt;a href=\u0026quot;/documentation/045_contribute/20_documentation/25_markup/attachments.files/movieselectricsheep-flock-244-32500-2.mp4\u0026quot; \u0026gt;\rmovieselectricsheep-flock-244-32500-2.mp4\r\u0026lt;/a\u0026gt;\r(0 ko)\r\u0026lt;/li\u0026gt;\r\u0026lt;div\u0026gt;\r \r"
},
{
	"uri": "https://gardener.cloud/blog/2018_week_46/",
	"title": "Auditing Kubernetes for Secure Setup",
	"tags": [],
	"description": "",
	"content": "In summer 2018, the Gardener project team asked Kinvolk to execute several penetration tests in its role as third-party contractor. The goal of this ongoing work is to increase the security of all Gardener stakeholders in the open source community. Following the Gardener architecture, the control plane of a Gardener managed shoot cluster resides in the corresponding seed cluster. This is a Control-Plane-as-a-Service with a network air gap.\n\rAlong the way we found various kinds of security issues, for example, due to misconfiguration or missing isolation, as well as two special problems with upstream Kubernetes and its Control-Plane-as-a-Service architecture.\n..read some more on Auditing Kubernetes for Secure Setup.\n"
},
{
	"uri": "https://gardener.cloud/documentation/050-tutorials/content/howto/insecure-configuration/",
	"title": "Auditing Kubernetes for Secure Setup",
	"tags": [],
	"description": "A few insecure configurations in Kubernetes",
	"content": "Auditing Kubernetes for Secure Setup \u0026lt;object type=\u0026quot;image/svg+xml\u0026quot; data=\u0026quot;./images/teaser.svg\u0026quot; style=\u0026quot;;visibility:hidden; margin: 3rem auto;display: block;\u0026quot; class=\u0026quot;inline reveal-fast drop-shadow\u0026quot;\u0026gt;\u0026lt;/object\u0026gt;\r Increasing the Security of all Gardener Stakeholders In summer 2018, the Gardener project team asked Kinvolk to execute several penetration tests in its role as third-party contractor. The goal of this ongoing work is to increase the security of all Gardener stakeholders in the open source community. Following the Gardener architecture, the control plane of a Gardener managed shoot cluster resides in the corresponding seed cluster. This is a Control-Plane-as-a-Service with a network air gap.\nAlong the way we found various kinds of security issues, for example, due to misconfiguration or missing isolation, as well as two special problems with upstream Kubernetes and its Control-Plane-as-a-Service architecture.\nMajor Findings From this experience, we’d like to share a few examples of security issues that could happen on a Kubernetes installation and how to fix them.\nAlban Crequy (Kinvolk) and Dirk Marwinski (SAP SE) gave a presentation entitled Hardening Multi-Cloud Kubernetes Clusters as a Service at KubeCon 2018 in Shanghai presenting some of the findings.\nHere is a summary of the findings:\n  Privilege escalation due to insecure configuration of the Kubernetes API server\n Root cause: Same certificate authority (CA) is used for both the API server and the proxy that allows accessing the API server. Risk: Users can get access to the API server. Recommendation: Always use different CAs.    Exploration of the control plane network with malicious HTTP-redirects\n  Root cause: See detailed description below.\n  Risk: Provoked error message contains full HTTP payload from an existing endpoint which can be exploited. The contents of the payload depends on your setup, but can potentially be user data, configuration data, and credentials.\n  Recommendation:\n Use the latest version of Gardener Ensure the seed cluster's container network supports network policies. Clusters that have been created with Kubify are not protected as Flannel is used there which doesn't support network policies.      Reading private AWS metadata via Grafana\n Root cause: It is possible to configuring a new custom data source in Grafana, we could send HTTP requests to target the control Risk: Users can get the \u0026ldquo;user-data\u0026rdquo; for the seed cluster from the metadata service and retrieve a kubeconfig for that Kubernetes cluster Recommendation: Lockdown Grafana features to only what's necessary in this setup, block all unnecessary outgoing traffic, move Grafana to a different network, lockdown unauthenticated endpoints    Scenario 1: Privilege Escalation with Insecure API Server In most configurations, different components connect directly to the Kubernetes API server, often using a kubeconfig with a client certificate. The API server is started with the flag:\n/hyperkube apiserver --client-ca-file=/srv/kubernetes/ca/ca.crt ...\rThe API server will check whether the client certificate presented by kubectl, kubelet, scheduler or another component is really signed by the configured certificate authority for clients.\n\rThe API server can have many clients of various kinds\r\rHowever, it is possible to configure the API server differently for use with an intermediate authenticating proxy. The proxy will authenticate the client with its own custom method and then issue HTTP requests to the API server with additional HTTP headers specifying the user name and group name. The API server should only accept HTTP requests with HTTP headers from a legitimate proxy. To allow the API server to check incoming requests, you need pass on a list of certificate authorities (CAs) to it. Requests coming from a proxy are only accepted if they use a client certificate that is signed by one of the CAs of that list.\n--requestheader-client-ca-file=/srv/kubernetes/ca/ca-proxy.crt\r--requestheader-username-headers=X-Remote-User\r--requestheader-group-headers=X-Remote-Group\r\rAPI server clients can reach the API server through an authenticating proxy\r\rSo far, so good. But what happens if malicious user “Mallory” tries to connect directly to the API server and reuses the HTTP headers to pretend to be someone else?\n\rWhat happens when a client bypasses the proxy, connecting directly to the API server?\r\rWith a correct configuration, Mallory’s kubeconfig will have a certificate signed by the API server certificate authority but not signed by the proxy certificate authority. So the API server will not accept the extra HTTP header “X-Remote-Group: system:masters”.\nYou only run into an issue when the same certificate authority is used for both the API server and the proxy. Then, any Kubernetes client certificate can be used to take the role of different user or group as the API server will accept the user header and group header.\nThe kubectl tool does not normally add those HTTP headers but it’s pretty easy to generate the corresponding HTTP requests manually.\nWe worked on improving the Kubernetes documentation to make clearer that this configuration should be avoided.\nScenario 2: Exploration of the Control Plane Network with Malicious HTTP-Redirects The API server is a central component of Kubernetes and many components initiate connections to it, including the Kubelet running on worker nodes. Most of the requests from those clients will end up updating Kubernetes objects (pods, services, deployments, and so on) in the etcd database but the API server usually does not need to initiate TCP connections itself.\n\rThe API server is mostly a component that receives requests\r\rHowever, there are exceptions. Some kubectl commands will trigger the API server to open a new connection to the Kubelet. Kubectl exec is one of those commands. In order to get the standard I/Os from the pod, the API server will start an HTTP connection to the Kubelet on the worker node where the pod is running. Depending on the container runtime used, it can be done in different ways, but one way to do it is for the Kubelet to reply with a HTTP-302 redirection to the Container Runtime Interface (CRI). Basically, the Kubelet is telling the API server to get the streams from CRI itself directly instead of forwarding. The redirection from the Kubelet will only change the port and path from the URL; the IP address will not be changed because the Kubelet and the CRI component run on the same worker node.\n\rBut the API server also initiates some connections, for example, to worker nodes\r\rIt’s often quite easy for users of a Kubernetes cluster to get access to worker nodes and tamper with the Kubelet. They could be given explicit SSH access or they could be given a kubeconfig with enough privileges to create privileged pods or even just pods with “host” volumes.\nIn contrast, users — even those with “system:masters” permissions or “root” rights — are often not given access to the control plane. On setups like for example GKE or Gardener, the control plane is running on separate nodes, with a different administrative access. It could be hosted on a different cloud provider account. So users are not free to explore the internal network in the control plane.\nWhat would happen if a user was tampering with the Kubelet to make it maliciously redirect kubectl exec requests to a different random endpoint? Most likely the given endpoint would not speak the streaming server protocol, so there would be an error. However, the full HTTP payload from the endpoint is included in the error message printed by kubectl exec.\n\rThe API server is tricked to connect to other components\r\rThe impact of this issue depends on the specific setup. But in many configurations, we could find a metadata service (such as the AWS metadata service) containing user data, configurations and credentials. The setup we explored had a different AWS account and a different EC2 instance profile for the worker nodes and the control plane. This issue allowed users to get access to the AWS metadata service in the context of the control plane, which they should not have access to.\nWe have reported this issue to the Kubernetes Security mailing list and the public pull request that addresses the issue has been merged PR#66516. It provides a way to enforce HTTP redirect validation (disabled by default).\nBut there are several other ways that users could trigger the API server to generate HTTP requests and get the reply payload back, so it is advised to isolate the API server and other components from the network as additional precautious measures. Depending on where the API server runs, it could be with Kubernetes Network Policies, EC2 Security Groups or just iptables directly. Following the defense in depth principle, it is a good idea to apply the API server HTTP redirect validation when it is available as well as firewall rules.\nIn Gardener, this has been fixed with Kubernetes network policies along with changes to ensure the API server does not need to contact the metadata service. You can see more details in the announcements on the Gardener mailing list. This is tracked in CVE-2018-2475.\nTo be protected from this issue, stakeholders should:\n Use the latest version of Gardener Ensure the seed cluster’s container network supports network policies. Clusters that have been created with Kubify are not protected as Flannel is used there which doesn’t support network policies.  Scenario 3: Reading Private AWS Metadata via Grafana For our tests, we had access to a Kubernetes setup where users are not only given access to the API server in the control plane, but also to a Grafana instance that is used to gather data from their Kubernetes clusters via Prometheus. The control plane is managed and users don’t have access to the nodes that it runs. They can only access the API server and Grafana via a load balancer. The internal network of the control plane is therefore hidden to users.\n\rPrometheus and Grafana can be used to monitor worker nodes\r\rUnfortunately, that setup was not protecting the control plane network from nosy users. By configuring a new custom data source in Grafana, we could send HTTP requests to target the control plane network, for example the AWS metadata service. The reply payload is not displayed on the Grafana Web UI but it is possible to access it from the debugging console of the Chrome browser.\n\rCredentials can be retrieved from the debugging console of Chrome\r\r\rAdding a Grafana data source is a way to issue HTTP requests to arbitrary targets\r\rIn that installation, users could get the “user-data” for the seed cluster from the metadata service and retrieve a kubeconfig for that Kubernetes cluster.\nThere are many possible measures to avoid this situation: lockdown Grafana features to only what’s necessary in this setup, block all unnecessary outgoing traffic, move Grafana to a different network, lockdown unauthenticated endpoints.\nConclusion The three scenarios above show pitfalls with a Kubernetes setup. A lot of them were specific to the Kubernetes installation: different cloud providers or different configurations will show different weakness. Users should no longer be given access to Grafana.\n"
},
{
	"uri": "https://gardener.cloud/documentation/050-tutorials/content/howto/oidc-login/",
	"title": "Authenticating with an Identity Provider",
	"tags": [],
	"description": "Authenticating with an Identity Provider using OpenID Connect",
	"content": "Authenticating In this blog you will learn how to:\n Configure an Identity Provider using OpenID Connect. Configure a local kubectl plugin to enable oidc-login . Configure the K8s API Server of Gardener managed Kubernetes cluster. Create an RBAC rule to authorize an authenticated user.  Motivation As a project owner of Gardener, I want my Kubernetes level user to be authenticated by an identity provider.\nPrerequisite Knowledge Please read the following background material on Authenticating\nInsights About Gardener The Gardener allows the administrator to modify every aspect of the control plane setup, e.g. all feature gateways and even configurations are programmatically accessible. In this way, every administrative user of the Gardener has full control of how the control plane should be parameterized. But with this power, the user can easily configure a control plane that is beyond any SLA that the Gardener team can arguably support. Therefore, use this power wisely! A configuration that enables experimental features for production becomes an operational responsibility of the cluster owner's team.\nBut Gardener does not stop you from experimenting!\nThere are currently no default IdP parameters.\nConfigure an Identity Provider Create a tenant in an OpenID-Connect compatible Identity Provider. For sake of simplicity, we shall use Auth0, which has a free plan for experimentations.\nIn your tenant, setup a native client/application that will use the authentication: Configure the client to have a callback url of http://localhost:8000. This callback will connect to your local kubectl oidc-login plugin: Note down the following parameters:\n Domain or Issuer url. It must be an https-secured endpoint (In case of Auth0, notice the trailing / at the end). Client ID Client Secret  Verify that https://\u0026lt;Issuer\u0026gt;/.well-known/openid-configuration is reachable.\nNow create some users (or connect to a user store): Notice that the users must have a verified email address. In doubt, just override that setting manually.\nConfigure kubectl oidc-login Please install the kubectl plugin oidc-login. We highly recommend the krew install tool, which also makes other plugins easily available.\n$ kubectl krew install oidc-login\rUpdated the local copy of plugin index.\rInstalling plugin: oidc-login\rCAVEATS:\r\\\r| You need to setup the OIDC provider, Kubernetes API server, role binding and kubeconfig.\r| See https://github.com/int128/kubelogin for more.\r/\rInstalled plugin: oidc-login\rPrepare a kubeconfig for later use:\n$ cp ~/.kube/config ~/.kube/config-oidc\rModify the configurations as follows:\napiVersion: v1\rkind: Config\r...\rcontexts:\r- context:\rcluster: shoot--project--mycluster\ruser: my-oidc\rname: shoot--project--mycluster\r...\rusers:\r- name: my-oidc\ruser:\rauth-provider:\rconfig:\rclient-id: \u0026lt;Client ID\u0026gt;\rclient-secret: \u0026lt;Client Secret\u0026gt;\r idp-issuer-url: \u0026#34;https://\u0026lt;Issuer\u0026gt;/\u0026#34;\rextra-scopes: email,offline_access,profile\rname: oidc\rEnsure that the modified context is the active context current-context: shoot--project--mycluster.\nConfigure the Gardener Shoot Spec Modify the Gardener shoot/cluster manifest as follows:\napiVersion: core.gardener.cloud/v1beta1\rkind: Shoot\rmetadata:\rname: mycluster\rnamespace: garden-project\r...\rspec:\rkubernetes:\rkubeAPIServer:\roidcConfig:\rclientID: \u0026lt;Client ID\u0026gt;\rissuerURL: \u0026#34;https://\u0026lt;Issuer\u0026gt;/\u0026#34;\r usernameClaim: email\rThis change of the shoot manifest triggers a reconciliation. Once the reconciliation is finished, your oidc configuration is applied. It does not invalidate other certificate based authentication methods. Wait for Gardener to reconcile the change. It can take upto 5min.\nAuthorize an authenticated user For simplicity, we will just authorize a single user with the all encompassing cluster role cluster-admin:\napiVersion: rbac.authorization.k8s.io/v1\rkind: ClusterRoleBinding\rmetadata:\rname: cluster-admin-test\rroleRef:\rapiGroup: rbac.authorization.k8s.io\rkind: ClusterRole\rname: cluster-admin\rsubjects:\r- apiGroup: rbac.authorization.k8s.io\rkind: User\rname: test@test.com\rAs administrator, activate/apply the above cluster role binding for `test@test.com`.\nVerify the Result Now activate the prepared kubeconfig-oidc and perform a login:\n$ export KUBECONFIG=~/.kube/config-oidc\r$ kubectl oidc-login\rOpen http://localhost:8000 for authentication\rThe plugin opens a browser for an interctive authentication session, and in parallel serves a local webserver for the configured callback.\nIf you successfully verified your user, then the console will display the validity of your returned token:\nYou got a valid token until 2019-08-14 06:26:49 +0200 CEST\rInspect the kubeconfig-oidc. You will find two additional parameters:\n...\rusers:\r- name: my-oidc\ruser:\rauth-provider:\rconfig:\rclient-id: \u0026lt;Client ID\u0026gt;\rclient-secret: \u0026lt;Client Secret\u0026gt;\r idp-issuer-url: \u0026#34;https://\u0026lt;Issuer\u0026gt;/\u0026#34;\rextra-scopes: email,offline_access,profile\rid-token: eyJ0eX ... 4In0.QQKS ... TTTw\rrefresh-token: LFt ... 0Skj\rname: oidc\rThe plugin persisted the id-token and refresh-token in your configuration file.\nVerify that your user actually has the cluster-admin role:\n$ kubectl get po --all-namespaces\rNAMESPACE NAME READY STATUS RESTARTS AGE\rkube-system blackbox-exporter-954dd954b-tk9vl 1/1 Running 0 7d5h\rkube-system calico-kube-controllers-5f4b46ffb5-ggb7z 1/1 Running 0 7d5h\r...\r$ kubectl who-can create clusterrolebinding\rNo subjects found with permissions to create clusterrolebinding assigned through RoleBindings\rCLUSTERROLEBINDING SUBJECT TYPE SA-NAMESPACE\rcluster-admin system:masters Group\rcluster-admin-test test@test.com User\r...\rCongratulations, you have just configured your cluster to authenticate against an Identity Provider using OpenID Connect!\n"
},
{
	"uri": "https://gardener.cloud/documentation/050-tutorials/content/howto/kubectl-apiserver/",
	"title": "Automated deployment",
	"tags": [],
	"description": "Automated deployment with kubectl",
	"content": "Introduction With kubectl you can easily deploy an image from your local environment.\nHowever, what if you want to use a automated deployment script on a CI server (e.g. Jenkins), but don't want to store the KUBECONFIG on that server?\nYou can use kubectl and connect to the API-server of your cluster.\nPrerequisites   Create a service account user\nkubectl create serviceaccount deploy-user -n default\r  Bind a role to the newly created serviceuser\n !!! Warning !!! In this example the preconfigured role edit and the namespace default is being used, please adjust the role to a more strict scope! see https://kubernetes.io/docs/admin/authorization/rbac/\n kubectl create rolebinding deploy-default-role --clusterrole=edit --serviceaccount=default:deploy-user --namespace=default\r  Get the URL of your API-server\nAPISERVER=$(kubectl config view | grep server | cut -f 2- -d \u0026quot;:\u0026quot; | tr -d \u0026quot; \u0026quot;)\r  Get the service account\nSERVICEACCOUNT=$(kubectl get serviceaccount deploy-user -n default -o=jsonpath={.secrets[0].name})\r  Generate a token for the serviceaccount\nTOKEN=$(kubectl get secret -n default $SERVICEACCOUNT -o=jsonpath={.data.token} | base64 -D)\r  Usage You can deploy your app without setting the kubeconfig locally, you just need to pass the environment variables (e.g. store them in the Jenkins credentials store)\nkubectl --server=${APIServer} --token=${TOKEN} --insecure-skip-tls-verify=true apply --filename myapp.yaml\r"
},
{
	"uri": "https://gardener.cloud/blog/2018_week_07/",
	"title": "Big things come in small packages",
	"tags": [],
	"description": "",
	"content": "Microservices tend to use smaller runtimes but you can use what you have today - and this can be a problem in kubernetes.\nSwitching your architecture from a monolith to microservices has many advantages, both in the way you write software and the way it is used throughout its lifecycle. In this post, my attempt is to cover one problem which does not get as much attention and discussion - size of the technology stack.\nGeneral purpose technology stack \u0026lt;img src=\u0026quot;../blog/2018_week_07/blog-service-common-stack.png\u0026quot; title=\u0026quot;logo\u0026quot; width=\u0026quot;100%\u0026quot; class=\u0026quot;drop-shadow reveal-fast\u0026quot; style=\u0026quot;\u0026quot;/\u0026gt;\r There is a tendency to be more generalized in development and to apply this pattern to all services. One feels that a homogeneous image of the technology stack is good if it is the same for all services.\nOne forgets, however, that a large percentage of the integrated infrastructure is not used by all services in the same way, and is therefore only a burden. Thus, resources are wasted and the entire application becomes expensive in operation and scales very badly.\nLight technology stack Due to the lightweight nature of your service, you can run more containers on a physical server and virtual machines. The result is higher resource utilization.\n\u0026lt;img src=\u0026quot;../blog/2018_week_07/blog-service-service-stack.png\u0026quot; title=\u0026quot;logo\u0026quot; width=\u0026quot;100%\u0026quot; class=\u0026quot;drop-shadow reveal-fast\u0026quot; style=\u0026quot;\u0026quot;/\u0026gt;\r Additionally, microservices are developed and deployed as containers independently of each another. This means that a development team can develop, optimize and deploy a microservice without impacting other subsystems.\n"
},
{
	"uri": "https://gardener.cloud/blog/",
	"title": "Blogs",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gardener.cloud/components/bouquet/",
	"title": "Bouquet",
	"tags": [],
	"description": "",
	"content": "Bouquet Bouquet is a draft addon manager for the Gardener. It incorporates some of the requested features of the community but not yet all of them.\n Caution: This software is early alpha. It is not meant for production use and shall (currently) only serve as a possible outlook of what is possible with pre-deployed software on Gardener Kubernetes clusters.\n Installation If you want to deploy Bouquet on a target Gardener cluster, run the following:\nhelm install charts/bouquet \\\r --name gardener-bouquet \\\r --namespace garden\rThis will deploy Bouquet with the required permissions into your garden cluster.\nStructure As of now, Bouquet comes with two new custom resources: AddonManifest and AddonInstance.\nAn AddonManifest can be considered equivalent to a Helm template. The manifest itself only contains metadata (like the name, default values etc.). The actual content of a manifest is specified via its source attribute. Currently, the only available source is a ConfigMap.\nAn AddonInstance references an AddonManifest and a target Shoot. It may also contain value overrides in its spec. As soon as an AddonInstance is created, Bouquet will apply the values to the templates and then ensure that the objects exist in the target shoot. If an AddonInstance is deleted, Bouquet will also make sure that the created objects are deleted as well.\nExample use case Say you want your cluster to contain istio right from the start. How can you do that?\nFirst you need to get the .yaml files necessary to deploy istio into your cluster. Download an istio release as follows:\nwget -O istio.yaml https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/istio/noauth/istio.yaml\rThis will fetch a .yaml file containing all necessary kubernetes objects of istio. To make this data available in your garden cluster, create a configmap in your cluster via\nkubectl -n garden create configmap istio-files --from-file ./istio.yaml\rNow you need to create an AddonManifest that references this file and push it to Kubernetes. The file could look like the following:\napiVersion: \u0026#34;garden.sapcloud.io/v1alpha1\u0026#34;\rkind: \u0026#34;AddonManifest\u0026#34;\rmetadata:\rname: \u0026#34;istio-0.0.1\u0026#34;\rspec:\rconfigMap: \u0026#34;istio-files\u0026#34;\rYou can submit this manifest to Kubernetes via kubectl (given that you saved the file to addonmanifest.yaml:\nkubectl -n garden apply -f addonmanifest.yaml\rOnce this is done, the only thing left to do is to create an AddonInstance referencing both your target Shoot and your AddonManifest. This AddonInstance has to be in the same namespace as your target Shoot:\napiVersion: \u0026#34;garden.sapcloud.io/v1alpha1\u0026#34;\rkind: \u0026#34;AddonInstance\u0026#34;\rmetadata:\rname: \u0026#34;example\u0026#34;\rfinalizers:\r- \u0026#34;bouquet\u0026#34;\rspec:\rmanifest:\rnamespace: \u0026#34;garden\u0026#34;\rname: \u0026#34;istio\u0026#34;\rversion: \u0026#34;0.0.1\u0026#34;\rtarget:\rshoot: \u0026#34;addon-test\u0026#34;\rAnd apply it via kubectl (given that you saved the file to addoninstance.yaml):\nkubectl -n garden-addon-test apply -f addoninstance.yaml\rBouquet will then start deploying your objects to the target Shoot once it is ready.\nOutlook / Future Since this is just a tech-preview, features like value / chart updates, more efficient templating, company addon guidelines etc. are not yet implemented / yet to come / yet to be discussed. It is also not yet clear whether this should eventually move into the Gardener or remain as a stand-alone component.\nCore points that have to be tackled are:\n Fire and forget mode (only deploy objects once, don't monitor afterwards) Reconciliation (currently, updating behavior is not correctly implemented) Updates of an addon (-\u0026gt; Update strategies) Dependent addons / dependency resolution / dependency lifecycle  As such, contributions and help on shaping this topic is highly appreciated.\n"
},
{
	"uri": "https://gardener.cloud/documentation/045_contribute/20_documentation/25_markup/button/",
	"title": "Button",
	"tags": [],
	"description": "Nice buttons on your page.",
	"content": "A button is a just a clickable button with optional icon.\n{{% button href=\u0026quot;https://getgrav.org/\u0026quot; %}}Get Grav{{% /button %}}\r{{% button href=\u0026quot;https://getgrav.org/\u0026quot; icon=\u0026quot;fa fa-download\u0026quot; %}}Get Grav with icon{{% /button %}}\r{{% button href=\u0026quot;https://getgrav.org/\u0026quot; icon=\u0026quot;fa fa-download\u0026quot; icon-position=\u0026quot;right\u0026quot; %}}Get Grav with icon right{{% /button %}}\rGet Grav\n\r\nGet Grav with icon\n\rGet Grav with icon right\n\n\r"
},
{
	"uri": "https://gardener.cloud/using-gardener/developer/experience/",
	"title": "By Skill",
	"tags": [],
	"description": "",
	"content": "Learning Material\rEverything you need to know about running your software.\r\r\r\r\rby Topic\r\r\r\r \rby Experience Level \r\r\r"
},
{
	"uri": "https://gardener.cloud/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gardener.cloud/components/cert-broker/",
	"title": "Cert Broker",
	"tags": [],
	"description": "",
	"content": "cert-broker Cert-Broker is a complementary component for Cert-Manager. It enables certificate management for Kubernetes clusters which don't operate their own (in-cluster) Cert-Manager, e.g. for organizational resposibilities the Cert-Manager is located in another cluster.\nConcept To use or contribute to Cert-Broker it is fundamental to understand its main concept of control and target cluster.\n Control cluster: The cluster which operates an instance of Cert-Manager. Target cluster: The cluster which demands TLS certificates by Cert-Manager through the Cert-Broker.  Cert-Broker replicates Ingress resources from the target cluster to a predefined namespace in the control cluster. After the matching TLS Secret resource was created by Cert-Manager, Cert-Broker copies it to the appropriate Namespace in the target cluster. This works similarily in case the certificate is renewed.\nInstallation To install Cert-Broker on the control cluster, fill out the place holders and run\nhelm install charts/cert-broker \\\r--name cert-broker \\\r--namespace \u0026lt;Namespace\u0026gt; \\\r--set certbroker.targetClusterSecret=\u0026lt;Target cluster Kubeconfig\u0026gt; \\\r--set certmanager.dns=\u0026quot;{\u0026quot;\u0026lt;Domain\u0026gt;\u0026quot;.\u0026quot;\u0026lt;DNS Provider\u0026gt;\u0026quot;, \u0026quot;\u0026lt;Domain\u0026gt;\u0026quot;.\u0026quot;\u0026lt;DNS Provider\u0026gt;\u0026quot;}\u0026quot; \\\r--set certmanager.clusterissuer=\u0026quot;\u0026lt;Issuer Name\u0026gt;\u0026quot;\rLimitations In case Cert-Manager issues certificates for the target cluster with Let's Encrypt, the domain's ownership can only be proven by DNS records, i.e. DNS01 Challenges must be used.\n"
},
{
	"uri": "https://gardener.cloud/documentation/045_contribute/10_code/14_cicd/",
	"title": "CI/CD",
	"tags": [],
	"description": "",
	"content": "CI/CD As an execution environment for CI/CD workloads, we use Concourse. We however abstract from the underlying \u0026ldquo;build executor\u0026rdquo; and instead offer a Pipeline Definition Contract, through which components declare their build pipelines as required.\nIn order to run continuous delivery workloads for all components contributing to the Gardener project, we operate a central service.\nTypical workloads encompass the execution of tests and builds of a variety of technologies, as well as building and publishing container images, typically containing build results.\nWe are building our CI/CD offering around some principles:\n container-native - each workload is executed within a container environment. Components may customise used container images automation - pipelines are generated without manual interaction self-service - components customise their pipelines by changing their sources standardisation  Learn more on our: Build Pipeline Reference Manual\n"
},
{
	"uri": "https://gardener.cloud/blog/2019_week_21/",
	"title": "Cluster Overprovisioning",
	"tags": [],
	"description": "",
	"content": "This tutorial describes how to overprovisioning of cluster nodes for scaling and failover. This is desired when you have work load that need to scale up quickly without waiting for the new cluster nodes to be created and join the cluster.\nA similar problem occurs when crashing a node from the Hyperscaler. This must be replaced by Kubernetes as fast as possible. The solution can be overprovisioning of nodes\n..read some more on Cluster Overprovisioning.\n"
},
{
	"uri": "https://gardener.cloud/documentation/050-tutorials/content/app/node-overprovisioning/",
	"title": "Cluster Overprovisioning",
	"tags": [],
	"description": "Cluster Overprovisioning",
	"content": "Cluster Overprovisioning This tutorial describes how to overprovisioning of cluster nodes for scaling and failover. This is desired when you have work loads that need to scale up quickly without waiting for the new cluster nodes to be created and join the cluster.\nA similar problem occurs when crashing a node from the Hyperscaler. This must be replaced by Kubernetes as fast as possible. The solution can be overprovisioning of nodes\n Overprovisioning: Allocating more computer resources than is strictly necessary\nhttps://en.wikipedia.org/wiki/Overprovisioning\n When does the autoscaler change the size of the cluster? Below is a description of how the cluster behaves when there is a requirement to scale.\nScaling without overprovisioning  load hits the cluster (or a node is crashed) cannot schedule application-pods due to insufficient resources, scaling fails 💀 cluster-autoscaler notices and begins to provision new instance wait for instance to be provisioned, boot, join the cluster and become ready kube-scheduler will notice there is somewhere to put the application-pods and will schedule them  Scaling with Overprovisioning  load hits the cluster (or a node is crashed) placeholder-pods are evicted, scaling of application-pod is immediately successful placeholder-pods cannot be scheduled due to insufficient resources wait for instance to be provisioned, boot, join the cluster and become ready kube-scheduler will notice there is somewhere to put the placeholder pods and will schedule them  You can apply the above scenario one-to-one to the case when a node of the Hyperscaler dies.\nReal Scenario Test We executed normal and overprovisioning tests on a gardener cluster on different infrastructure provider (aws, azure, gcp, alicloud). All of them tested the downtime of the application pod running in the cluster, when a node dies.\nThe test results for the different IaaS provider are shown below.\nResults The results provided should only show how long the downtimes can be approximately.\n The downtime results could vary +- 1 min, because the minimum request interval in UpTime is 1 minute\n Amazon Normal Overprovisioning Azure Normal Overprovisioning GCP Normal Overprovisioning AliCloud Normal Overprovisioning Summary of results Normal    Provider AWS Azure GCP AliCloud     Node deleted 08:56 09:32 09:39 09:53   Pod rescheduled 09:17 09:50 09:53 10:14   Downtime 21 min 18 min 14 min 21 min    Overprovisioning    Provider AWS Azure GCP AliCloud     Node deleted 14:20 06:00 06:05 08:23   Pod rescheduled 14:22 06:02 06:06 08:25   Downtime 2 min 2 min 1 min 2 min    Test Description We deployed a nginx web server and a service of type LoadBalancer to expose it. So we are able to call our endpoint with external tools like UpTime to check the availability of our nginx. It takes only a few seconds to deploy a nginx web server on kubernetes, so we could say: when your endpoint works, your node is up and running.\nWe wanted to test how much time it takes, when your node gets killed and your cluster has to create a new one to run your application on it.\nkubectl get nodes\r# select the node where your nginx is running on\rkubectl delete node \u0026lt;NGINX-HOSTED-NODE\u0026gt;\rThe downtime is tested with UpTime, which does every minute a request to our endpoint. Further we checked manually, if the node startup time and the timestamps on UpTime are almost similar.\nNext, deploy the overprovisioned version of our demo application and kill the node with the NGINX. As you can see - the pod comes up very fast and can serve content again.\n"
},
{
	"uri": "https://gardener.cloud/contribute/code/",
	"title": "Code",
	"tags": [],
	"description": "",
	"content": " Contributing Code\rHow to Contribute to the Open Source Project Gardener\r\r\r\rYou are welcome to contribute code to Gardener in order to fix a bug or to implement a new feature.\nThe following rules govern code contributions:\n Contributions must be licensed under the Apache 2.0 License You need to sign the Contributor License Agreement. We are using CLA assistant providing a click-through workflow for accepting the CLA. For company contributors additionally the company needs to sign a corporate license agreement. See the following sections for details.  "
},
{
	"uri": "https://gardener.cloud/community/",
	"title": "Community",
	"tags": [],
	"description": "",
	"content": "\r#body-inner {\rpadding-top: 5rem;\r}\r.body-inner-wrapper {\rmin-height: calc(100vh - var(--GITHUB-FOOTER-HEIGHT) - 314px);\r}\r#body-inner .title, .subtitle {\rtext-align: center;\rborder: none;\rmargin-bottom: 0;\rfont-size: 3.3rem;\rpadding: 0;\rcolor: #222!important;\rfont-family: var(--font-family-regular), sans-serif!important;\r}\r#body-inner .subtitle {\rdisplay: block;\rfont-size: 2rem;\rcolor: #555;\rfont-family: var(--font-family-regular), sans-serif;\r}\r#body-inner .row:first-of-type {\rmargin-top: 50px;\r}\r#body-inner .action-header {\rdisplay:flex;\ralign-items: center;\rcolor: #222;\rfont-family: var(--font-family-regular), sans-serif;\rcursor: pointer;\rtext-decoration: none;\rmargin-bottom: 12px;\r}\r#body-inner .action-header img {\rmargin-right: 22px;\rwidth: 40px;\r}\r#body-inner .action-header span {\rfont-size: 20px;\rcolor: var(--color-emphasis);\rfont-family: var(--font-family-medium);\r}\r#body-inner .action-header+*{\rfont-size: 18px;\rline-height: 1.3;\rcolor: #333;\r}\r@media (min-width: 750px) {\r#body-inner .row {\rmargin-top: 50px;\r} }\r\rGardener Community\rFollow - Engage - Contribute\r@GardenerProject\r\rFollow the latest project updates on Twitter\r\rCommunity Meetings\r\rYou are welcome on our community meetings where you can engage with other contributors in person. See calendar for schedule or watch past recordings to get the idea.\r\rGitHub\r\rEveyone is welcome to contribute with what they can - an issue or a pull request. Check Gardener project there and our contributors guide to help you get started.\r\r\rGardener Project\r\rWatch videos and community meetings recordings on our YouTube channel\r\r#gardener\r\rDiscuss Gardener on our Slack channel in the Kubernetes workspace\r\r\rCOMMUNITY\rThe Gardener development process is an open process. Here are the general communication channels we use to communicate.\rWe work with the wider community to create a strong, vibrant codebase.\r60+\rCommitter\r\r1300+\rMerged Pull Requests\r\r1400+\rGithub Stars\r\r500+\rClosed Community Issues\r\r\rWe are cordially inviting interested parties to join our weekly meetings.\rHere you can address questions regarding the direction of the project, technical problems and support.\r\r\rOur Slack Channel is the best way to contact the experts in all questions about Kubernetes and\rthe Gardener and share your ideas with them or ask for support.\r\r\rFind out more about the project and consider making a contribution..\r\r\r\r\r"
},
{
	"uri": "https://gardener.cloud/documentation/045_contribute/10_code/15_conf_secrets/",
	"title": "Configuration and Usage",
	"tags": [],
	"description": "",
	"content": "Gardener Configuration and Usage Gardener automates the full lifecycle of Kubernetes clusters as a service. Additionally, it has several extension points allowing external controllers to plug-in to the lifecycle. As a consequence, there are several configuration options for the various custom resources that are partially required.\nThis document describes the\n configuration and usage of Gardener as operator/administrator. configuration and usage of Gardener as end-user/stakeholder/customer.  Configuration and Usage of Gardener as Operator/Administrator When we use the terms \u0026ldquo;operator/administrator\u0026rdquo; we refer to both the people deploying and operating Gardener. Gardener consists out of four components:\n gardener-apiserver, a Kubernetes-native API extension that serves custom resources in the Kubernetes-style (like Seeds and Shoots), and a component that contains multiple admission plugins. gardener-controller-manager, a component consisting out of multiple controllers that implement reconciliation and deletion flows for some of the custom resources (e.g., it contains the logic for maintaining Shoots, reconciling Plants, etc.). gardener-scheduler, a component that assigns newly created Shoot clusters to appropriate Seed clusters. gardenlet, a component running in seed clusters and consisting out of multiple controllers that implement reconciliation and deletion flows for some of the custom resources (e.g., it contains the logic for reconciliation and deletion of Shoots).  Each of these components have various configuration options. The gardener-apiserver uses the standard API server library maintained by the Kubernetes community, and as such it mainly supports command line flags. The two other components are using so-called componentconfig files that describe their configuration in a Kubernetes-style versioned object.\nConfiguration file for Gardener controller manager The Gardener controller manager does only support one command line flag which should be a path to a valid controller-manager configuration file. Please take a look at this example configuration.\nConfiguration file for Gardener scheduler The Gardener scheduler also only supports one command line flag which should be a path to a valid scheduler configuration file. Please take a look at this example configuration. Information about the concepts of the Gardener scheduler can be found here\nConfiguration file for Gardenlet The Gardenlet also only supports one command line flag which should be a path to a valid gardenlet configuration file. Please take a look at this example configuration. Information about the concepts of the Gardenlet can be found here\nSystem configuration After successful deployment of the four components you need to setup the system. Let's first focus on some \u0026ldquo;static\u0026rdquo; configuration. When the gardenlet starts it scans the garden namespace of the garden cluster for Secrets that have influence on its reconciliation loops, mainly the Shoot reconciliation:\n  Internal domain secret, contains the DNS provider credentials (having appropriate privileges) which will be used to create/delete so-called \u0026ldquo;internal\u0026rdquo; DNS records for the Shoot clusters, please see this for an example.\n This secret is used in order to establish a stable endpoint for shoot clusters which is used internally by all control plane components. The DNS records are normal DNS records but called \u0026ldquo;internal\u0026rdquo; in our scenario because only the kubeconfigs for the control plane components use this endpoint when talking to the shoot clusters. It is forbidden to change the internal domain secret if there are existing shoot clusters.    Default domain secrets (optional), contain the DNS provider credentials (having appropriate privileges) which will be used to create/delete DNS records for a default domain for shoots (e.g., example.com), please see this for an example.\n Not every end-user/stakeholder/customer has its own domain, however, Gardener needs to create a DNS record for every shoot cluster. As landscape operator you might want to define a default domain owned and controlled by you that is used for all shoot clusters that don't specify their own domain.    :warning: Please note that the mentioned domain secrets are only needed if you have at least one seed cluster that is not tainted with seed.gardener.cloud/disable-dns. Seeds with this taint don't create any DNS records for shoots scheduled on it, hence, if you only have such seeds, you don't need to create the domain secrets.\n  Alerting secrets (optional), contain the alerting configuration and credentials for the AlertManager to send email alerts. It is also possible to configure the monitoring stack to send alerts to an AlertManager not deployed by Gardener to handle alerting. Please see this for an example.\n If email alerting is configured:  An AlertManager is deployed into each seed cluster that handles the alerting for all shoots on the seed cluster. Gardener will inject the SMTP credentials into the configuration of the AlertManager. The AlertManager will send emails to the configured email address in case any alerts are firing.   If an external AlertManager is configured:  Each shoot has a Prometheus responsible for monitoring components and sending out alerts. The alerts will be sent to a URL configured in the alerting secret. This external AlertManager is not managed by Gardener and can be configured however the operator sees fit. Supported authentication types are no authentication, basic, or mutual TLS.      OpenVPN Diffie-Hellmann Key secret (optional), contains the self-generated Diffie-Hellmann key used by OpenVPN in your landscape, please see this for an example.\n If you don't specify a custom key then a default key is used, but for productive landscapes it's recommend to create a landscape-specific key and define it.    Global monitoring secrets (optional), contains basic authentication credentials for the Prometheus aggregating metrics for all clusters.\n These secrets are synced to each seed cluster and used to gain access to the aggregate monitoring components.    Apart from this \u0026ldquo;static\u0026rdquo; configuration there are several custom resources extending the Kubernetes API and used by Gardener. As an operator/administrator you have to configure some of them to make the system work.\nConfiguration and Usage of Gardener as End-User/Stakeholder/Customer As an end-user/stakeholder/customer you are using a Gardener landscape that has been setup for you by another team. You don't need to care about how Gardener itself has to be configured or how it has to be deployed. Take a look at this document - it describes which resources are offered by Gardener. You may want to have a more detailed look for Projects, SecretBindings, Shoots, Plants, and (Cluster)OpenIDConnectPresets.\n"
},
{
	"uri": "https://gardener.cloud/documentation/050-tutorials/content/howto/ssh-into-node/",
	"title": "Connecting to a worker node using SSH",
	"tags": [],
	"description": "Connecting to a worker node using SSH",
	"content": "Term clarification We are talking about connection via SSH to a node, and not open a shell in an existing pod or rather container.\nFor ways how to connect via SSH into container please check official kubernetes tutorial\nWhy When we hear this question, we respond with another question: \u0026ldquo;Why would you need to?\u0026quot;. The background of this question is that all VMs are ephemeral (cattle, no pets). Any machine can be terminated any time. When updating a cluster, this will even happen to all machines (one by one). Anyway, sometimes curiosity is the driving factor and in this case, that's a good thing.\nHow create a new file privileged-pod.yaml with the content:\napiVersion: v1\rkind: Pod\rmetadata:\rname: privileged-pod\rnamespace: default\rspec:\rcontainers:\r- name: busybox\rimage: busybox\rresources:\rlimits:\rcpu: 200m\rmemory: 100Mi\rrequests:\rcpu: 100m\rmemory: 50Mi\rstdin: true\rsecurityContext:\rprivileged: true\rvolumeMounts:\r- name: host-root-volume\rmountPath: /host\rreadOnly: true\rvolumes:\r- name: host-root-volume\rhostPath:\rpath: /\rhostNetwork: true\rhostPID: true\rrestartPolicy: Never\rkubectl create -f privileged-pod.yaml\rNow you can look around in the pod:\nkubectl exec -ti privileged-pod sh\rps aux\rip a\rls -la /host\rRun as root using the node's filesystem instead of the filesystem in the container running on the node:\nchroot /host/\rThen you can run commands such as docker ps\nDon't forget to delete your pod afterwards:\nkubectl delete pod privileged-pod\r"
},
{
	"uri": "https://gardener.cloud/documentation/050-tutorials/content/howto/missing-registry-permission/",
	"title": "Container image not pulled",
	"tags": [],
	"description": "Wrong Container Image or Invalid Registry Permissions",
	"content": "Problem Two of the most common problems are specifying the wrong container image or trying to use private images without providing registry credentials.\nNote: There is no observable difference in Pod status between a missing image and incorrect registry permissions. In either case, Kubernetes will report an ErrImagePull status for the Pods. For this reason, this article deals with both scenarios.\nExample Let's see an example. We'll create a pod named fail referencing a non-existent Docker image:\nkubectl run -i --tty fail --image=tutum/curl:1.123456\rthe command prompt doesn't return and you can press ctrl+c\nError analysis We can then inspect our Pods and see that we have one Pod with a status of ErrImagePull or ImagePullBackOff.\n$ (minikube) kubectl get pods\rNAME READY STATUS RESTARTS AGE\rclient-5b65b6c866-cs4ch 1/1 Running 1 1m\rfail-6667d7685d-7v6w8 0/1 ErrImagePull 0 \u0026lt;invalid\u0026gt;\rvuejs-578574b75f-5x98z 1/1 Running 0 1d\r$ (minikube) For some additional information, we can describe the failing Pod.\nkubectl describe pod fail-6667d7685d-7v6w8\rAs you can see in the events section, your image can't be pulled\nName:\tfail-6667d7685d-7v6w8\rNamespace:\tdefault\rNode:\tminikube/192.168.64.10\rStart Time:\tWed, 22 Nov 2017 10:01:59 +0100\rLabels:\tpod-template-hash=2223832418\rrun=fail\rAnnotations:\tkubernetes.io/created-by={\u0026quot;kind\u0026quot;:\u0026quot;SerializedReference\u0026quot;,\u0026quot;apiVersion\u0026quot;:\u0026quot;v1\u0026quot;,\u0026quot;reference\u0026quot;:{\u0026quot;kind\u0026quot;:\u0026quot;ReplicaSet\u0026quot;,\u0026quot;namespace\u0026quot;:\u0026quot;default\u0026quot;,\u0026quot;name\u0026quot;:\u0026quot;fail-6667d7685d\u0026quot;,\u0026quot;uid\u0026quot;:\u0026quot;cc4ccb3f-cf63-11e7-afca-4a7a1fa05b3f\u0026quot;,\u0026quot;a...\r.\r.\r.\r.\rEvents:\rFirstSeen\tLastSeen\tCount\tFrom\tSubObjectPath\tType\tReason\tMessage\r---------\t--------\t-----\t----\t-------------\t--------\t------\t-------\r1m\t1m\t1\tdefault-scheduler\tNormal\tScheduled\tSuccessfully assigned fail-6667d7685d-7v6w8 to minikube\r1m\t1m\t1\tkubelet, minikube\tNormal\tSuccessfulMountVolume\tMountVolume.SetUp succeeded for volume \u0026quot;default-token-9fr6r\u0026quot; 1m\t6s\t4\tkubelet, minikube\tspec.containers{fail}\tNormal\tPulling\tpulling image \u0026quot;tutum/curl:1.123456\u0026quot;\r1m\t5s\t4\tkubelet, minikube\tspec.containers{fail}\tWarning\tFailed\tFailed to pull image \u0026quot;tutum/curl:1.123456\u0026quot;: rpc error: code = Unknown desc = Error response from daemon: manifest for tutum/curl:1.123456 not found\r1m\t\u0026lt;invalid\u0026gt;\t10\tkubelet, minikube\tWarning\tFailedSync\tError syncing pod\r1m\t\u0026lt;invalid\u0026gt;\t6\tkubelet, minikube\tspec.containers{fail}\tNormal\tBackOff\tBack-off pulling image \u0026quot;tutum/curl:1.123456\u0026quot;\rWhy couldn't Kubernetes pull the image? There are three primary candidates besides network connectivity issues:\n The image tag is incorrect The image doesn't exist Kubernetes doesn't have permissions to pull that image  If you don't notice a typo in your image tag, then it's time to test using your local machine. I usually start by running docker pull on my local development machine with the exact same image tag. In this case, I would run docker pull tutum/curl:1.123456\nIf this succeeds, then it probably means that Kubernetes doesn't have correct permissions to pull that image.\nAdd the docker registry user/pwd to your cluster\nkubectl create secret docker-registry dockersecret --docker-server=https://index.docker.io/v1/ --docker-username=\u0026lt;username\u0026gt; --docker-password=\u0026lt;password\u0026gt; --docker-email=\u0026lt;email\u0026gt;\rIf the exact image tag fails, then I will test without an explicit image tag - docker pull tutum/curl - which will attempt to pull the latest tag. If this succeeds, then that means the originally specified tag doesn't exist. Go to the Docker registry and check which tags are available for this image.\nIf docker pull tutum/curl (without an exact tag) fails, then we have a bigger problem - that image does not exist at all in our image registry.\n"
},
{
	"uri": "https://gardener.cloud/documentation/050-tutorials/content/howto/image-pull-policy/",
	"title": "Container image not updating",
	"tags": [],
	"description": "Updating Images in your cluster during development",
	"content": "Preface A container image should use a fixed tag or the content hash of the image. It should not use the tags latest, head, canary, or other tags that are designed to be floating.\nProblem Many Kubernetes users have run into this problem. The story goes something like this:\n Deploy any image using an image tag (e.g. cp-enablement/awesomeapp:1.0) Fix a bug in awesomeapp Build a new image and push it with the same tag (cp-enablement/awesomeapp:1.0) Update your deployment Realize that the bug is still present Rinse and repeat steps 3 to 5 until you recognize this doesn't work  The problem relates to how Kubernetes decides whether to do a docker pull when starting a container. Since we tagged our image as :1.0, the default pull policy is IfNotPresent. The Kubelet already has a local copy of cp-enablement/awesomeapp:1.0, hence it doesn't attempt to do a docker pull. When the new Pods come up, they still use the old broken Docker image.\nThere are three ways to resolve this:\n Switch to using the tag :latest (DO NOT DO THIS!) Specify ImagePullPolicy: always (not recomended). Use unique tags (best practice)  Solution In the quest to automate myself out of a job, I created a bash script that runs anytime to create a new tag and push the build result to the registry.\n#!/usr/bin/env bash\r\r# Set the docker image name and the corresponding repository\r# Ensure that you change them in the deployment.yml as well.\r# You must be logged in with docker login…\r#\r# CHANGE THIS TO YOUR Docker.io SETTINGS\r#\rPROJECT=awesomeapp\rREPOSITORY=cp-enablement\r# exit if any subcommand or pipeline returns a non-zero status.\rset -e\r# set debug mode\r#set -x\r# build my nodeJS app\r#\rnpm run build\r# get latest version IDs from the Docker.io registry and increment them\r#\rVERSION=$(curl https://registry.hub.docker.com/v1/repositories/$REPOSITORY/$PROJECT/tags | sed -e \u0026#39;s/[][]//g\u0026#39; -e \u0026#39;s/\u0026#34;//g\u0026#39; -e \u0026#39;s/ //g\u0026#39; | tr \u0026#39;}\u0026#39; \u0026#39;\\n\u0026#39; | awk -F: \u0026#39;{print $3}\u0026#39; | grep v| tail -n 1)\rVERSION=${VERSION:1}\r((VERSION++))\rVERSION=\u0026#34;v$VERSION\u0026#34;\r# build a new docker image\r#\recho \u0026#39;\u0026gt;\u0026gt;\u0026gt; Building new image\u0026#39;\r# Due to a bug in Docker we need to analyse the log to find out if build passed (see https://github.com/dotcloud/docker/issues/1875)\rdocker build --no-cache=true -t $REPOSITORY/$PROJECT:$VERSION . | tee /tmp/docker_build_result.log\rRESULT=$(cat /tmp/docker_build_result.log | tail -n 1)\rif [[ \u0026#34;$RESULT\u0026#34; != *Successfully* ]];\rthen\rexit -1\rfi\recho \u0026#39;\u0026gt;\u0026gt;\u0026gt; Push new image\u0026#39;\rdocker push $REPOSITORY/$PROJECT:$VERSION\r"
},
{
	"uri": "https://gardener.cloud/documentation/050-tutorials/content/",
	"title": "Content",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gardener.cloud/contribute/",
	"title": "Contribute",
	"tags": [],
	"description": "",
	"content": "graph TB;\rB(Contributor)\rB -- C{I want to..}\rC --|Code| D(\"fa:fa-code-fork \u0026lt;a href\u0026#61;\u0026#39;./code\u0026#39;\u0026gt;Code Contribute\u0026lt;/a\u0026gt;\")\rC --|Docs| E(\"fa:fa-paragraph \u0026lt;a href\u0026#61;\u0026#39;./docs\u0026#39;\u0026gt;Doc Contribute\u0026lt;/a\u0026gt;\")\r\rCode of conduct All members of the Gardener community must abide by the CNCF Code of Conduct. Only by respecting each other can we develop a productive, collaborative community. Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting gardener.opensource@sap.com and/or a Gardener project maintainer.\nContributing Gardener uses GitHub to manage reviews of pull requests.\n  If you are a new contributor see: Steps to Contribute\n  If you have a trivial fix or improvement, go ahead and create a pull request, addressing (with @...) a suitable maintainer of this repository (see CODEOWNERS of the repository you want to contribute to) in the description of the pull request.\n  If you plan to do something more involved, first discuss your ideas on our mailing list. This will avoid unnecessary work and surely give you and us a good deal of inspiration.\n  Relevant coding style guidelines are the Go Code Review Comments and the Formatting and style section of Peter Bourgon's Go: Best Practices for Production Environments.\n  Steps to Contribute Should you wish to work on an issue, please claim it first by commenting on the GitHub issue that you want to work on it. This is to prevent duplicated efforts from contributors on the same issue.\nIf you have questions about one of the issues, with or without the tag, please comment on them and one of the maintainers will clarify it.\nIndividual Contributor License Agreement When you contribute (code, documentation, or anything else), be aware that we only accept contributions under the Gardener project's license (see previous sections) and you need to agree to the Individual Contributor License Agreement. This applies to all contributors, including those contributing on behalf of a company. If you agree to its content, click on the link posted by the CLA assistant as a comment to the pull request. Click it to review the CLA, then accept it on the next screen if you agree to it. CLA assistant will save your decision for upcoming contributions and will notify you if there is any change to the CLA in the meantime.\nCorporate Contributor License Agreement If employees of a company contribute code, in addition to the individual agreement above, there needs to be one company agreement submitted. This is mainly for the protection of the contributing employees.\nAn authorized company representative needs to download, fill, and print the Corporate Contributor License Agreement form. Then either:\n Scan it and e-mail it to opensource@sap.com and gardener.opensource@sap.com Fax it to: +49 6227 78-45813 Send it by letter to: Industry Standards \u0026amp; Open Source Team, Dietmar-Hopp-Allee 16, 69190 Walldorf, Germany  The form contains a list of employees who are authorized to contribute on behalf of your company. When this list changes, please let us know.\nPull Request Checklist   Branch from the master branch and, if needed, rebase to the current master branch before submitting your pull request. If it doesn't merge cleanly with master you may be asked to rebase your changes.\n  Commits should be as small as possible, while ensuring that each commit is correct independently (i.e., each commit should compile and pass tests).\n  If your patch is not getting reviewed or you need a specific person to review it, you can @-reply a reviewer asking for a review in the pull request or a comment, or you can ask for a review on our mailing list.\n  Add tests relevant to the fixed bug or new feature.\n  Issues and Planning We use GitHub issues to track bugs and enhancement requests and ZenHub for planning.\n Install the ZenHub Chrome plugin Login to ZenHub Open the Gardener ZenHub workspace  Security Release Process See Security Release Process\n"
},
{
	"uri": "https://gardener.cloud/documentation/045_contribute/10_code/10-contribution_guide/",
	"title": "Contribution Guide",
	"tags": [],
	"description": "",
	"content": "Contributing to Gardener Code of conduct All members of the Gardener community must abide by the CNCF Code of Conduct. Only by respecting each other can we develop a productive, collaborative community. Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting gardener.opensource@sap.com and/or a Gardener project maintainer.\nContributing Gardener uses GitHub to manage reviews of pull requests.\n  If you are a new contributor see: Steps to Contribute\n  If you have a trivial fix or improvement, go ahead and create a pull request, addressing (with @...) a suitable maintainer of this repository (see CODEOWNERS of the repository you want to contribute to) in the description of the pull request.\n  If you plan to do something more involved, first discuss your ideas on our mailing list. This will avoid unnecessary work and surely give you and us a good deal of inspiration.\n  Relevant coding style guidelines are the Go Code Review Comments and the Formatting and style section of Peter Bourgon's Go: Best Practices for Production Environments.\n  Steps to Contribute Should you wish to work on an issue, please claim it first by commenting on the GitHub issue that you want to work on it. This is to prevent duplicated efforts from contributors on the same issue.\nIf you have questions about one of the issues, with or without the tag, please comment on them and one of the maintainers will clarify it.\nWe kindly ask you to follow the Pull Request Checklist to ensure reviews can happen accordingly.\nContributing Code You are welcome to contribute code to Gardener in order to fix a bug or to implement a new feature.\nThe following rules govern code contributions:\n Contributions must be licensed under the Apache 2.0 License You need to sign the Contributor License Agreement. We are using CLA assistant providing a click-through workflow for accepting the CLA. For company contributors additionally the company needs to sign a corporate license agreement. See the following sections for details.  Contributing Documentation You are welcome to contribute documentation to Gardener.\nThe following rules govern documentation contributions:\n Contributions must be licensed under the Creative Commons Attribution 4.0 International License You need to sign the Contributor License Agreement. We are using CLA assistant providing a click-through workflow for accepting the CLA. For company contributors additionally the company needs to sign a corporate license agreement. See the following sections for details.  Individual Contributor License Agreement When you contribute (code, documentation, or anything else), be aware that we only accept contributions under the Gardener project's license (see previous sections) and you need to agree to the Individual Contributor License Agreement. This applies to all contributors, including those contributing on behalf of a company. If you agree to its content, click on the link posted by the CLA assistant as a comment to the pull request. Click it to review the CLA, then accept it on the next screen if you agree to it. CLA assistant will save your decision for upcoming contributions and will notify you if there is any change to the CLA in the meantime.\nCorporate Contributor License Agreement If employees of a company contribute code, in addition to the individual agreement above, there needs to be one company agreement submitted. This is mainly for the protection of the contributing employees.\nAn authorized company representative needs to download, fill, and print the Corporate Contributor License Agreement form. Then either:\n Scan it and e-mail it to opensource@sap.com and gardener.opensource@sap.com Fax it to: +49 6227 78-45813 Send it by letter to: Industry Standards \u0026amp; Open Source Team, Dietmar-Hopp-Allee 16, 69190 Walldorf, Germany  The form contains a list of employees who are authorized to contribute on behalf of your company. When this list changes, please let us know.\nPull Request Checklist   Branch from the master branch and, if needed, rebase to the current master branch before submitting your pull request. If it doesn't merge cleanly with master you may be asked to rebase your changes.\n  Commits should be as small as possible, while ensuring that each commit is correct independently (i.e., each commit should compile and pass tests).\n  Test your changes as thoroughly as possible before your commit them. Preferably, automate your test by unit / integration (e.g. Gardener integration testing) tests. If tested manually, provide information about the test scope in the PR description (e.g. “Test passed: Upgrade K8s version from 1.14.5 to 1.15.2 on AWS, Azure, GCP, Alicloud, Openstack.”).\n  Create Work In Progress [WIP] pull requests only if you need a clarification or an explicit review before you can continue your work item.\n  If your patch is not getting reviewed or you need a specific person to review it, you can @-reply a reviewer asking for a review in the pull request or a comment, or you can ask for a review on our mailing list.\n  Post review:\n If a review requires you to change your commit(s), please test the changes again. Amend the affected commit(s) and force push onto your branch. Set respective comments in your GitHub review to resolved. Create a general PR comment to notify the reviewers that your amendments are ready for another round of review.    Issues and Planning We use GitHub issues to track bugs and enhancement requests. Please provide as much context as possible when you open an issue. The information you provide must be comprehensive enough to reproduce that issue for the assignee. Therefore, contributors may use but aren't restricted to the issue template provided by the Gardener maintainers.\nZenHub is used for planning:\n Install the ZenHub Chrome plugin Login to ZenHub Open the Gardener ZenHub workspace  Security Release Process See Security Release Process\nCommunity Slack Channel #gardener, sign up here\nMailing List gardener@googlegroups.com\nThe mailing list is hosted through Google Groups. To receive the lists\u0026rsquo; emails, join the group, as you would any other Google Group.\nTwitter Follow @GardenerProject on Twitter. Please mention @GardenerProject in your own posts about Gardener.\nAccessing community documents In order to foster real time collaboration there are working documents and notes that are taken in Google Docs, and then transferred to this repository if appropriate.\nTo gain edit access for these documents, you must subscribe to the gardener mailing list, as these documents are shared automatically with anyone who subscribes to that list.\nWeekly Meeting We have a PUBLIC and RECORDED weekly meeting. We meet every Friday at 10:00 CET over Zoom. Find recordings in the Gardener Youtube channel. Let us know if you want to participate and live in a timezone where 10:00 CET is in the night, we can also schedule meetings on Thursday 17:00 CET.\nSee the meeting calendar on the web at calendar.google.com, or paste this iCal url into any iCal client.\nIf you have a topic you'd like to present or would like to see discussed, please propose a specific date on the Gardener Community Meeting Agenda. Find minutes in the same document. Please upload slides or other documents you presented to the Gardener Community Meeting folder. Subscribe to the gardener mailing list to get edit permissions.\n"
},
{
	"uri": "https://gardener.cloud/blog/2018_week_51/",
	"title": "Cookies are dangerous...",
	"tags": [],
	"description": "",
	"content": "\u0026hellip;they mess up the figure.\nFor a team event during the Christmas season we decided to completely reinterpret the topic cookies\u0026hellip; since the vegetables have gone on a well-deserved vacation. :-)\nGet recipe on Gardener Cookies.\n"
},
{
	"uri": "https://gardener.cloud/api-reference/core/",
	"title": "Core",
	"tags": [],
	"description": "",
	"content": "Packages:\n\r\rcore.gardener.cloud/v1beta1\r\r\rcore.gardener.cloud/v1beta1\r\rPackage v1beta1 is a version of the API.\nResource Types:\r\rBackupBucket\r\rBackupEntry\r\rCloudProfile\r\rControllerInstallation\r\rControllerRegistration\r\rPlant\r\rProject\r\rQuota\r\rSecretBinding\r\rSeed\r\rShoot\r\rBackupBucket\r\r\rBackupBucket holds details about backup bucket\n\r\r\rField\rDescription\r\r\r\r\r\rapiVersion\rstring\r\r\rcore.gardener.cloud/v1beta1\r\r\r\r\r\rkind\rstring\r\rBackupBucket\r\r\r\rmetadata\r\rKubernetes meta/v1.ObjectMeta\r\r\r\r\rStandard object metadata.\nRefer to the Kubernetes API documentation for the fields of the\rmetadata field.\r\r\r\r\rspec\r\rBackupBucketSpec\r\r\r\r\rSpecification of the Backup Bucket.\n\r\r\r\r\rprovider\r\rBackupBucketProvider\r\r\r\r\rProvider hold the details of cloud provider of the object store.\n\r\r\r\rproviderConfig\r\rProviderConfig\r\r\r\r\r(Optional)\rProviderConfig is the configuration passed to BackupBucket resource.\n\r\r\r\rsecretRef\r\rKubernetes core/v1.SecretReference\r\r\r\r\rSecretRef is a reference to a secret that contains the credentials to access object store.\n\r\r\r\rseedName\r\rstring\r\r\r\r(Optional)\rSeedName holds the name of the seed allocated to BackupBucket for running controller.\n\r\r\r\r\r\r\rstatus\r\rBackupBucketStatus\r\r\r\r\rMost recently observed status of the Backup Bucket.\n\r\r\r\rBackupEntry\r\r\rBackupEntry holds details about shoot backup.\n\r\r\rField\rDescription\r\r\r\r\r\rapiVersion\rstring\r\r\rcore.gardener.cloud/v1beta1\r\r\r\r\r\rkind\rstring\r\rBackupEntry\r\r\r\rmetadata\r\rKubernetes meta/v1.ObjectMeta\r\r\r\r\rStandard object metadata.\nRefer to the Kubernetes API documentation for the fields of the\rmetadata field.\r\r\r\r\rspec\r\rBackupEntrySpec\r\r\r\r\r(Optional)\rSpec contains the specification of the Backup Entry.\n\r\r\r\r\rbucketName\r\rstring\r\r\r\rBucketName is the name of backup bucket for this Backup Entry.\n\r\r\r\rseedName\r\rstring\r\r\r\r(Optional)\rSeedName holds the name of the seed allocated to BackupEntry for running controller.\n\r\r\r\r\r\r\rstatus\r\rBackupEntryStatus\r\r\r\r\r(Optional)\rStatus contains the most recently observed status of the Backup Entry.\n\r\r\r\rCloudProfile\r\r\rCloudProfile represents certain properties about a provider environment.\n\r\r\rField\rDescription\r\r\r\r\r\rapiVersion\rstring\r\r\rcore.gardener.cloud/v1beta1\r\r\r\r\r\rkind\rstring\r\rCloudProfile\r\r\r\rmetadata\r\rKubernetes meta/v1.ObjectMeta\r\r\r\r\r(Optional)\rStandard object metadata.\nRefer to the Kubernetes API documentation for the fields of the\rmetadata field.\r\r\r\r\rspec\r\rCloudProfileSpec\r\r\r\r\r(Optional)\rSpec defines the provider environment properties.\n\r\r\r\r\rcaBundle\r\rstring\r\r\r\r(Optional)\rCABundle is a certificate bundle which will be installed onto every host machine of shoot cluster targeting this profile.\n\r\r\r\rkubernetes\r\rKubernetesSettings\r\r\r\r\rKubernetes contains constraints regarding allowed values of the \u0026lsquo;kubernetes\u0026rsquo; block in the Shoot specification.\n\r\r\r\rmachineImages\r\r[]MachineImage\r\r\r\r\rMachineImages contains constraints regarding allowed values for machine images in the Shoot specification.\n\r\r\r\rmachineTypes\r\r[]MachineType\r\r\r\r\rMachineTypes contains constraints regarding allowed values for machine types in the \u0026lsquo;workers\u0026rsquo; block in the Shoot specification.\n\r\r\r\rproviderConfig\r\rProviderConfig\r\r\r\r\r(Optional)\rProviderConfig contains provider-specific configuration for the profile.\n\r\r\r\rregions\r\r[]Region\r\r\r\r\rRegions contains constraints regarding allowed values for regions and zones.\n\r\r\r\rseedSelector\r\rKubernetes meta/v1.LabelSelector\r\r\r\r\r(Optional)\rSeedSelector contains an optional list of labels on Seed resources that marks those seeds whose shoots may use this provider profile.\rAn empty list means that all seeds of the same provider type are supported.\rThis is useful for environments that are of the same type (like openstack) but may have different \u0026ldquo;instances\u0026rdquo;/landscapes.\n\r\r\r\rtype\r\rstring\r\r\r\rType is the name of the provider.\n\r\r\r\rvolumeTypes\r\r[]VolumeType\r\r\r\r\r(Optional)\rVolumeTypes contains constraints regarding allowed values for volume types in the \u0026lsquo;workers\u0026rsquo; block in the Shoot specification.\n\r\r\r\r\r\r\rControllerInstallation\r\r\rControllerInstallation represents an installation request for an external controller.\n\r\r\rField\rDescription\r\r\r\r\r\rapiVersion\rstring\r\r\rcore.gardener.cloud/v1beta1\r\r\r\r\r\rkind\rstring\r\rControllerInstallation\r\r\r\rmetadata\r\rKubernetes meta/v1.ObjectMeta\r\r\r\r\rStandard object metadata.\nRefer to the Kubernetes API documentation for the fields of the\rmetadata field.\r\r\r\r\rspec\r\rControllerInstallationSpec\r\r\r\r\rSpec contains the specification of this installation.\n\r\r\r\r\rregistrationRef\r\rKubernetes core/v1.ObjectReference\r\r\r\r\rRegistrationRef is used to reference a ControllerRegistration resources.\n\r\r\r\rseedRef\r\rKubernetes core/v1.ObjectReference\r\r\r\r\rSeedRef is used to reference a Seed resources.\n\r\r\r\r\r\r\rstatus\r\rControllerInstallationStatus\r\r\r\r\rStatus contains the status of this installation.\n\r\r\r\rControllerRegistration\r\r\rControllerRegistration represents a registration of an external controller.\n\r\r\rField\rDescription\r\r\r\r\r\rapiVersion\rstring\r\r\rcore.gardener.cloud/v1beta1\r\r\r\r\r\rkind\rstring\r\rControllerRegistration\r\r\r\rmetadata\r\rKubernetes meta/v1.ObjectMeta\r\r\r\r\rStandard object metadata.\nRefer to the Kubernetes API documentation for the fields of the\rmetadata field.\r\r\r\r\rspec\r\rControllerRegistrationSpec\r\r\r\r\rSpec contains the specification of this registration.\n\r\r\r\r\rresources\r\r[]ControllerResource\r\r\r\r\rResources is a list of combinations of kinds (DNSProvider, Infrastructure, Generic, \u0026hellip;) and their actual types\r(aws-route53, gcp, auditlog, \u0026hellip;).\n\r\r\r\rdeployment\r\rControllerDeployment\r\r\r\r\r(Optional)\rDeployment contains information for how this controller is deployed.\n\r\r\r\r\r\r\rPlant\r\r\r\r\r\rField\rDescription\r\r\r\r\r\rapiVersion\rstring\r\r\rcore.gardener.cloud/v1beta1\r\r\r\r\r\rkind\rstring\r\rPlant\r\r\r\rmetadata\r\rKubernetes meta/v1.ObjectMeta\r\r\r\r\r(Optional)\rStandard object metadata.\nRefer to the Kubernetes API documentation for the fields of the\rmetadata field.\r\r\r\r\rspec\r\rPlantSpec\r\r\r\r\rSpec contains the specification of this Plant.\n\r\r\r\r\rsecretRef\r\rKubernetes core/v1.LocalObjectReference\r\r\r\r\rSecretRef is a reference to a Secret object containing the Kubeconfig of the external kubernetes\rclusters to be added to Gardener.\n\r\r\r\rendpoints\r\r[]Endpoint\r\r\r\r\r(Optional)\rEndpoints is the configuration plant endpoints\n\r\r\r\r\r\r\rstatus\r\rPlantStatus\r\r\r\r\rStatus contains the status of this Plant.\n\r\r\r\rProject\r\r\rProject holds certain properties about a Gardener project.\n\r\r\rField\rDescription\r\r\r\r\r\rapiVersion\rstring\r\r\rcore.gardener.cloud/v1beta1\r\r\r\r\r\rkind\rstring\r\rProject\r\r\r\rmetadata\r\rKubernetes meta/v1.ObjectMeta\r\r\r\r\r(Optional)\rStandard object metadata.\nRefer to the Kubernetes API documentation for the fields of the\rmetadata field.\r\r\r\r\rspec\r\rProjectSpec\r\r\r\r\r(Optional)\rSpec defines the project properties.\n\r\r\r\r\rcreatedBy\r\rKubernetes rbac/v1.Subject\r\r\r\r\r(Optional)\rCreatedBy is a subject representing a user name, an email address, or any other identifier of a user\rwho created the project.\n\r\r\r\rdescription\r\rstring\r\r\r\r(Optional)\rDescription is a human-readable description of what the project is used for.\n\r\r\r\rowner\r\rKubernetes rbac/v1.Subject\r\r\r\r\r(Optional)\rOwner is a subject representing a user name, an email address, or any other identifier of a user owning\rthe project.\rIMPORTANT: Be aware that this field will be removed in the v1 version of this API in favor of the owner\rrole. The only way to change the owner will be by moving the owner role. In this API version the only way\rto change the owner is to use this field.\rTODO: Remove this field in favor of the owner role in v1.\n\r\r\r\rpurpose\r\rstring\r\r\r\r(Optional)\rPurpose is a human-readable explanation of the project\u0026rsquo;s purpose.\n\r\r\r\rmembers\r\r[]ProjectMember\r\r\r\r\r(Optional)\rMembers is a list of subjects representing a user name, an email address, or any other identifier of a user,\rgroup, or service account that has a certain role.\n\r\r\r\rnamespace\r\rstring\r\r\r\r(Optional)\rNamespace is the name of the namespace that has been created for the Project object.\rA nil value means that Gardener will determine the name of the namespace.\n\r\r\r\r\r\r\rstatus\r\rProjectStatus\r\r\r\r\r(Optional)\rMost recently observed status of the Project.\n\r\r\r\rQuota\r\r\r\r\r\rField\rDescription\r\r\r\r\r\rapiVersion\rstring\r\r\rcore.gardener.cloud/v1beta1\r\r\r\r\r\rkind\rstring\r\rQuota\r\r\r\rmetadata\r\rKubernetes meta/v1.ObjectMeta\r\r\r\r\r(Optional)\rStandard object metadata.\nRefer to the Kubernetes API documentation for the fields of the\rmetadata field.\r\r\r\r\rspec\r\rQuotaSpec\r\r\r\r\r(Optional)\rSpec defines the Quota constraints.\n\r\r\r\r\rclusterLifetimeDays\r\rint32\r\r\r\r(Optional)\rClusterLifetimeDays is the lifetime of a Shoot cluster in days before it will be terminated automatically.\n\r\r\r\rmetrics\r\rKubernetes core/v1.ResourceList\r\r\r\r\rMetrics is a list of resources which will be put under constraints.\n\r\r\r\rscope\r\rKubernetes core/v1.ObjectReference\r\r\r\r\rScope is the scope of the Quota object, either \u0026lsquo;project\u0026rsquo; or \u0026lsquo;secret\u0026rsquo;.\n\r\r\r\r\r\r\rSecretBinding\r\r\r\r\r\rField\rDescription\r\r\r\r\r\rapiVersion\rstring\r\r\rcore.gardener.cloud/v1beta1\r\r\r\r\r\rkind\rstring\r\rSecretBinding\r\r\r\rmetadata\r\rKubernetes meta/v1.ObjectMeta\r\r\r\r\r(Optional)\rStandard object metadata.\nRefer to the Kubernetes API documentation for the fields of the\rmetadata field.\r\r\r\r\rsecretRef\r\rKubernetes core/v1.SecretReference\r\r\r\r\rSecretRef is a reference to a secret object in the same or another namespace.\n\r\r\r\rquotas\r\r[]Kubernetes core/v1.ObjectReference\r\r\r\r\r(Optional)\rQuotas is a list of references to Quota objects in the same or another namespace.\n\r\r\r\rSeed\r\r\rSeed represents an installation request for an external controller.\n\r\r\rField\rDescription\r\r\r\r\r\rapiVersion\rstring\r\r\rcore.gardener.cloud/v1beta1\r\r\r\r\r\rkind\rstring\r\rSeed\r\r\r\rmetadata\r\rKubernetes meta/v1.ObjectMeta\r\r\r\r\rStandard object metadata.\nRefer to the Kubernetes API documentation for the fields of the\rmetadata field.\r\r\r\r\rspec\r\rSeedSpec\r\r\r\r\rSpec contains the specification of this installation.\n\r\r\r\r\rbackup\r\rSeedBackup\r\r\r\r\r(Optional)\rBackup holds the object store configuration for the backups of shoot (currently only etcd).\rIf it is not specified, then there won\u0026rsquo;t be any backups taken for shoots associated with this seed.\rIf backup field is present in seed, then backups of the etcd from shoot control plane will be stored\runder the configured object store.\n\r\r\r\rdns\r\rSeedDNS\r\r\r\r\rDNS contains DNS-relevant information about this seed cluster.\n\r\r\r\rnetworks\r\rSeedNetworks\r\r\r\r\rNetworks defines the pod, service and worker network of the Seed cluster.\n\r\r\r\rprovider\r\rSeedProvider\r\r\r\r\rProvider defines the provider type and region for this Seed cluster.\n\r\r\r\rsecretRef\r\rKubernetes core/v1.SecretReference\r\r\r\r\r(Optional)\rSecretRef is a reference to a Secret object containing the Kubeconfig and the cloud provider credentials for\rthe account the Seed cluster has been deployed to.\n\r\r\r\rtaints\r\r[]SeedTaint\r\r\r\r\r(Optional)\rTaints describes taints on the seed.\n\r\r\r\rvolume\r\rSeedVolume\r\r\r\r\r(Optional)\rVolume contains settings for persistentvolumes created in the seed cluster.\n\r\r\r\r\r\r\rstatus\r\rSeedStatus\r\r\r\r\rStatus contains the status of this installation.\n\r\r\r\rShoot\r\r\r\r\r\rField\rDescription\r\r\r\r\r\rapiVersion\rstring\r\r\rcore.gardener.cloud/v1beta1\r\r\r\r\r\rkind\rstring\r\rShoot\r\r\r\rmetadata\r\rKubernetes meta/v1.ObjectMeta\r\r\r\r\r(Optional)\rStandard object metadata.\nRefer to the Kubernetes API documentation for the fields of the\rmetadata field.\r\r\r\r\rspec\r\rShootSpec\r\r\r\r\r(Optional)\rSpecification of the Shoot cluster.\n\r\r\r\r\raddons\r\rAddons\r\r\r\r\r(Optional)\rAddons contains information about enabled/disabled addons and their configuration.\n\r\r\r\rcloudProfileName\r\rstring\r\r\r\rCloudProfileName is a name of a CloudProfile object.\n\r\r\r\rdns\r\rDNS\r\r\r\r\r(Optional)\rDNS contains information about the DNS settings of the Shoot.\n\r\r\r\rextensions\r\r[]Extension\r\r\r\r\r(Optional)\rExtensions contain type and provider information for Shoot extensions.\n\r\r\r\rhibernation\r\rHibernation\r\r\r\r\r(Optional)\rHibernation contains information whether the Shoot is suspended or not.\n\r\r\r\rkubernetes\r\rKubernetes\r\r\r\r\rKubernetes contains the version and configuration settings of the control plane components.\n\r\r\r\rnetworking\r\rNetworking\r\r\r\r\rNetworking contains information about cluster networking such as CNI Plugin type, CIDRs, \u0026hellip;etc.\n\r\r\r\rmaintenance\r\rMaintenance\r\r\r\r\r(Optional)\rMaintenance contains information about the time window for maintenance operations and which\roperations should be performed.\n\r\r\r\rmonitoring\r\rMonitoring\r\r\r\r\r(Optional)\rMonitoring contains information about custom monitoring configurations for the shoot.\n\r\r\r\rprovider\r\rProvider\r\r\r\r\rProvider contains all provider-specific and provider-relevant information.\n\r\r\r\rpurpose\r\rShootPurpose\r\r\r\r\r(Optional)\rPurpose is the purpose class for this cluster.\n\r\r\r\rregion\r\rstring\r\r\r\rRegion is a name of a region.\n\r\r\r\rsecretBindingName\r\rstring\r\r\r\rSecretBindingName is the name of the a SecretBinding that has a reference to the provider secret.\rThe credentials inside the provider secret will be used to create the shoot in the respective account.\n\r\r\r\rseedName\r\rstring\r\r\r\r(Optional)\rSeedName is the name of the seed cluster that runs the control plane of the Shoot.\n\r\r\r\r\r\r\rstatus\r\rShootStatus\r\r\r\r\r(Optional)\rMost recently observed status of the Shoot cluster.\n\r\r\r\rAddon\r\r\r(Appears on:\rKubernetesDashboard, NginxIngress)\r\rAddon allows enabling or disabling a specific addon and is used to derive from.\n\r\r\rField\rDescription\r\r\r\r\r\renabled\r\rbool\r\r\r\rEnabled indicates whether the addon is enabled or not.\n\r\r\r\rAddons\r\r\r(Appears on:\rShootSpec)\r\rAddons is a collection of configuration for specific addons which are managed by the Gardener.\n\r\r\rField\rDescription\r\r\r\r\r\rkubernetesDashboard\r\rKubernetesDashboard\r\r\r\r\r(Optional)\rKubernetesDashboard holds configuration settings for the kubernetes dashboard addon.\n\r\r\r\rnginxIngress\r\rNginxIngress\r\r\r\r\r(Optional)\rNginxIngress holds configuration settings for the nginx-ingress addon.\n\r\r\r\rAdmissionPlugin\r\r\r(Appears on:\rKubeAPIServerConfig)\r\rAdmissionPlugin contains information about a specific admission plugin and its corresponding configuration.\n\r\r\rField\rDescription\r\r\r\r\r\rname\r\rstring\r\r\r\rName is the name of the plugin.\n\r\r\r\rconfig\r\rProviderConfig\r\r\r\r\r(Optional)\rConfig is the configuration of the plugin.\n\r\r\r\rAlerting\r\r\r(Appears on:\rMonitoring)\r\rAlerting contains information about how alerting will be done (i.e. who will receive alerts and how).\n\r\r\rField\rDescription\r\r\r\r\r\remailReceivers\r\r[]string\r\r\r\r(Optional)\rMonitoringEmailReceivers is a list of recipients for alerts\n\r\r\r\rAuditConfig\r\r\r(Appears on:\rKubeAPIServerConfig)\r\rAuditConfig contains settings for audit of the api server\n\r\r\rField\rDescription\r\r\r\r\r\rauditPolicy\r\rAuditPolicy\r\r\r\r\r(Optional)\rAuditPolicy contains configuration settings for audit policy of the kube-apiserver.\n\r\r\r\rAuditPolicy\r\r\r(Appears on:\rAuditConfig)\r\rAuditPolicy contains audit policy for kube-apiserver\n\r\r\rField\rDescription\r\r\r\r\r\rconfigMapRef\r\rKubernetes core/v1.ObjectReference\r\r\r\r\r(Optional)\rConfigMapRef is a reference to a ConfigMap object in the same namespace,\rwhich contains the audit policy for the kube-apiserver.\n\r\r\r\rAvailabilityZone\r\r\r(Appears on:\rRegion)\r\rAvailabilityZone is an availability zone.\n\r\r\rField\rDescription\r\r\r\r\r\rname\r\rstring\r\r\r\rName is an an availability zone name.\n\r\r\r\runavailableMachineTypes\r\r[]string\r\r\r\r(Optional)\rUnavailableMachineTypes is a list of machine type names that are not availability in this zone.\n\r\r\r\runavailableVolumeTypes\r\r[]string\r\r\r\r(Optional)\rUnavailableVolumeTypes is a list of volume type names that are not availability in this zone.\n\r\r\r\rBackupBucketProvider\r\r\r(Appears on:\rBackupBucketSpec)\r\rBackupBucketProvider holds the details of cloud provider of the object store.\n\r\r\rField\rDescription\r\r\r\r\r\rtype\r\rstring\r\r\r\rType is the type of provider.\n\r\r\r\rregion\r\rstring\r\r\r\rRegion is the region of the bucket.\n\r\r\r\rBackupBucketSpec\r\r\r(Appears on:\rBackupBucket)\r\rBackupBucketSpec is the specification of a Backup Bucket.\n\r\r\rField\rDescription\r\r\r\r\r\rprovider\r\rBackupBucketProvider\r\r\r\r\rProvider hold the details of cloud provider of the object store.\n\r\r\r\rproviderConfig\r\rProviderConfig\r\r\r\r\r(Optional)\rProviderConfig is the configuration passed to BackupBucket resource.\n\r\r\r\rsecretRef\r\rKubernetes core/v1.SecretReference\r\r\r\r\rSecretRef is a reference to a secret that contains the credentials to access object store.\n\r\r\r\rseedName\r\rstring\r\r\r\r(Optional)\rSeedName holds the name of the seed allocated to BackupBucket for running controller.\n\r\r\r\rBackupBucketStatus\r\r\r(Appears on:\rBackupBucket)\r\rBackupBucketStatus holds the most recently observed status of the Backup Bucket.\n\r\r\rField\rDescription\r\r\r\r\r\rproviderStatus\r\rProviderConfig\r\r\r\r\r(Optional)\rProviderStatus is the configuration passed to BackupBucket resource.\n\r\r\r\rlastOperation\r\rLastOperation\r\r\r\r\r(Optional)\rLastOperation holds information about the last operation on the BackupBucket.\n\r\r\r\rlastError\r\rLastError\r\r\r\r\r(Optional)\rLastError holds information about the last occurred error during an operation.\n\r\r\r\robservedGeneration\r\rint64\r\r\r\r(Optional)\rObservedGeneration is the most recent generation observed for this BackupBucket. It corresponds to the\rBackupBucket\u0026rsquo;s generation, which is updated on mutation by the API Server.\n\r\r\r\rgeneratedSecretRef\r\rKubernetes core/v1.SecretReference\r\r\r\r\r(Optional)\rGeneratedSecretRef is reference to the secret generated by backup bucket, which\rwill have object store specific credentials.\n\r\r\r\rBackupEntrySpec\r\r\r(Appears on:\rBackupEntry)\r\rBackupEntrySpec is the specification of a Backup Entry.\n\r\r\rField\rDescription\r\r\r\r\r\rbucketName\r\rstring\r\r\r\rBucketName is the name of backup bucket for this Backup Entry.\n\r\r\r\rseedName\r\rstring\r\r\r\r(Optional)\rSeedName holds the name of the seed allocated to BackupEntry for running controller.\n\r\r\r\rBackupEntryStatus\r\r\r(Appears on:\rBackupEntry)\r\rBackupEntryStatus holds the most recently observed status of the Backup Entry.\n\r\r\rField\rDescription\r\r\r\r\r\rlastOperation\r\rLastOperation\r\r\r\r\r(Optional)\rLastOperation holds information about the last operation on the BackupEntry.\n\r\r\r\rlastError\r\rLastError\r\r\r\r\r(Optional)\rLastError holds information about the last occurred error during an operation.\n\r\r\r\robservedGeneration\r\rint64\r\r\r\r(Optional)\rObservedGeneration is the most recent generation observed for this BackupEntry. It corresponds to the\rBackupEntry\u0026rsquo;s generation, which is updated on mutation by the API Server.\n\r\r\r\rCRI\r\r\r(Appears on:\rWorker)\r\rCRI contains information about the Container Runtimes.\n\r\r\rField\rDescription\r\r\r\r\r\rname\r\rCRIName\r\r\r\r\rThe name of the CRI library\n\r\r\r\rcontainerRuntimes\r\r[]ContainerRuntime\r\r\r\r\r(Optional)\rContainerRuntimes is the list of the required container runtimes supported for a worker pool.\n\r\r\r\rCRIName\r(string alias)\n\r\r(Appears on:\rCRI)\r\rCRIName is a type alias for the CRI name string.\nCloudInfo\r\r\r(Appears on:\rClusterInfo)\r\rCloudInfo contains information about the cloud\n\r\r\rField\rDescription\r\r\r\r\r\rtype\r\rstring\r\r\r\rType is the cloud type\n\r\r\r\rregion\r\rstring\r\r\r\rRegion is the cloud region\n\r\r\r\rCloudProfileSpec\r\r\r(Appears on:\rCloudProfile)\r\rCloudProfileSpec is the specification of a CloudProfile.\rIt must contain exactly one of its defined keys.\n\r\r\rField\rDescription\r\r\r\r\r\rcaBundle\r\rstring\r\r\r\r(Optional)\rCABundle is a certificate bundle which will be installed onto every host machine of shoot cluster targeting this profile.\n\r\r\r\rkubernetes\r\rKubernetesSettings\r\r\r\r\rKubernetes contains constraints regarding allowed values of the \u0026lsquo;kubernetes\u0026rsquo; block in the Shoot specification.\n\r\r\r\rmachineImages\r\r[]MachineImage\r\r\r\r\rMachineImages contains constraints regarding allowed values for machine images in the Shoot specification.\n\r\r\r\rmachineTypes\r\r[]MachineType\r\r\r\r\rMachineTypes contains constraints regarding allowed values for machine types in the \u0026lsquo;workers\u0026rsquo; block in the Shoot specification.\n\r\r\r\rproviderConfig\r\rProviderConfig\r\r\r\r\r(Optional)\rProviderConfig contains provider-specific configuration for the profile.\n\r\r\r\rregions\r\r[]Region\r\r\r\r\rRegions contains constraints regarding allowed values for regions and zones.\n\r\r\r\rseedSelector\r\rKubernetes meta/v1.LabelSelector\r\r\r\r\r(Optional)\rSeedSelector contains an optional list of labels on Seed resources that marks those seeds whose shoots may use this provider profile.\rAn empty list means that all seeds of the same provider type are supported.\rThis is useful for environments that are of the same type (like openstack) but may have different \u0026ldquo;instances\u0026rdquo;/landscapes.\n\r\r\r\rtype\r\rstring\r\r\r\rType is the name of the provider.\n\r\r\r\rvolumeTypes\r\r[]VolumeType\r\r\r\r\r(Optional)\rVolumeTypes contains constraints regarding allowed values for volume types in the \u0026lsquo;workers\u0026rsquo; block in the Shoot specification.\n\r\r\r\rClusterAutoscaler\r\r\r(Appears on:\rKubernetes)\r\rClusterAutoscaler contains the configration flags for the Kubernetes cluster autoscaler.\n\r\r\rField\rDescription\r\r\r\r\r\rscaleDownDelayAfterAdd\r\rKubernetes meta/v1.Duration\r\r\r\r\r(Optional)\rScaleDownDelayAfterAdd defines how long after scale up that scale down evaluation resumes (default: 1 hour).\n\r\r\r\rscaleDownDelayAfterDelete\r\rKubernetes meta/v1.Duration\r\r\r\r\r(Optional)\rScaleDownDelayAfterDelete how long after node deletion that scale down evaluation resumes, defaults to scanInterval (defaults to ScanInterval).\n\r\r\r\rscaleDownDelayAfterFailure\r\rKubernetes meta/v1.Duration\r\r\r\r\r(Optional)\rScaleDownDelayAfterFailure how long after scale down failure that scale down evaluation resumes (default: 3 mins).\n\r\r\r\rscaleDownUnneededTime\r\rKubernetes meta/v1.Duration\r\r\r\r\r(Optional)\rScaleDownUnneededTime defines how long a node should be unneeded before it is eligible for scale down (default: 30 mins).\n\r\r\r\rscaleDownUtilizationThreshold\r\rfloat64\r\r\r\r(Optional)\rScaleDownUtilizationThreshold defines the threshold in % under which a node is being removed\n\r\r\r\rscanInterval\r\rKubernetes meta/v1.Duration\r\r\r\r\r(Optional)\rScanInterval how often cluster is reevaluated for scale up or down (default: 10 secs).\n\r\r\r\rClusterInfo\r\r\r(Appears on:\rPlantStatus)\r\rClusterInfo contains information about the Plant cluster\n\r\r\rField\rDescription\r\r\r\r\r\rcloud\r\rCloudInfo\r\r\r\r\rCloud describes the cloud information\n\r\r\r\rkubernetes\r\rKubernetesInfo\r\r\r\r\rKubernetes describes kubernetes meta information (e.g., version)\n\r\r\r\rCondition\r\r\r(Appears on:\rControllerInstallationStatus, PlantStatus, SeedStatus, ShootStatus)\r\rCondition holds the information about the state of a resource.\n\r\r\rField\rDescription\r\r\r\r\r\rtype\r\rConditionType\r\r\r\r\rType of the Shoot condition.\n\r\r\r\rstatus\r\rConditionStatus\r\r\r\r\rStatus of the condition, one of True, False, Unknown.\n\r\r\r\rlastTransitionTime\r\rKubernetes meta/v1.Time\r\r\r\r\rLast time the condition transitioned from one status to another.\n\r\r\r\rlastUpdateTime\r\rKubernetes meta/v1.Time\r\r\r\r\rLast time the condition was updated.\n\r\r\r\rreason\r\rstring\r\r\r\rThe reason for the condition\u0026rsquo;s last transition.\n\r\r\r\rmessage\r\rstring\r\r\r\rA human readable message indicating details about the transition.\n\r\r\r\rcodes\r\r[]ErrorCode\r\r\r\r\r(Optional)\rWell-defined error codes in case the condition reports a problem.\n\r\r\r\rConditionStatus\r(string alias)\n\r\r(Appears on:\rCondition)\r\rConditionStatus is the status of a condition.\nConditionType\r(string alias)\n\r\r(Appears on:\rCondition)\r\rConditionType is a string alias.\nContainerRuntime\r\r\r(Appears on:\rCRI)\r\rContainerRuntime contains information about worker\u0026rsquo;s available container runtime\n\r\r\rField\rDescription\r\r\r\r\r\rtype\r\rstring\r\r\r\rType is the type of the Container Runtime.\n\r\r\r\rproviderConfig\r\rProviderConfig\r\r\r\r\r(Optional)\rProviderConfig is the configuration passed to container runtime resource.\n\r\r\r\rControllerDeployment\r\r\r(Appears on:\rControllerRegistrationSpec)\r\rControllerDeployment contains information for how this controller is deployed.\n\r\r\rField\rDescription\r\r\r\r\r\rtype\r\rstring\r\r\r\rType is the deployment type.\n\r\r\r\rproviderConfig\r\rProviderConfig\r\r\r\r\r(Optional)\rProviderConfig contains type-specific configuration.\n\r\r\r\rControllerInstallationSpec\r\r\r(Appears on:\rControllerInstallation)\r\rControllerInstallationSpec is the specification of a ControllerInstallation.\n\r\r\rField\rDescription\r\r\r\r\r\rregistrationRef\r\rKubernetes core/v1.ObjectReference\r\r\r\r\rRegistrationRef is used to reference a ControllerRegistration resources.\n\r\r\r\rseedRef\r\rKubernetes core/v1.ObjectReference\r\r\r\r\rSeedRef is used to reference a Seed resources.\n\r\r\r\rControllerInstallationStatus\r\r\r(Appears on:\rControllerInstallation)\r\rControllerInstallationStatus is the status of a ControllerInstallation.\n\r\r\rField\rDescription\r\r\r\r\r\rconditions\r\r[]Condition\r\r\r\r\r(Optional)\rConditions represents the latest available observations of a ControllerInstallations\u0026rsquo;s current state.\n\r\r\r\rproviderStatus\r\rProviderConfig\r\r\r\r\r(Optional)\rProviderStatus contains type-specific status.\n\r\r\r\rControllerRegistrationSpec\r\r\r(Appears on:\rControllerRegistration)\r\rControllerRegistrationSpec is the specification of a ControllerRegistration.\n\r\r\rField\rDescription\r\r\r\r\r\rresources\r\r[]ControllerResource\r\r\r\r\rResources is a list of combinations of kinds (DNSProvider, Infrastructure, Generic, \u0026hellip;) and their actual types\r(aws-route53, gcp, auditlog, \u0026hellip;).\n\r\r\r\rdeployment\r\rControllerDeployment\r\r\r\r\r(Optional)\rDeployment contains information for how this controller is deployed.\n\r\r\r\rControllerResource\r\r\r(Appears on:\rControllerRegistrationSpec)\r\rControllerResource is a combination of a kind (DNSProvider, Infrastructure, Generic, \u0026hellip;) and the actual type for this\rkind (aws-route53, gcp, auditlog, \u0026hellip;).\n\r\r\rField\rDescription\r\r\r\r\r\rkind\r\rstring\r\r\r\rKind is the resource kind, for example \u0026ldquo;OperatingSystemConfig\u0026rdquo;.\n\r\r\r\rtype\r\rstring\r\r\r\rType is the resource type, for example \u0026ldquo;coreos\u0026rdquo; or \u0026ldquo;ubuntu\u0026rdquo;.\n\r\r\r\rgloballyEnabled\r\rbool\r\r\r\r(Optional)\rGloballyEnabled determines if this ControllerResource is required by all Shoot clusters.\n\r\r\r\rreconcileTimeout\r\rKubernetes meta/v1.Duration\r\r\r\r\r(Optional)\rReconcileTimeout defines how long Gardener should wait for the resource reconciliation.\n\r\r\r\rDNS\r\r\r(Appears on:\rShootSpec)\r\rDNS holds information about the provider, the hosted zone id and the domain.\n\r\r\rField\rDescription\r\r\r\r\r\rdomain\r\rstring\r\r\r\r(Optional)\rDomain is the external available domain of the Shoot cluster. This domain will be written into the\rkubeconfig that is handed out to end-users.\n\r\r\r\rproviders\r\r[]DNSProvider\r\r\r\r\r(Optional)\rProviders is a list of DNS providers that shall be enabled for this shoot cluster. Only relevant if\rnot a default domain is used.\n\r\r\r\rDNSIncludeExclude\r\r\r(Appears on:\rDNSProvider)\r\r\r\r\rField\rDescription\r\r\r\r\r\rinclude\r\r[]string\r\r\r\r(Optional)\rInclude is a list of resources that shall be included.\n\r\r\r\rexclude\r\r[]string\r\r\r\r(Optional)\rExclude is a list of resources that shall be excluded.\n\r\r\r\rDNSProvider\r\r\r(Appears on:\rDNS)\r\rDNSProvider contains information about a DNS provider.\n\r\r\rField\rDescription\r\r\r\r\r\rdomains\r\rDNSIncludeExclude\r\r\r\r\r(Optional)\rDomains contains information about which domains shall be included/excluded for this provider.\n\r\r\r\rprimary\r\rbool\r\r\r\r(Optional)\rPrimary indicates that this DNSProvider is used for shoot related domains.\n\r\r\r\rsecretName\r\rstring\r\r\r\r(Optional)\rSecretName is a name of a secret containing credentials for the stated domain and the\rprovider. When not specified, the Gardener will use the cloud provider credentials referenced\rby the Shoot and try to find respective credentials there (primary provider only). Specifying this field may override\rthis behavior, i.e. forcing the Gardener to only look into the given secret.\n\r\r\r\rtype\r\rstring\r\r\r\r(Optional)\rType is the DNS provider type.\n\r\r\r\rzones\r\rDNSIncludeExclude\r\r\r\r\r(Optional)\rZones contains information about which hosted zones shall be included/excluded for this provider.\n\r\r\r\rEndpoint\r\r\r(Appears on:\rPlantSpec)\r\rEndpoint is an endpoint for monitoring, logging and other services around the plant.\n\r\r\rField\rDescription\r\r\r\r\r\rname\r\rstring\r\r\r\rName is the name of the endpoint\n\r\r\r\rurl\r\rstring\r\r\r\rURL is the url of the endpoint\n\r\r\r\rpurpose\r\rstring\r\r\r\rPurpose is the purpose of the endpoint\n\r\r\r\rErrorCode\r(string alias)\n\r\r(Appears on:\rCondition, LastError)\r\rErrorCode is a string alias.\nExpirableVersion\r\r\r(Appears on:\rKubernetesSettings, MachineImage)\r\rExpirableVersion contains a version and an expiration date.\n\r\r\rField\rDescription\r\r\r\r\r\rversion\r\rstring\r\r\r\rVersion is the version identifier.\n\r\r\r\rexpirationDate\r\rKubernetes meta/v1.Time\r\r\r\r\r(Optional)\rExpirationDate defines the time at which this version expires.\n\r\r\r\rclassification\r\rVersionClassification\r\r\r\r\r(Optional)\rClassification defines the state of a version (preview, supported, deprecated)\n\r\r\r\rExtension\r\r\r(Appears on:\rShootSpec)\r\rExtension contains type and provider information for Shoot extensions.\n\r\r\rField\rDescription\r\r\r\r\r\rtype\r\rstring\r\r\r\rType is the type of the extension resource.\n\r\r\r\rproviderConfig\r\rProviderConfig\r\r\r\r\r(Optional)\rProviderConfig is the configuration passed to extension resource.\n\r\r\r\rGardener\r\r\r(Appears on:\rSeedStatus, ShootStatus)\r\rGardener holds the information about the Gardener version that operated a resource.\n\r\r\rField\rDescription\r\r\r\r\r\rid\r\rstring\r\r\r\rID is the Docker container id of the Gardener which last acted on a resource.\n\r\r\r\rname\r\rstring\r\r\r\rName is the hostname (pod name) of the Gardener which last acted on a resource.\n\r\r\r\rversion\r\rstring\r\r\r\rVersion is the version of the Gardener which last acted on a resource.\n\r\r\r\rHibernation\r\r\r(Appears on:\rShootSpec)\r\rHibernation contains information whether the Shoot is suspended or not.\n\r\r\rField\rDescription\r\r\r\r\r\renabled\r\rbool\r\r\r\r(Optional)\rEnabled specifies whether the Shoot needs to be hibernated or not. If it is true, the Shoot\u0026rsquo;s desired state is to be hibernated.\rIf it is false or nil, the Shoot\u0026rsquo;s desired state is to be awaken.\n\r\r\r\rschedules\r\r[]HibernationSchedule\r\r\r\r\r(Optional)\rSchedules determine the hibernation schedules.\n\r\r\r\rHibernationSchedule\r\r\r(Appears on:\rHibernation)\r\rHibernationSchedule determines the hibernation schedule of a Shoot.\rA Shoot will be regularly hibernated at each start time and will be woken up at each end time.\rStart or End can be omitted, though at least one of each has to be specified.\n\r\r\rField\rDescription\r\r\r\r\r\rstart\r\rstring\r\r\r\r(Optional)\rStart is a Cron spec at which time a Shoot will be hibernated.\n\r\r\r\rend\r\rstring\r\r\r\r(Optional)\rEnd is a Cron spec at which time a Shoot will be woken up.\n\r\r\r\rlocation\r\rstring\r\r\r\r(Optional)\rLocation is the time location in which both start and and shall be evaluated.\n\r\r\r\rHorizontalPodAutoscalerConfig\r\r\r(Appears on:\rKubeControllerManagerConfig)\r\rHorizontalPodAutoscalerConfig contains horizontal pod autoscaler configuration settings for the kube-controller-manager.\rNote: Descriptions were taken from the Kubernetes documentation.\n\r\r\rField\rDescription\r\r\r\r\r\rcpuInitializationPeriod\r\rKubernetes meta/v1.Duration\r\r\r\r\r(Optional)\rThe period after which a ready pod transition is considered to be the first.\n\r\r\r\rdownscaleDelay\r\rKubernetes meta/v1.Duration\r\r\r\r\r(Optional)\rThe period since last downscale, before another downscale can be performed in horizontal pod autoscaler.\n\r\r\r\rdownscaleStabilization\r\rKubernetes meta/v1.Duration\r\r\r\r\r(Optional)\rThe configurable window at which the controller will choose the highest recommendation for autoscaling.\n\r\r\r\rinitialReadinessDelay\r\rKubernetes meta/v1.Duration\r\r\r\r\r(Optional)\rThe configurable period at which the horizontal pod autoscaler considers a Pod “not yet ready” given that it’s unready and it has transitioned to unready during that time.\n\r\r\r\rsyncPeriod\r\rKubernetes meta/v1.Duration\r\r\r\r\r(Optional)\rThe period for syncing the number of pods in horizontal pod autoscaler.\n\r\r\r\rtolerance\r\rfloat64\r\r\r\r(Optional)\rThe minimum change (from 1.0) in the desired-to-actual metrics ratio for the horizontal pod autoscaler to consider scaling.\n\r\r\r\rupscaleDelay\r\rKubernetes meta/v1.Duration\r\r\r\r\r(Optional)\rThe period since last upscale, before another upscale can be performed in horizontal pod autoscaler.\n\r\r\r\rKubeAPIServerConfig\r\r\r(Appears on:\rKubernetes)\r\rKubeAPIServerConfig contains configuration settings for the kube-apiserver.\n\r\r\rField\rDescription\r\r\r\r\r\rKubernetesConfig\r\rKubernetesConfig\r\r\r\r\r\r(Members of KubernetesConfig are embedded into this type.)\r\r\r\r\radmissionPlugins\r\r[]AdmissionPlugin\r\r\r\r\r(Optional)\rAdmissionPlugins contains the list of user-defined admission plugins (additional to those managed by Gardener), and, if desired, the corresponding\rconfiguration.\n\r\r\r\rapiAudiences\r\r[]string\r\r\r\r(Optional)\rAPIAudiences are the identifiers of the API. The service account token authenticator will\rvalidate that tokens used against the API are bound to at least one of these audiences.\rDefaults to [\u0026ldquo;kubernetes\u0026rdquo;].\n\r\r\r\rauditConfig\r\rAuditConfig\r\r\r\r\r(Optional)\rAuditConfig contains configuration settings for the audit of the kube-apiserver.\n\r\r\r\renableBasicAuthentication\r\rbool\r\r\r\r(Optional)\rEnableBasicAuthentication defines whether basic authentication should be enabled for this cluster or not.\n\r\r\r\roidcConfig\r\rOIDCConfig\r\r\r\r\r(Optional)\rOIDCConfig contains configuration settings for the OIDC provider.\n\r\r\r\rruntimeConfig\r\rmap[string]bool\r\r\r\r(Optional)\rRuntimeConfig contains information about enabled or disabled APIs.\n\r\r\r\rserviceAccountConfig\r\rServiceAccountConfig\r\r\r\r\r(Optional)\rServiceAccountConfig contains configuration settings for the service account handling\rof the kube-apiserver.\n\r\r\r\rKubeControllerManagerConfig\r\r\r(Appears on:\rKubernetes)\r\rKubeControllerManagerConfig contains configuration settings for the kube-controller-manager.\n\r\r\rField\rDescription\r\r\r\r\r\rKubernetesConfig\r\rKubernetesConfig\r\r\r\r\r\r(Members of KubernetesConfig are embedded into this type.)\r\r\r\r\rhorizontalPodAutoscaler\r\rHorizontalPodAutoscalerConfig\r\r\r\r\r(Optional)\rHorizontalPodAutoscalerConfig contains horizontal pod autoscaler configuration settings for the kube-controller-manager.\n\r\r\r\rnodeCIDRMaskSize\r\rint32\r\r\r\r(Optional)\rNodeCIDRMaskSize defines the mask size for node cidr in cluster (default is 24)\n\r\r\r\rKubeProxyConfig\r\r\r(Appears on:\rKubernetes)\r\rKubeProxyConfig contains configuration settings for the kube-proxy.\n\r\r\rField\rDescription\r\r\r\r\r\rKubernetesConfig\r\rKubernetesConfig\r\r\r\r\r\r(Members of KubernetesConfig are embedded into this type.)\r\r\r\r\rmode\r\rProxyMode\r\r\r\r\r(Optional)\rMode specifies which proxy mode to use.\rdefaults to IPTables.\n\r\r\r\rKubeSchedulerConfig\r\r\r(Appears on:\rKubernetes)\r\rKubeSchedulerConfig contains configuration settings for the kube-scheduler.\n\r\r\rField\rDescription\r\r\r\r\r\rKubernetesConfig\r\rKubernetesConfig\r\r\r\r\r\r(Members of KubernetesConfig are embedded into this type.)\r\r\r\r\rKubeletConfig\r\r\r(Appears on:\rKubernetes, WorkerKubernetes)\r\rKubeletConfig contains configuration settings for the kubelet.\n\r\r\rField\rDescription\r\r\r\r\r\rKubernetesConfig\r\rKubernetesConfig\r\r\r\r\r\r(Members of KubernetesConfig are embedded into this type.)\r\r\r\r\rcpuCFSQuota\r\rbool\r\r\r\r(Optional)\rCPUCFSQuota allows you to disable/enable CPU throttling for Pods.\n\r\r\r\rcpuManagerPolicy\r\rstring\r\r\r\r(Optional)\rCPUManagerPolicy allows to set alternative CPU management policies (default: none).\n\r\r\r\revictionHard\r\rKubeletConfigEviction\r\r\r\r\r(Optional)\rEvictionHard describes a set of eviction thresholds (e.g. memory.available\r\r\revictionMaxPodGracePeriod\r\rint32\r\r\r\r(Optional)\rEvictionMaxPodGracePeriod describes the maximum allowed grace period (in seconds) to use when terminating pods in response to a soft eviction threshold being met.\rDefault: 90\n\r\r\r\revictionMinimumReclaim\r\rKubeletConfigEvictionMinimumReclaim\r\r\r\r\r(Optional)\rEvictionMinimumReclaim configures the amount of resources below the configured eviction threshold that the kubelet attempts to reclaim whenever the kubelet observes resource pressure.\rDefault: 0 for each resource\n\r\r\r\revictionPressureTransitionPeriod\r\rKubernetes meta/v1.Duration\r\r\r\r\r(Optional)\rEvictionPressureTransitionPeriod is the duration for which the kubelet has to wait before transitioning out of an eviction pressure condition.\rDefault: 4m0s\n\r\r\r\revictionSoft\r\rKubeletConfigEviction\r\r\r\r\r(Optional)\rEvictionSoft describes a set of eviction thresholds (e.g. memory.available\r\r\revictionSoftGracePeriod\r\rKubeletConfigEvictionSoftGracePeriod\r\r\r\r\r(Optional)\rEvictionSoftGracePeriod describes a set of eviction grace periods (e.g. memory.available=1m30s) that correspond to how long a soft eviction threshold must hold before triggering a Pod eviction.\rDefault:\rmemory.available: 1m30s\rnodefs.available: 1m30s\rnodefs.inodesFree: 1m30s\rimagefs.available: 1m30s\rimagefs.inodesFree: 1m30s\n\r\r\r\rmaxPods\r\rint32\r\r\r\r(Optional)\rMaxPods is the maximum number of Pods that are allowed by the Kubelet.\rDefault: 110\n\r\r\r\rpodPidsLimit\r\rint64\r\r\r\r(Optional)\rPodPIDsLimit is the maximum number of process IDs per pod allowed by the kubelet.\n\r\r\r\rimagePullProgressDeadline\r\rKubernetes meta/v1.Duration\r\r\r\r\r(Optional)\rImagePullProgressDeadline describes the time limit under which if no pulling progress is made, the image pulling will be cancelled.\rDefault: 1m\n\r\r\r\rKubeletConfigEviction\r\r\r(Appears on:\rKubeletConfig)\r\rKubeletConfigEviction contains kubelet eviction thresholds supporting either a resource.Quantity or a percentage based value.\n\r\r\rField\rDescription\r\r\r\r\r\rmemoryAvailable\r\rstring\r\r\r\r(Optional)\rMemoryAvailable is the threshold for the free memory on the host server.\n\r\r\r\rimageFSAvailable\r\rstring\r\r\r\r(Optional)\rImageFSAvailable is the threshold for the free disk space in the imagefs filesystem (docker images and container writable layers).\n\r\r\r\rimageFSInodesFree\r\rstring\r\r\r\r(Optional)\rImageFSInodesFree is the threshold for the available inodes in the imagefs filesystem.\n\r\r\r\rnodeFSAvailable\r\rstring\r\r\r\r(Optional)\rNodeFSAvailable is the threshold for the free disk space in the nodefs filesystem (docker volumes, logs, etc).\n\r\r\r\rnodeFSInodesFree\r\rstring\r\r\r\r(Optional)\rNodeFSInodesFree is the threshold for the available inodes in the nodefs filesystem.\n\r\r\r\rKubeletConfigEvictionMinimumReclaim\r\r\r(Appears on:\rKubeletConfig)\r\rKubeletConfigEvictionMinimumReclaim contains configuration for the kubelet eviction minimum reclaim.\n\r\r\rField\rDescription\r\r\r\r\r\rmemoryAvailable\r\rk8s.io/apimachinery/pkg/api/resource.Quantity\r\r\r\r\r(Optional)\rMemoryAvailable is the threshold for the memory reclaim on the host server.\n\r\r\r\rimageFSAvailable\r\rk8s.io/apimachinery/pkg/api/resource.Quantity\r\r\r\r\r(Optional)\rImageFSAvailable is the threshold for the disk space reclaim in the imagefs filesystem (docker images and container writable layers).\n\r\r\r\rimageFSInodesFree\r\rk8s.io/apimachinery/pkg/api/resource.Quantity\r\r\r\r\r(Optional)\rImageFSInodesFree is the threshold for the inodes reclaim in the imagefs filesystem.\n\r\r\r\rnodeFSAvailable\r\rk8s.io/apimachinery/pkg/api/resource.Quantity\r\r\r\r\r(Optional)\rNodeFSAvailable is the threshold for the disk space reclaim in the nodefs filesystem (docker volumes, logs, etc).\n\r\r\r\rnodeFSInodesFree\r\rk8s.io/apimachinery/pkg/api/resource.Quantity\r\r\r\r\r(Optional)\rNodeFSInodesFree is the threshold for the inodes reclaim in the nodefs filesystem.\n\r\r\r\rKubeletConfigEvictionSoftGracePeriod\r\r\r(Appears on:\rKubeletConfig)\r\rKubeletConfigEvictionSoftGracePeriod contains grace periods for kubelet eviction thresholds.\n\r\r\rField\rDescription\r\r\r\r\r\rmemoryAvailable\r\rKubernetes meta/v1.Duration\r\r\r\r\r(Optional)\rMemoryAvailable is the grace period for the MemoryAvailable eviction threshold.\n\r\r\r\rimageFSAvailable\r\rKubernetes meta/v1.Duration\r\r\r\r\r(Optional)\rImageFSAvailable is the grace period for the ImageFSAvailable eviction threshold.\n\r\r\r\rimageFSInodesFree\r\rKubernetes meta/v1.Duration\r\r\r\r\r(Optional)\rImageFSInodesFree is the grace period for the ImageFSInodesFree eviction threshold.\n\r\r\r\rnodeFSAvailable\r\rKubernetes meta/v1.Duration\r\r\r\r\r(Optional)\rNodeFSAvailable is the grace period for the NodeFSAvailable eviction threshold.\n\r\r\r\rnodeFSInodesFree\r\rKubernetes meta/v1.Duration\r\r\r\r\r(Optional)\rNodeFSInodesFree is the grace period for the NodeFSInodesFree eviction threshold.\n\r\r\r\rKubernetes\r\r\r(Appears on:\rShootSpec)\r\rKubernetes contains the version and configuration variables for the Shoot control plane.\n\r\r\rField\rDescription\r\r\r\r\r\rallowPrivilegedContainers\r\rbool\r\r\r\r(Optional)\rAllowPrivilegedContainers indicates whether privileged containers are allowed in the Shoot (default: true).\n\r\r\r\rclusterAutoscaler\r\rClusterAutoscaler\r\r\r\r\r(Optional)\rClusterAutoscaler contains the configration flags for the Kubernetes cluster autoscaler.\n\r\r\r\rkubeAPIServer\r\rKubeAPIServerConfig\r\r\r\r\r(Optional)\rKubeAPIServer contains configuration settings for the kube-apiserver.\n\r\r\r\rkubeControllerManager\r\rKubeControllerManagerConfig\r\r\r\r\r(Optional)\rKubeControllerManager contains configuration settings for the kube-controller-manager.\n\r\r\r\rkubeScheduler\r\rKubeSchedulerConfig\r\r\r\r\r(Optional)\rKubeScheduler contains configuration settings for the kube-scheduler.\n\r\r\r\rkubeProxy\r\rKubeProxyConfig\r\r\r\r\r(Optional)\rKubeProxy contains configuration settings for the kube-proxy.\n\r\r\r\rkubelet\r\rKubeletConfig\r\r\r\r\r(Optional)\rKubelet contains configuration settings for the kubelet.\n\r\r\r\rversion\r\rstring\r\r\r\rVersion is the semantic Kubernetes version to use for the Shoot cluster.\n\r\r\r\rKubernetesConfig\r\r\r(Appears on:\rKubeAPIServerConfig, KubeControllerManagerConfig, KubeProxyConfig, KubeSchedulerConfig, KubeletConfig)\r\rKubernetesConfig contains common configuration fields for the control plane components.\n\r\r\rField\rDescription\r\r\r\r\r\rfeatureGates\r\rmap[string]bool\r\r\r\r(Optional)\rFeatureGates contains information about enabled feature gates.\n\r\r\r\rKubernetesDashboard\r\r\r(Appears on:\rAddons)\r\rKubernetesDashboard describes configuration values for the kubernetes-dashboard addon.\n\r\r\rField\rDescription\r\r\r\r\r\rAddon\r\rAddon\r\r\r\r\r\r(Members of Addon are embedded into this type.)\r\r\r\r\rauthenticationMode\r\rstring\r\r\r\r(Optional)\rAuthenticationMode defines the authentication mode for the kubernetes-dashboard.\n\r\r\r\rKubernetesInfo\r\r\r(Appears on:\rClusterInfo)\r\rKubernetesInfo contains the version and configuration variables for the Plant cluster.\n\r\r\rField\rDescription\r\r\r\r\r\rversion\r\rstring\r\r\r\rVersion is the semantic Kubernetes version to use for the Plant cluster.\n\r\r\r\rKubernetesSettings\r\r\r(Appears on:\rCloudProfileSpec)\r\rKubernetesSettings contains constraints regarding allowed values of the \u0026lsquo;kubernetes\u0026rsquo; block in the Shoot specification.\n\r\r\rField\rDescription\r\r\r\r\r\rversions\r\r[]ExpirableVersion\r\r\r\r\r(Optional)\rVersions is the list of allowed Kubernetes versions with optional expiration dates for Shoot clusters.\n\r\r\r\rLastError\r\r\r(Appears on:\rBackupBucketStatus, BackupEntryStatus, ShootStatus)\r\rLastError indicates the last occurred error for an operation on a resource.\n\r\r\rField\rDescription\r\r\r\r\r\rdescription\r\rstring\r\r\r\rA human readable message indicating details about the last error.\n\r\r\r\rtaskID\r\rstring\r\r\r\r(Optional)\rID of the task which caused this last error\n\r\r\r\rcodes\r\r[]ErrorCode\r\r\r\r\r(Optional)\rWell-defined error codes of the last error(s).\n\r\r\r\rlastUpdateTime\r\rKubernetes meta/v1.Time\r\r\r\r\r(Optional)\rLast time the error was reported\n\r\r\r\rLastOperation\r\r\r(Appears on:\rBackupBucketStatus, BackupEntryStatus, ShootStatus)\r\rLastOperation indicates the type and the state of the last operation, along with a description\rmessage and a progress indicator.\n\r\r\rField\rDescription\r\r\r\r\r\rdescription\r\rstring\r\r\r\rA human readable message indicating details about the last operation.\n\r\r\r\rlastUpdateTime\r\rKubernetes meta/v1.Time\r\r\r\r\rLast time the operation state transitioned from one to another.\n\r\r\r\rprogress\r\rint32\r\r\r\rThe progress in percentage (0-100) of the last operation.\n\r\r\r\rstate\r\rLastOperationState\r\r\r\r\rStatus of the last operation, one of Aborted, Processing, Succeeded, Error, Failed.\n\r\r\r\rtype\r\rLastOperationType\r\r\r\r\rType of the last operation, one of Create, Reconcile, Delete.\n\r\r\r\rLastOperationState\r(string alias)\n\r\r(Appears on:\rLastOperation)\r\rLastOperationState is a string alias.\nLastOperationType\r(string alias)\n\r\r(Appears on:\rLastOperation)\r\rLastOperationType is a string alias.\nMachine\r\r\r(Appears on:\rWorker)\r\rMachine contains information about the machine type and image.\n\r\r\rField\rDescription\r\r\r\r\r\rtype\r\rstring\r\r\r\rType is the machine type of the worker group.\n\r\r\r\rimage\r\rShootMachineImage\r\r\r\r\r(Optional)\rImage holds information about the machine image to use for all nodes of this pool. It will default to the\rlatest version of the first image stated in the referenced CloudProfile if no value has been provided.\n\r\r\r\rMachineImage\r\r\r(Appears on:\rCloudProfileSpec)\r\rMachineImage defines the name and multiple versions of the machine image in any environment.\n\r\r\rField\rDescription\r\r\r\r\r\rname\r\rstring\r\r\r\rName is the name of the image.\n\r\r\r\rversions\r\r[]ExpirableVersion\r\r\r\r\rVersions contains versions and expiration dates of the machine image\n\r\r\r\rMachineType\r\r\r(Appears on:\rCloudProfileSpec)\r\rMachineType contains certain properties of a machine type.\n\r\r\rField\rDescription\r\r\r\r\r\rcpu\r\rk8s.io/apimachinery/pkg/api/resource.Quantity\r\r\r\r\rCPU is the number of CPUs for this machine type.\n\r\r\r\rgpu\r\rk8s.io/apimachinery/pkg/api/resource.Quantity\r\r\r\r\rGPU is the number of GPUs for this machine type.\n\r\r\r\rmemory\r\rk8s.io/apimachinery/pkg/api/resource.Quantity\r\r\r\r\rMemory is the amount of memory for this machine type.\n\r\r\r\rname\r\rstring\r\r\r\rName is the name of the machine type.\n\r\r\r\rstorage\r\rMachineTypeStorage\r\r\r\r\r(Optional)\rStorage is the amount of storage associated with the root volume of this machine type.\n\r\r\r\rusable\r\rbool\r\r\r\r(Optional)\rUsable defines if the machine type can be used for shoot clusters.\n\r\r\r\rMachineTypeStorage\r\r\r(Appears on:\rMachineType)\r\rMachineTypeStorage is the amount of storage associated with the root volume of this machine type.\n\r\r\rField\rDescription\r\r\r\r\r\rclass\r\rstring\r\r\r\rClass is the class of the storage type.\n\r\r\r\rsize\r\rk8s.io/apimachinery/pkg/api/resource.Quantity\r\r\r\r\rStorageSize is the storage size.\n\r\r\r\rtype\r\rstring\r\r\r\rType is the type of the storage.\n\r\r\r\rMaintenance\r\r\r(Appears on:\rShootSpec)\r\rMaintenance contains information about the time window for maintenance operations and which\roperations should be performed.\n\r\r\rField\rDescription\r\r\r\r\r\rautoUpdate\r\rMaintenanceAutoUpdate\r\r\r\r\r(Optional)\rAutoUpdate contains information about which constraints should be automatically updated.\n\r\r\r\rtimeWindow\r\rMaintenanceTimeWindow\r\r\r\r\r(Optional)\rTimeWindow contains information about the time window for maintenance operations.\n\r\r\r\rconfineSpecUpdateRollout\r\rbool\r\r\r\r(Optional)\rConfineSpecUpdateRollout prevents that changes/updates to the shoot specification will be rolled out immediately.\rInstead, they are rolled out during the shoot\u0026rsquo;s maintenance time window. There is one exception that will trigger\ran immediate roll out which is changes to the Spec.Hibernation.Enabled field.\n\r\r\r\rMaintenanceAutoUpdate\r\r\r(Appears on:\rMaintenance)\r\rMaintenanceAutoUpdate contains information about which constraints should be automatically updated.\n\r\r\rField\rDescription\r\r\r\r\r\rkubernetesVersion\r\rbool\r\r\r\rKubernetesVersion indicates whether the patch Kubernetes version may be automatically updated (default: true).\n\r\r\r\rmachineImageVersion\r\rbool\r\r\r\rMachineImageVersion indicates whether the machine image version may be automatically updated (default: true).\n\r\r\r\rMaintenanceTimeWindow\r\r\r(Appears on:\rMaintenance)\r\rMaintenanceTimeWindow contains information about the time window for maintenance operations.\n\r\r\rField\rDescription\r\r\r\r\r\rbegin\r\rstring\r\r\r\rBegin is the beginning of the time window in the format HHMMSS+ZONE, e.g. \u0026ldquo;220000+0100\u0026rdquo;.\rIf not present, a random value will be computed.\n\r\r\r\rend\r\rstring\r\r\r\rEnd is the end of the time window in the format HHMMSS+ZONE, e.g. \u0026ldquo;220000+0100\u0026rdquo;.\rIf not present, the value will be computed based on the \u0026ldquo;Begin\u0026rdquo; value.\n\r\r\r\rMonitoring\r\r\r(Appears on:\rShootSpec)\r\rMonitoring contains information about the monitoring configuration for the shoot.\n\r\r\rField\rDescription\r\r\r\r\r\ralerting\r\rAlerting\r\r\r\r\r(Optional)\rAlerting contains information about the alerting configuration for the shoot cluster.\n\r\r\r\rNetworking\r\r\r(Appears on:\rShootSpec)\r\rNetworking defines networking parameters for the shoot cluster.\n\r\r\rField\rDescription\r\r\r\r\r\rtype\r\rstring\r\r\r\rType identifies the type of the networking plugin.\n\r\r\r\rproviderConfig\r\rProviderConfig\r\r\r\r\r(Optional)\rProviderConfig is the configuration passed to network resource.\n\r\r\r\rpods\r\rstring\r\r\r\r(Optional)\rPods is the CIDR of the pod network.\n\r\r\r\rnodes\r\rstring\r\r\r\r(Optional)\rNodes is the CIDR of the entire node network.\n\r\r\r\rservices\r\rstring\r\r\r\r(Optional)\rServices is the CIDR of the service network.\n\r\r\r\rNginxIngress\r\r\r(Appears on:\rAddons)\r\rNginxIngress describes configuration values for the nginx-ingress addon.\n\r\r\rField\rDescription\r\r\r\r\r\rAddon\r\rAddon\r\r\r\r\r\r(Members of Addon are embedded into this type.)\r\r\r\r\rloadBalancerSourceRanges\r\r[]string\r\r\r\r(Optional)\rLoadBalancerSourceRanges is list of whitelist IP sources for NginxIngress\n\r\r\r\rconfig\r\rmap[string]string\r\r\r\r(Optional)\rConfig contains custom configuration for the nginx-ingress-controller configuration.\rSee https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/nginx-configuration/configmap.md#configuration-options\n\r\r\r\rexternalTrafficPolicy\r\rKubernetes core/v1.ServiceExternalTrafficPolicyType\r\r\r\r\r(Optional)\rExternalTrafficPolicy controls the .spec.externalTrafficPolicy value of the load balancer Service\rexposing the nginx-ingress. Defaults to Cluster.\n\r\r\r\rOIDCConfig\r\r\r(Appears on:\rKubeAPIServerConfig)\r\rOIDCConfig contains configuration settings for the OIDC provider.\rNote: Descriptions were taken from the Kubernetes documentation.\n\r\r\rField\rDescription\r\r\r\r\r\rcaBundle\r\rstring\r\r\r\r(Optional)\rIf set, the OpenID server\u0026rsquo;s certificate will be verified by one of the authorities in the oidc-ca-file, otherwise the host\u0026rsquo;s root CA set will be used.\n\r\r\r\rclientAuthentication\r\rOpenIDConnectClientAuthentication\r\r\r\r\r(Optional)\rClientAuthentication can optionally contain client configuration used for kubeconfig generation.\n\r\r\r\rclientID\r\rstring\r\r\r\r(Optional)\rThe client ID for the OpenID Connect client, must be set if oidc-issuer-url is set.\n\r\r\r\rgroupsClaim\r\rstring\r\r\r\r(Optional)\rIf provided, the name of a custom OpenID Connect claim for specifying user groups. The claim value is expected to be a string or array of strings. This flag is experimental, please see the authentication documentation for further details.\n\r\r\r\rgroupsPrefix\r\rstring\r\r\r\r(Optional)\rIf provided, all groups will be prefixed with this value to prevent conflicts with other authentication strategies.\n\r\r\r\rissuerURL\r\rstring\r\r\r\r(Optional)\rThe URL of the OpenID issuer, only HTTPS scheme will be accepted. If set, it will be used to verify the OIDC JSON Web Token (JWT).\n\r\r\r\rrequiredClaims\r\rmap[string]string\r\r\r\r(Optional)\rATTENTION: Only meaningful for Kubernetes \u0026gt;= 1.11\rkey=value pairs that describes a required claim in the ID Token. If set, the claim is verified to be present in the ID Token with a matching value.\n\r\r\r\rsigningAlgs\r\r[]string\r\r\r\r(Optional)\rList of allowed JOSE asymmetric signing algorithms. JWTs with a \u0026lsquo;alg\u0026rsquo; header value not in this list will be rejected. Values are defined by RFC 7518 https://tools.ietf.org/html/rfc7518#section-3.1\n\r\r\r\rusernameClaim\r\rstring\r\r\r\r(Optional)\rThe OpenID claim to use as the user name. Note that claims other than the default (\u0026lsquo;sub\u0026rsquo;) is not guaranteed to be unique and immutable. This flag is experimental, please see the authentication documentation for further details. (default \u0026ldquo;sub\u0026rdquo;)\n\r\r\r\rusernamePrefix\r\rstring\r\r\r\r(Optional)\rIf provided, all usernames will be prefixed with this value. If not provided, username claims other than \u0026lsquo;email\u0026rsquo; are prefixed by the issuer URL to avoid clashes. To skip any prefixing, provide the value \u0026lsquo;-\u0026rsquo;.\n\r\r\r\rOpenIDConnectClientAuthentication\r\r\r(Appears on:\rOIDCConfig)\r\rOpenIDConnectClientAuthentication contains configuration for OIDC clients.\n\r\r\rField\rDescription\r\r\r\r\r\rextraConfig\r\rmap[string]string\r\r\r\r(Optional)\rExtra configuration added to kubeconfig\u0026rsquo;s auth-provider.\rMust not be any of idp-issuer-url, client-id, client-secret, idp-certificate-authority, idp-certificate-authority-data, id-token or refresh-token\n\r\r\r\rsecret\r\rstring\r\r\r\r(Optional)\rThe client Secret for the OpenID Connect client.\n\r\r\r\rPlantSpec\r\r\r(Appears on:\rPlant)\r\rPlantSpec is the specification of a Plant.\n\r\r\rField\rDescription\r\r\r\r\r\rsecretRef\r\rKubernetes core/v1.LocalObjectReference\r\r\r\r\rSecretRef is a reference to a Secret object containing the Kubeconfig of the external kubernetes\rclusters to be added to Gardener.\n\r\r\r\rendpoints\r\r[]Endpoint\r\r\r\r\r(Optional)\rEndpoints is the configuration plant endpoints\n\r\r\r\rPlantStatus\r\r\r(Appears on:\rPlant)\r\rPlantStatus is the status of a Plant.\n\r\r\rField\rDescription\r\r\r\r\r\rconditions\r\r[]Condition\r\r\r\r\r(Optional)\rConditions represents the latest available observations of a Plant\u0026rsquo;s current state.\n\r\r\r\robservedGeneration\r\rint64\r\r\r\r(Optional)\rObservedGeneration is the most recent generation observed for this Plant. It corresponds to the\rPlant\u0026rsquo;s generation, which is updated on mutation by the API Server.\n\r\r\r\rclusterInfo\r\rClusterInfo\r\r\r\r\rClusterInfo is additional computed information about the newly added cluster (Plant)\n\r\r\r\rProjectMember\r\r\r(Appears on:\rProjectSpec)\r\rProjectMember is a member of a project.\n\r\r\rField\rDescription\r\r\r\r\r\rSubject\r\rKubernetes rbac/v1.Subject\r\r\r\r\r\r(Members of Subject are embedded into this type.)\rSubject is representing a user name, an email address, or any other identifier of a user, group, or service\raccount that has a certain role.\n\r\r\r\rrole\r\rstring\r\r\r\rRole represents the role of this member.\rIMPORTANT: Be aware that this field will be removed in the v1 version of this API in favor of the roles\rlist.\rTODO: Remove this field in favor of the owner role in v1.\n\r\r\r\rroles\r\r[]string\r\r\r\r(Optional)\rRoles represents the list of roles of this member.\n\r\r\r\rProjectPhase\r(string alias)\n\r\r(Appears on:\rProjectStatus)\r\rProjectPhase is a label for the condition of a project at the current time.\nProjectSpec\r\r\r(Appears on:\rProject)\r\rProjectSpec is the specification of a Project.\n\r\r\rField\rDescription\r\r\r\r\r\rcreatedBy\r\rKubernetes rbac/v1.Subject\r\r\r\r\r(Optional)\rCreatedBy is a subject representing a user name, an email address, or any other identifier of a user\rwho created the project.\n\r\r\r\rdescription\r\rstring\r\r\r\r(Optional)\rDescription is a human-readable description of what the project is used for.\n\r\r\r\rowner\r\rKubernetes rbac/v1.Subject\r\r\r\r\r(Optional)\rOwner is a subject representing a user name, an email address, or any other identifier of a user owning\rthe project.\rIMPORTANT: Be aware that this field will be removed in the v1 version of this API in favor of the owner\rrole. The only way to change the owner will be by moving the owner role. In this API version the only way\rto change the owner is to use this field.\rTODO: Remove this field in favor of the owner role in v1.\n\r\r\r\rpurpose\r\rstring\r\r\r\r(Optional)\rPurpose is a human-readable explanation of the project\u0026rsquo;s purpose.\n\r\r\r\rmembers\r\r[]ProjectMember\r\r\r\r\r(Optional)\rMembers is a list of subjects representing a user name, an email address, or any other identifier of a user,\rgroup, or service account that has a certain role.\n\r\r\r\rnamespace\r\rstring\r\r\r\r(Optional)\rNamespace is the name of the namespace that has been created for the Project object.\rA nil value means that Gardener will determine the name of the namespace.\n\r\r\r\rProjectStatus\r\r\r(Appears on:\rProject)\r\rProjectStatus holds the most recently observed status of the project.\n\r\r\rField\rDescription\r\r\r\r\r\robservedGeneration\r\rint64\r\r\r\r(Optional)\rObservedGeneration is the most recent generation observed for this project.\n\r\r\r\rphase\r\rProjectPhase\r\r\r\r\rPhase is the current phase of the project.\n\r\r\r\rProvider\r\r\r(Appears on:\rShootSpec)\r\rProvider contains provider-specific information that are handed-over to the provider-specific\rextension controller.\n\r\r\rField\rDescription\r\r\r\r\r\rtype\r\rstring\r\r\r\rType is the type of the provider.\n\r\r\r\rcontrolPlaneConfig\r\rProviderConfig\r\r\r\r\r(Optional)\rControlPlaneConfig contains the provider-specific control plane config blob. Please look up the concrete\rdefinition in the documentation of your provider extension.\n\r\r\r\rinfrastructureConfig\r\rProviderConfig\r\r\r\r\r(Optional)\rInfrastructureConfig contains the provider-specific infrastructure config blob. Please look up the concrete\rdefinition in the documentation of your provider extension.\n\r\r\r\rworkers\r\r[]Worker\r\r\r\r\rWorkers is a list of worker groups.\n\r\r\r\rProviderConfig\r\r\r(Appears on:\rAdmissionPlugin, BackupBucketSpec, BackupBucketStatus, CloudProfileSpec, ContainerRuntime, ControllerDeployment, ControllerInstallationStatus, Extension, Networking, Provider, SeedBackup, SeedProvider, ShootMachineImage, Worker)\r\rProviderConfig is a workaround for missing OpenAPI functions on runtime.RawExtension struct.\rhttps://github.com/kubernetes/kubernetes/issues/55890\rhttps://github.com/kubernetes-sigs/cluster-api/issues/137\n\r\r\rField\rDescription\r\r\r\r\r\rRawExtension\r\rk8s.io/apimachinery/pkg/runtime.RawExtension\r\r\r\r\r\r(Members of RawExtension are embedded into this type.)\r\r\r\r\rProxyMode\r(string alias)\n\r\r(Appears on:\rKubeProxyConfig)\r\rProxyMode available in Linux platform: \u0026lsquo;userspace\u0026rsquo; (older, going to be EOL), \u0026lsquo;iptables\u0026rsquo;\r(newer, faster), \u0026lsquo;ipvs\u0026rsquo; (newest, better in performance and scalability).\rAs of now only \u0026lsquo;iptables\u0026rsquo; and \u0026lsquo;ipvs\u0026rsquo; is supported by Gardener.\rIn Linux platform, if the iptables proxy is selected, regardless of how, but the system\u0026rsquo;s kernel or iptables versions are\rinsufficient, this always falls back to the userspace proxy. IPVS mode will be enabled when proxy mode is set to \u0026lsquo;ipvs\u0026rsquo;,\rand the fall back path is firstly iptables and then userspace.\nQuotaSpec\r\r\r(Appears on:\rQuota)\r\rQuotaSpec is the specification of a Quota.\n\r\r\rField\rDescription\r\r\r\r\r\rclusterLifetimeDays\r\rint32\r\r\r\r(Optional)\rClusterLifetimeDays is the lifetime of a Shoot cluster in days before it will be terminated automatically.\n\r\r\r\rmetrics\r\rKubernetes core/v1.ResourceList\r\r\r\r\rMetrics is a list of resources which will be put under constraints.\n\r\r\r\rscope\r\rKubernetes core/v1.ObjectReference\r\r\r\r\rScope is the scope of the Quota object, either \u0026lsquo;project\u0026rsquo; or \u0026lsquo;secret\u0026rsquo;.\n\r\r\r\rRegion\r\r\r(Appears on:\rCloudProfileSpec)\r\rRegion contains certain properties of a region.\n\r\r\rField\rDescription\r\r\r\r\r\rname\r\rstring\r\r\r\rName is a region name.\n\r\r\r\rzones\r\r[]AvailabilityZone\r\r\r\r\r(Optional)\rZones is a list of availability zones in this region.\n\r\r\r\rSeedBackup\r\r\r(Appears on:\rSeedSpec)\r\rSeedBackup contains the object store configuration for backups for shoot (currently only etcd).\n\r\r\rField\rDescription\r\r\r\r\r\rprovider\r\rstring\r\r\r\rProvider is a provider name.\n\r\r\r\rproviderConfig\r\rProviderConfig\r\r\r\r\r(Optional)\rProviderConfig is the configuration passed to BackupBucket resource.\n\r\r\r\rregion\r\rstring\r\r\r\r(Optional)\rRegion is a region name.\n\r\r\r\rsecretRef\r\rKubernetes core/v1.SecretReference\r\r\r\r\rSecretRef is a reference to a Secret object containing the cloud provider credentials for\rthe object store where backups should be stored. It should have enough privileges to manipulate\rthe objects as well as buckets.\n\r\r\r\rSeedDNS\r\r\r(Appears on:\rSeedSpec)\r\rSeedDNS contains DNS-relevant information about this seed cluster.\n\r\r\rField\rDescription\r\r\r\r\r\ringressDomain\r\rstring\r\r\r\rIngressDomain is the domain of the Seed cluster pointing to the ingress controller endpoint. It will be used\rto construct ingress URLs for system applications running in Shoot clusters.\n\r\r\r\rSeedNetworks\r\r\r(Appears on:\rSeedSpec)\r\rSeedNetworks contains CIDRs for the pod, service and node networks of a Kubernetes cluster.\n\r\r\rField\rDescription\r\r\r\r\r\rnodes\r\rstring\r\r\r\r(Optional)\rNodes is the CIDR of the node network.\n\r\r\r\rpods\r\rstring\r\r\r\rPods is the CIDR of the pod network.\n\r\r\r\rservices\r\rstring\r\r\r\rServices is the CIDR of the service network.\n\r\r\r\rshootDefaults\r\rShootNetworks\r\r\r\r\r(Optional)\rShootDefaults contains the default networks CIDRs for shoots.\n\r\r\r\rblockCIDRs\r\r[]string\r\r\r\r(Optional)\rBlockCIDRs is a list of network addresses that should be blocked for shoot control plane components running\rin the seed cluster.\n\r\r\r\rSeedProvider\r\r\r(Appears on:\rSeedSpec)\r\rSeedProvider defines the provider type and region for this Seed cluster.\n\r\r\rField\rDescription\r\r\r\r\r\rtype\r\rstring\r\r\r\rType is the name of the provider.\n\r\r\r\rproviderConfig\r\rProviderConfig\r\r\r\r\r(Optional)\rProviderConfig is the configuration passed to Seed resource.\n\r\r\r\rregion\r\rstring\r\r\r\rRegion is a name of a region.\n\r\r\r\rSeedSpec\r\r\r(Appears on:\rSeed)\r\rSeedSpec is the specification of a Seed.\n\r\r\rField\rDescription\r\r\r\r\r\rbackup\r\rSeedBackup\r\r\r\r\r(Optional)\rBackup holds the object store configuration for the backups of shoot (currently only etcd).\rIf it is not specified, then there won\u0026rsquo;t be any backups taken for shoots associated with this seed.\rIf backup field is present in seed, then backups of the etcd from shoot control plane will be stored\runder the configured object store.\n\r\r\r\rdns\r\rSeedDNS\r\r\r\r\rDNS contains DNS-relevant information about this seed cluster.\n\r\r\r\rnetworks\r\rSeedNetworks\r\r\r\r\rNetworks defines the pod, service and worker network of the Seed cluster.\n\r\r\r\rprovider\r\rSeedProvider\r\r\r\r\rProvider defines the provider type and region for this Seed cluster.\n\r\r\r\rsecretRef\r\rKubernetes core/v1.SecretReference\r\r\r\r\r(Optional)\rSecretRef is a reference to a Secret object containing the Kubeconfig and the cloud provider credentials for\rthe account the Seed cluster has been deployed to.\n\r\r\r\rtaints\r\r[]SeedTaint\r\r\r\r\r(Optional)\rTaints describes taints on the seed.\n\r\r\r\rvolume\r\rSeedVolume\r\r\r\r\r(Optional)\rVolume contains settings for persistentvolumes created in the seed cluster.\n\r\r\r\rSeedStatus\r\r\r(Appears on:\rSeed)\r\rSeedStatus is the status of a Seed.\n\r\r\rField\rDescription\r\r\r\r\r\rgardener\r\rGardener\r\r\r\r\r(Optional)\rGardener holds information about the Gardener which last acted on the Shoot.\n\r\r\r\rkubernetesVersion\r\rstring\r\r\r\r(Optional)\rKubernetesVersion is the Kubernetes version of the seed cluster.\n\r\r\r\rconditions\r\r[]Condition\r\r\r\r\r(Optional)\rConditions represents the latest available observations of a Seed\u0026rsquo;s current state.\n\r\r\r\robservedGeneration\r\rint64\r\r\r\r(Optional)\rObservedGeneration is the most recent generation observed for this Seed. It corresponds to the\rSeed\u0026rsquo;s generation, which is updated on mutation by the API Server.\n\r\r\r\rSeedTaint\r\r\r(Appears on:\rSeedSpec)\r\rSeedTaint describes a taint on a seed.\n\r\r\rField\rDescription\r\r\r\r\r\rkey\r\rstring\r\r\r\rKey is the taint key to be applied to a seed.\n\r\r\r\rvalue\r\rstring\r\r\r\r(Optional)\rValue is the taint value corresponding to the taint key.\n\r\r\r\rSeedVolume\r\r\r(Appears on:\rSeedSpec)\r\rSeedVolume contains settings for persistentvolumes created in the seed cluster.\n\r\r\rField\rDescription\r\r\r\r\r\rminimumSize\r\rk8s.io/apimachinery/pkg/api/resource.Quantity\r\r\r\r\r(Optional)\rMinimumSize defines the minimum size that should be used for PVCs in the seed.\n\r\r\r\rproviders\r\r[]SeedVolumeProvider\r\r\r\r\r(Optional)\rProviders is a list of storage class provisioner types for the seed.\n\r\r\r\rSeedVolumeProvider\r\r\r(Appears on:\rSeedVolume)\r\rSeedVolumeProvider is a storage class provisioner type.\n\r\r\rField\rDescription\r\r\r\r\r\rpurpose\r\rstring\r\r\r\rPurpose is the purpose of this provider.\n\r\r\r\rname\r\rstring\r\r\r\rName is the name of the storage class provisioner type.\n\r\r\r\rServiceAccountConfig\r\r\r(Appears on:\rKubeAPIServerConfig)\r\rServiceAccountConfig is the kube-apiserver configuration for service accounts.\n\r\r\rField\rDescription\r\r\r\r\r\rissuer\r\rstring\r\r\r\r(Optional)\rIssuer is the identifier of the service account token issuer. The issuer will assert this\ridentifier in \u0026ldquo;iss\u0026rdquo; claim of issued tokens. This value is a string or URI.\rDefaults to URI of the API server.\n\r\r\r\rsigningKeySecretName\r\rKubernetes core/v1.LocalObjectReference\r\r\r\r\r(Optional)\rSigningKeySecret is a reference to a secret that contains an optional private key of the\rservice account token issuer. The issuer will sign issued ID tokens with this private key.\rOnly useful if service account tokens are also issued by another external system.\n\r\r\r\rShootMachineImage\r\r\r(Appears on:\rMachine)\r\rShootMachineImage defines the name and the version of the shoot\u0026rsquo;s machine image in any environment. Has to be\rdefined in the respective CloudProfile.\n\r\r\rField\rDescription\r\r\r\r\r\rname\r\rstring\r\r\r\rName is the name of the image.\n\r\r\r\rproviderConfig\r\rProviderConfig\r\r\r\r\r(Optional)\rProviderConfig is the shoot\u0026rsquo;s individual configuration passed to an extension resource.\n\r\r\r\rversion\r\rstring\r\r\r\r(Optional)\rVersion is the version of the shoot\u0026rsquo;s image.\rIf version is not provided, it will be defaulted to the latest version from the CloudProfile.\n\r\r\r\rShootNetworks\r\r\r(Appears on:\rSeedNetworks)\r\rShootNetworks contains the default networks CIDRs for shoots.\n\r\r\rField\rDescription\r\r\r\r\r\rpods\r\rstring\r\r\r\r(Optional)\rPods is the CIDR of the pod network.\n\r\r\r\rservices\r\rstring\r\r\r\r(Optional)\rServices is the CIDR of the service network.\n\r\r\r\rShootPurpose\r(string alias)\n\r\r(Appears on:\rShootSpec)\r\rShootPurpose is a type alias for string.\nShootSpec\r\r\r(Appears on:\rShoot)\r\rShootSpec is the specification of a Shoot.\n\r\r\rField\rDescription\r\r\r\r\r\raddons\r\rAddons\r\r\r\r\r(Optional)\rAddons contains information about enabled/disabled addons and their configuration.\n\r\r\r\rcloudProfileName\r\rstring\r\r\r\rCloudProfileName is a name of a CloudProfile object.\n\r\r\r\rdns\r\rDNS\r\r\r\r\r(Optional)\rDNS contains information about the DNS settings of the Shoot.\n\r\r\r\rextensions\r\r[]Extension\r\r\r\r\r(Optional)\rExtensions contain type and provider information for Shoot extensions.\n\r\r\r\rhibernation\r\rHibernation\r\r\r\r\r(Optional)\rHibernation contains information whether the Shoot is suspended or not.\n\r\r\r\rkubernetes\r\rKubernetes\r\r\r\r\rKubernetes contains the version and configuration settings of the control plane components.\n\r\r\r\rnetworking\r\rNetworking\r\r\r\r\rNetworking contains information about cluster networking such as CNI Plugin type, CIDRs, \u0026hellip;etc.\n\r\r\r\rmaintenance\r\rMaintenance\r\r\r\r\r(Optional)\rMaintenance contains information about the time window for maintenance operations and which\roperations should be performed.\n\r\r\r\rmonitoring\r\rMonitoring\r\r\r\r\r(Optional)\rMonitoring contains information about custom monitoring configurations for the shoot.\n\r\r\r\rprovider\r\rProvider\r\r\r\r\rProvider contains all provider-specific and provider-relevant information.\n\r\r\r\rpurpose\r\rShootPurpose\r\r\r\r\r(Optional)\rPurpose is the purpose class for this cluster.\n\r\r\r\rregion\r\rstring\r\r\r\rRegion is a name of a region.\n\r\r\r\rsecretBindingName\r\rstring\r\r\r\rSecretBindingName is the name of the a SecretBinding that has a reference to the provider secret.\rThe credentials inside the provider secret will be used to create the shoot in the respective account.\n\r\r\r\rseedName\r\rstring\r\r\r\r(Optional)\rSeedName is the name of the seed cluster that runs the control plane of the Shoot.\n\r\r\r\rShootStatus\r\r\r(Appears on:\rShoot)\r\rShootStatus holds the most recently observed status of the Shoot cluster.\n\r\r\rField\rDescription\r\r\r\r\r\rconditions\r\r[]Condition\r\r\r\r\r(Optional)\rConditions represents the latest available observations of a Shoots\u0026rsquo;s current state.\n\r\r\r\rconstraints\r\r[]Condition\r\r\r\r\r(Optional)\rConstraints represents conditions of a Shoot\u0026rsquo;s current state that constraint some operations on it.\n\r\r\r\rgardener\r\rGardener\r\r\r\r\rGardener holds information about the Gardener which last acted on the Shoot.\n\r\r\r\rhibernated\r\rbool\r\r\r\rIsHibernated indicates whether the Shoot is currently hibernated.\n\r\r\r\rlastOperation\r\rLastOperation\r\r\r\r\r(Optional)\rLastOperation holds information about the last operation on the Shoot.\n\r\r\r\rlastErrors\r\r[]LastError\r\r\r\r\r(Optional)\rLastErrors holds information about the last occurred error(s) during an operation.\n\r\r\r\robservedGeneration\r\rint64\r\r\r\r(Optional)\rObservedGeneration is the most recent generation observed for this Shoot. It corresponds to the\rShoot\u0026rsquo;s generation, which is updated on mutation by the API Server.\n\r\r\r\rretryCycleStartTime\r\rKubernetes meta/v1.Time\r\r\r\r\r(Optional)\rRetryCycleStartTime is the start time of the last retry cycle (used to determine how often an operation\rmust be retried until we give up).\n\r\r\r\rseedName\r\rstring\r\r\r\r(Optional)\rSeedName is the name of the seed cluster that runs the control plane of the Shoot. This value is only written\rafter a successful create/reconcile operation. It will be used when control planes are moved between Seeds.\n\r\r\r\rtechnicalID\r\rstring\r\r\r\rTechnicalID is the name that is used for creating the Seed namespace, the infrastructure resources, and\rbasically everything that is related to this particular Shoot.\n\r\r\r\ruid\r\rk8s.io/apimachinery/pkg/types.UID\r\r\r\r\rUID is a unique identifier for the Shoot cluster to avoid portability between Kubernetes clusters.\rIt is used to compute unique hashes.\n\r\r\r\rVersionClassification\r(string alias)\n\r\r(Appears on:\rExpirableVersion)\r\rVersionClassification is the logical state of a version according to https://github.com/gardener/gardener/blob/master/docs/operations/versioning.md\nVolume\r\r\r(Appears on:\rWorker)\r\rVolume contains information about the volume type and size.\n\r\r\rField\rDescription\r\r\r\r\r\rname\r\rstring\r\r\r\r(Optional)\rName of the volume to make it referencable.\n\r\r\r\rtype\r\rstring\r\r\r\r(Optional)\rType is the type of the volume.\n\r\r\r\rsize\r\rstring\r\r\r\rVolumeSize is the size of the volume.\n\r\r\r\rencrypted\r\rbool\r\r\r\r(Optional)\rEncrypted determines if the volume should be encrypted.\n\r\r\r\rVolumeType\r\r\r(Appears on:\rCloudProfileSpec)\r\rVolumeType contains certain properties of a volume type.\n\r\r\rField\rDescription\r\r\r\r\r\rclass\r\rstring\r\r\r\rClass is the class of the volume type.\n\r\r\r\rname\r\rstring\r\r\r\rName is the name of the volume type.\n\r\r\r\rusable\r\rbool\r\r\r\r(Optional)\rUsable defines if the volume type can be used for shoot clusters.\n\r\r\r\rWorker\r\r\r(Appears on:\rProvider)\r\rWorker is the base definition of a worker group.\n\r\r\rField\rDescription\r\r\r\r\r\rannotations\r\rmap[string]string\r\r\r\r(Optional)\rAnnotations is a map of key/value pairs for annotations for all the Node objects in this worker pool.\n\r\r\r\rcaBundle\r\rstring\r\r\r\r(Optional)\rCABundle is a certificate bundle which will be installed onto every machine of this worker pool.\n\r\r\r\rcri\r\rCRI\r\r\r\r\r(Optional)\rCRI contains configurations of CRI support of every machine in the worker pool\n\r\r\r\rkubernetes\r\rWorkerKubernetes\r\r\r\r\r(Optional)\rKubernetes contains configuration for Kubernetes components related to this worker pool.\n\r\r\r\rlabels\r\rmap[string]string\r\r\r\r(Optional)\rLabels is a map of key/value pairs for labels for all the Node objects in this worker pool.\n\r\r\r\rname\r\rstring\r\r\r\rName is the name of the worker group.\n\r\r\r\rmachine\r\rMachine\r\r\r\r\rMachine contains information about the machine type and image.\n\r\r\r\rmaximum\r\rint32\r\r\r\rMaximum is the maximum number of VMs to create.\n\r\r\r\rminimum\r\rint32\r\r\r\rMinimum is the minimum number of VMs to create.\n\r\r\r\rmaxSurge\r\rk8s.io/apimachinery/pkg/util/intstr.IntOrString\r\r\r\r\r(Optional)\rMaxSurge is maximum number of VMs that are created during an update.\n\r\r\r\rmaxUnavailable\r\rk8s.io/apimachinery/pkg/util/intstr.IntOrString\r\r\r\r\r(Optional)\rMaxUnavailable is the maximum number of VMs that can be unavailable during an update.\n\r\r\r\rproviderConfig\r\rProviderConfig\r\r\r\r\r(Optional)\rProviderConfig is the provider-specific configuration for this worker pool.\n\r\r\r\rtaints\r\r[]Kubernetes core/v1.Taint\r\r\r\r\r(Optional)\rTaints is a list of taints for all the Node objects in this worker pool.\n\r\r\r\rvolume\r\rVolume\r\r\r\r\r(Optional)\rVolume contains information about the volume type and size.\n\r\r\r\rdataVolumes\r\r[]Volume\r\r\r\r\r(Optional)\rDataVolumes contains a list of additional worker volumes.\n\r\r\r\rkubeletDataVolumeName\r\rstring\r\r\r\r(Optional)\rKubeletDataVolumeName contains the name of a dataVolume that should be used for storing kubelet state.\n\r\r\r\rzones\r\r[]string\r\r\r\r(Optional)\rZones is a list of availability zones that are used to evenly distribute this worker pool. Optional\ras not every provider may support availability zones.\n\r\r\r\rWorkerKubernetes\r\r\r(Appears on:\rWorker)\r\rWorkerKubernetes contains configuration for Kubernetes components related to this worker pool.\n\r\r\rField\rDescription\r\r\r\r\r\rkubelet\r\rKubeletConfig\r\r\r\r\r(Optional)\rKubelet contains configuration settings for all kubelets of this worker pool.\n\r\r\r\r\r"
},
{
	"uri": "https://gardener.cloud/documentation/050-tutorials/content/howto/create-delete-shoot/",
	"title": "Create / Delete a Shoot cluster",
	"tags": [],
	"description": "Creating / Deleting a Shoot cluster",
	"content": "Create a Shoot Cluster As you have already prepared an example Shoot manifest in the steps described in the development documentation, please open another Terminal pane/window with the KUBECONFIG environment variable pointing to the Garden development cluster and send the manifest to the Kubernetes API server:\n$ kubectl apply -f your-shoot-aws.yaml\rYou should see that the Gardener has immediately picked up your manifest and started to deploy the Shoot cluster.\nIn order to investigate what is happening in the Seed cluster, please download its proper Kubeconfig yourself (see next paragraph). The namespace of the Shoot cluster in the Seed cluster will look like that: shoot-johndoe-johndoe-1, whereas the first johndoe is your namespace in the Garden cluster (also called \u0026ldquo;project\u0026rdquo;) and the johndoe-1 suffix is the actual name of the Shoot cluster.\nTo connect to the newly created Shoot cluster, you must download its Kubeconfig as well. Please connect to the proper Seed cluster, navigate to the Shoot namespace, and download the Kubeconfig from the kubecfg secret in that namespace.\nDelete a Shoot Cluster In order to delete your cluster, you have to set an annotation confirming the deletion first, and trigger the deletion after that. You can use the prepared delete shoot script which takes the Shoot name as first parameter. The namespace can be specified by the second parameter, but it is optional. If you don't state it, it defaults to your namespace (the username you are logged in with to your machine).\n$ ./hack/usage/delete shoot johndoe-1 johndoe\r( hack bash script can be found here https://github.com/gardener/gardener/blob/master/hack/usage/delete)\nConfigure a Shoot cluster alert receiver The receiver of the Shoot alerts can be configured from the .spec.monitoring.alerting.emailReceivers section in the Shoot specification. The value of the field has to be a list of valid mail addresses.\nThe alerting for the Shoot clusters is handled by the Prometheus Alertmanager. The Alertmanager will be deployed next to the control plane when the Shoot resource specifies .spec.monitoring.alerting.emailReceivers and if a SMTP secret exists.\nIf the field gets removed then the Alertmanager will be also removed during the next reconcilation of the cluster. The opposite is also valid if the field is added to an existing cluster.\n"
},
{
	"uri": "https://gardener.cloud/documentation/050-tutorials/content/howto/gardener_aws/",
	"title": "Create a kubernetes cluster in AWS with Gardener",
	"tags": [],
	"description": "How to create a Kubernetes Cluster with Gardener in AWS",
	"content": "Introduction Creating a Kubernetes cluster in an AWS Account is easy and the Gardener UI should be self-explanatory.\nGardener Create a new Project in Gardener Create new Project\nCopy policy from the Gardener AWS Create new policy Create new policy\nCreate a new technical user Create a new technical user\nsave the keys of the user, you will need them later on\rGardener Add AWS Secret Create a new Cluster Create a new cluster\nCopy kubeconfig "
},
{
	"uri": "https://gardener.cloud/documentation/050-tutorials/content/howto/gardener_gcp/",
	"title": "Create a kubernetes cluster on GCP with Gardener",
	"tags": [],
	"description": "How to create a Kubernetes Cluster with Gardener on GCP",
	"content": "Introduction Creating a Kubernetes cluster in the GCP Account is easy and the Gardener UI should be self-explanatory.\nGardener Create a new Project in Gardener Create new Project\nCheck which roles are required by the Gardener GCP Create a new serviceaccount and assign roles Create a new serviceaccount\nCreate key for the serviceaccount Download the key of the serviceaccount as json save the keys of the user, you will need it later on\nEnable the Google compute API Enable the Google compute API Enable the Google IAM API Enable the Google IAM API Gardener Add GCP Secret Create a new Cluster Create a new cluster\nCopy kubeconfig "
},
{
	"uri": "https://gardener.cloud/documentation/050-tutorials/content/howto/create-shoot-into-existing-aws-vpc/",
	"title": "Create a Shoot cluster into existing AWS VPC",
	"tags": [],
	"description": "Create a Shoot cluster into existing AWS VPC",
	"content": "Create a Shoot cluster into existing AWS VPC Gardener can create a new VPC, or use an existing one for your Shoot cluster. Depending on your needs you may want to create Shoot(s) into already created VPC. The tutorial describes how to create a Shoot cluster into existing AWS VPC. The steps are identical for Alicloud, Azure, and GCP. Please note that the existing VPC must be in the same region like the shoot cluster that you want to deploy into the VPC.\nTL;DR If .spec.provider.infrastructureConfig.networks.vpc.cidr is specified, Gardener will create a new VPC with the given CIDR block and respectively will delete it on Shoot deletion.\nIf .spec.provider.infrastructureConfig.networks.vpc.id is specified, Gardener will use the existing VPC and respectively won't delete it on Shoot deletion.\n It's not recommended to create a Shoot cluster into VPC that is managed by Gardener (that is created for another Shoot cluster). In this case the deletion of the initial Shoot cluster will fail to delete the VPC because there will be resources attached to it.\nGardener won't delete any manually created (unmanaged) resources in your cloud provider account.\n 1. Configure AWS CLI The aws configure command is a convenient way to setup your AWS CLI. It will prompt you for your credentials and settings which will be used in the following AWS CLI invocations.\n$ aws configure\rAWS Access Key ID [None]: \u0026lt;ACCESS_KEY_ID\u0026gt;\rAWS Secret Access Key [None]: \u0026lt;SECRET_ACCESS_KEY\u0026gt;\rDefault region name [None]: \u0026lt;DEFAULT_REGION\u0026gt;\rDefault output format [None]: \u0026lt;DEFAULT_OUTPUT_FORMAT\u0026gt;\r2. Create VPC $ aws ec2 create-vpc --cidr-block \u0026lt;cidr-block\u0026gt;\r{\r\u0026#34;Vpc\u0026#34;: {\r\u0026#34;VpcId\u0026#34;: \u0026#34;vpc-ff7bbf86\u0026#34;,\r\u0026#34;InstanceTenancy\u0026#34;: \u0026#34;default\u0026#34;,\r\u0026#34;Tags\u0026#34;: [],\r\u0026#34;CidrBlockAssociations\u0026#34;: [\r{\r\u0026#34;AssociationId\u0026#34;: \u0026#34;vpc-cidr-assoc-6e42b505\u0026#34;,\r\u0026#34;CidrBlock\u0026#34;: \u0026#34;10.0.0.0/16\u0026#34;,\r\u0026#34;CidrBlockState\u0026#34;: {\r\u0026#34;State\u0026#34;: \u0026#34;associated\u0026#34;\r}\r}\r],\r\u0026#34;Ipv6CidrBlockAssociationSet\u0026#34;: [],\r\u0026#34;State\u0026#34;: \u0026#34;pending\u0026#34;,\r\u0026#34;DhcpOptionsId\u0026#34;: \u0026#34;dopt-38f7a057\u0026#34;,\r\u0026#34;CidrBlock\u0026#34;: \u0026#34;10.0.0.0/16\u0026#34;,\r\u0026#34;IsDefault\u0026#34;: false\r}\r}\r3. Create Internet Gateway Gardener also requires that an internet gateway is attached to the VPC. You can create one using:\n$ aws ec2 create-internet-gateway\r{\r\u0026#34;InternetGateway\u0026#34;: {\r\u0026#34;Tags\u0026#34;: [],\r\u0026#34;InternetGatewayId\u0026#34;: \u0026#34;igw-c0a643a9\u0026#34;,\r\u0026#34;Attachments\u0026#34;: []\r}\r}\rand attach it to the VPC using:\n$ aws ec2 attach-internet-gateway --internet-gateway-id igw-c0a643a9 --vpc-id vpc-ff7bbf86\r4. Create the Shoot Prepare your Shoot manifest (you could check the example manifests). Please make sure that you choose the region in which you had created the VPC earlier (step 2). Also, put your VPC ID in the .spec.provider.infrastructureConfig.networks.vpc.id field:\nspec:\rregion: \u0026lt;aws-region-of-vpc\u0026gt;\rprovider:\r type: aws\rinfrastructureConfig:\rapiVersion: aws.provider.extensions.gardener.cloud/v1alpha1\rkind: InfrastructureConfig\rnetworks:\rvpc:\rid: vpc-ff7bbf86\r# ...\r Apply your Shoot manifest.\n$ kubectl apply -f your-shoot-aws.yaml\rEnsure that the Shoot cluster is properly created.\n$ kubectl get shoot $SHOOT_NAME -n $SHOOT_NAMESPACE\rNAME CLOUDPROFILE VERSION SEED DOMAIN OPERATION PROGRESS APISERVER CONTROL NODES SYSTEM AGE\r\u0026lt;SHOOT_NAME\u0026gt; aws 1.15.0 aws \u0026lt;SHOOT_DOMAIN\u0026gt; Succeeded 100 True True True True 20m\r"
},
{
	"uri": "https://gardener.cloud/curated-links/",
	"title": "Curated Links",
	"tags": [],
	"description": "",
	"content": "Curated Links\rA curated list of Kubernetes resources and projects\r\r\r\rA curated list of awesome kubernetes sources Inspired by @sindresorhus\u0026rsquo; awesome\nSetup  Install Docker for Mac Install Docker for Windows Run a Kubernetes Cluster on your local machine  A place that marks the beginning of a journey  Kubernetes Community Overview and Contributions Guide by Ihor Dvoretskyi An Intro to Google’s Kubernetes and How to Use It by Laura Frank Getting Started on Kubernetes by Rajdeep Dua Kubernetes: The Future of Cloud Hosting by Meteorhacks Kubernetes by Google by Gaston Pantana Key Concepts by Arun Gupta Application Containers: Kubernetes and Docker from Scratch by Keith Tenzer Learn the Kubernetes Key Concepts in 10 Minutes by Omer Dawelbeit Top Reasons Businesses Should Move to Kubernetes Now by Mike Johnston The Children's Illustrated Guide to Kubernetes by Deis :-) The ‘kubectl run’ command by Michael Hausenblas Docker Kubernetes Lab Handbook by Peng Xiao  Interactive Learning Environments Learn Kubernetes using an interactive environment without requiring downloads or configuration\n Interactive Tutorial Katacoda Play with Kubernetes Kubernetes Bootcamp  Massive Open Online Courses / Tutorials List of available free online courses(MOOC) and tutorials\nCourses  Scalable Microservices with Kubernetes at Udacity Introduction to Kubernetes at edX  Tutorials  Kubernetes Tutorials by Kubernetes Team Kubernetes By Example by OpenShift Team Kubernetes Tutorial by Tutorialspoint  Package Managers  Helm KPM  RPC  gRPC Micro  Secret generation and management  Vault auth plugin backend: Kubernetes Vault controller kube-lego k8sec kubernetes-vault kubesec - Secure Secret management  Machine Learning  TensorFlow k8s mxnet-operator - Tools for ML/MXNet on Kubernetes. kubeflow - Machine Learning Toolkit for Kubernetes. seldon-core - Open source framework for deploying machine learning models on Kubernetes  Raspberry Pi Some of the awesome findings and experiments on using Kubernetes with Raspberry Pi.\n Kubecloud Setting up a Kubernetes on ARM cluster Setup Kubernetes on a Raspberry Pi Cluster easily the official way! by Mathias Renner and Lucas Käldström How to Build a Kubernetes Cluster with ARM Raspberry Pi then run .NET Core on OpenFaas by Scott Hanselman  Contributing Contributions are most welcome!\nThis list is just getting started, please contribute to make it super awesome.\n"
},
{
	"uri": "https://gardener.cloud/documentation/050-tutorials/content/howto/secure-seccomp/",
	"title": "Custom Seccomp profile",
	"tags": [],
	"description": "Custom Seccomp profile",
	"content": "Custom Seccomp profile Context Seccomp (secure computing mode) is a security facility in the Linux kernel for restricting the set of system calls applications can make.\nStarting from Kubernetes v1.3.0 the Seccomp feature is in Alpha. To configure it on a Pod, the following annotations can be used:\n seccomp.security.alpha.kubernetes.io/pod: \u0026lt;seccomp-profile\u0026gt; where \u0026lt;seccomp-profile\u0026gt; is the seccomp profile to apply to all containers in a Pod. container.seccomp.security.alpha.kubernetes.io/\u0026lt;container-name\u0026gt;: \u0026lt;seccomp-profile\u0026gt; where \u0026lt;seccomp-profile\u0026gt; is the seccomp profile to apply to \u0026lt;container-name\u0026gt; in a Pod.  More details can be found in the PodSecurityPolicy documentation.\nInstallation of custom profile By default, kubelet loads custom Seccomp profiles from /var/lib/kubelet/seccomp/. There are two ways in which Seccomp profiles can be added to a Node:\n to be baked in the machine image to be added at runtime.  This guide focuses on creating those profiles via a DaemonSet.\nCreate a file called seccomp-profile.yaml with the following content:\napiVersion: v1\rkind: ConfigMap\rmetadata:\rname: seccomp-profile\rnamespace: kube-system\rdata:\rmy-profile.json: |\r{\r \u0026#34;defaultAction\u0026#34;: \u0026#34;SCMP_ACT_ALLOW\u0026#34;,\r\u0026#34;syscalls\u0026#34;: [\r{\r\u0026#34;name\u0026#34;: \u0026#34;chmod\u0026#34;,\r\u0026#34;action\u0026#34;: \u0026#34;SCMP_ACT_ERRNO\u0026#34;\r}\r]\r}\r The policy above is a very simple one and not siutable for complex applications. The default docker profile can be used a reference. Feel free to modify it to your needs.\n Apply the ConfigMap in your cluster:\n$ kubectl apply -f seccomp-profile.yaml\rconfigmap/seccomp-profile created\rThe next steps is to create the DaemonSet seccomp installer. It's going to copy the policy from above in /var/lib/kubelet/seccomp/my-profile.json.\nCreate a file called seccomp-installer.yaml with the following content:\napiVersion: apps/v1\rkind: DaemonSet\rmetadata:\rname: seccomp\rnamespace: kube-system\rlabels:\rsecurity: seccomp\rspec:\rselector:\rmatchLabels:\rsecurity: seccomp\rtemplate:\rmetadata:\rlabels:\rsecurity: seccomp\rspec:\rinitContainers:\r- name: installer\rimage: alpine:3.10.0\rcommand: [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;cp -r -L /seccomp/*.json /host/seccomp/\u0026#34;]\rvolumeMounts:\r- name: profiles\rmountPath: /seccomp\r- name: hostseccomp\rmountPath: /host/seccomp\rreadOnly: false\rcontainers:\r- name: pause\rimage: k8s.gcr.io/pause:3.1\rterminationGracePeriodSeconds: 5\rvolumes:\r- name: hostseccomp\rhostPath:\rpath: /var/lib/kubelet/seccomp\r- name: profiles\rconfigMap:\rname: seccomp-profile\rCreate the installer and wait until it's ready on all Nodes:\n$ kubectl apply -f seccomp-installer.yaml\rdaemonset.apps/seccomp-installer created\r$ kubectl -n kube-system get pods -l security=seccomp\rNAME READY STATUS RESTARTS AGE\rseccomp-installer-wjbxq 1/1 Running 0 21s\rCreate a Pod using custom Seccomp profile Finally we want to create a profile which uses our new Seccomp profile my-profile.json.\nCreate a file called my-seccomp-pod.yaml with the following content:\napiVersion: v1\rkind: Pod\rmetadata:\rname: seccomp-app\rnamespace: default\rannotations:\rseccomp.security.alpha.kubernetes.io/pod: \u0026#34;localhost/my-profile.json\u0026#34;\r# you can specify seccomp profile per container. If you add another profile you can configure\r # it for a specific container - \u0026#39;pause\u0026#39; in this case.\r # container.seccomp.security.alpha.kubernetes.io/pause: \u0026#34;localhost/some-other-profile.json\u0026#34;\r spec:\rcontainers:\r- name: pause\rimage: k8s.gcr.io/pause:3.1\rCreate the Pod and see that's running:\n$ kubectl apply -f my-seccomp-pod.yaml\rpod/seccomp-app created\r$ kubectl get pod seccomp-app\rNAME READY STATUS RESTARTS AGE\rseccomp-app 1/1 Running 0 42s\rThroubleshooting If an invalid or not existing profile is used then the Pod will be stuck in ContainerCreating phase:\nbroken-seccomp-pod.yaml:\napiVersion: v1\rkind: Pod\rmetadata:\rname: broken-seccomp\rnamespace: default\rannotations:\rseccomp.security.alpha.kubernetes.io/pod: \u0026#34;localhost/not-existing-profile.json\u0026#34;\rspec:\rcontainers:\r- name: pause\rimage: k8s.gcr.io/pause:3.1\r$ kubectl apply -f broken-seccomp-pod.yaml\rpod/broken-seccomp created\r$ kubectl get pod broken-seccomp\rNAME READY STATUS RESTARTS AGE\rbroken-seccomp 1/1 ContainerCreating 0 2m\r$ kubectl describe pod broken-seccomp\rName: broken-seccomp\rNamespace: default\r....\rEvents:\rType Reason Age From Message\r---- ------ ---- ---- -------\rNormal Scheduled 18s default-scheduler Successfully assigned kube-system/broken-seccomp to docker-desktop\rWarning FailedCreatePodSandBox 4s (x2 over 18s) kubelet, docker-desktop Failed create pod sandbox: rpc error: code = Unknown desc = failed to make sandbox docker config for pod \u0026quot;broken-seccomp\u0026quot;: failed to generate sandbox security options\rfor sandbox \u0026quot;broken-seccomp\u0026quot;: failed to generate seccomp security options for container: cannot load seccomp profile \u0026quot;/var/lib/kubelet/seccomp/not-existing-profile.json\u0026quot;: open /var/lib/kubelet/seccomp/not-existing-profile.json: no such file or directory\rFurther reading  https://en.wikipedia.org/wiki/Seccomp https://docs.docker.com/engine/security/seccomp https://lwn.net/Articles/656307/ http://man7.org/conf/lpc2015/limiting_kernel_attack_surface_with_seccomp-LPC_2015-Kerrisk.pdf  "
},
{
	"uri": "https://gardener.cloud/components/dashboard/",
	"title": "Dashboard",
	"tags": [],
	"description": "",
	"content": "Gardener Dashboard  \nDemo Development Setup Install Install all dependencies\nyarn\rConfiguration KUBECONFIG If the dashboard is not running in the Garden Cluster you have to point the kubeconfig to Garden Cluster. This can be done in the default kubeconfig file in ${HOME}/.kube/config or by the KUBECONFIG environment variable.\nGARDENER_CONFIG The configuration file of the Gardener Dashboard can be specified as first command line argument or as environment variable GARDENER_CONFIG at the server process. If nothing is specified the default location is ${HOME}/.gardener/config.yaml.\nA local configuration example for minikube and dex could look like follows:\nport: 3030\rlogLevel: debug\rlogFormat: text\rapiServerUrl: https://minkube # garden cluster kube-apiserver url\r sessionSecret: c2VjcmV0 # symetric key used for encryption\r oidc:\rissuer: https://minikube:32001\rclient_id: dashboard\rclient_secret: c2VjcmV0 # oauth client secret\r redirect_uri: http://localhost:8080/auth/callback\rscope: \u0026#39;openid email profile groups audience:server:client_id:dashboard audience:server:client_id:kube-kubectl\u0026#39;\rclockTolerance: 15\rfrontend:\rdashboardUrl:\rpathname: /api/v1/namespaces/kube-system/services/kubernetes-dashboard/proxy/\rdefaultHibernationSchedule:\revaluation:\r- start: 00 17 * * 1,2,3,4,5\rdevelopment:\r- start: 00 17 * * 1,2,3,4,5\rend: 00 08 * * 1,2,3,4,5\rproduction: ~\rRun locally (during development) Concurrently run the backend server (port 3030) and the frontend server (port 8080) both with hot reload enabled.\nyarn serve\rAll request to /api, /auth and /config.json will be proxied by default to the backend server.\nBuild Build docker image locally.\nmake build\rPush Push docker image to Google Container Registry.\nmake push\rThis command expects a valid gcloud configuration named gardener.\ngcloud config configurations describe gardener\ris_active: true\rname: gardener\rproperties:\rcore:\raccount: john.doe@example.org\rproject: johndoe-1008\rPeople The following SAP developers contributed to this project until this initial contribution was published as open source.\n   contributor commits (%) +lines -lines first commit last commit     Holger Koser 313 (42%) 57878 18562 2017-07-13 2018-01-23   Andreas Herz 307 (41%) 13666 11099 2017-07-14 2017-10-27   Peter Sutter 99 (13%) 4838 3967 2017-11-07 2018-01-23   Gross, Lukas 31 (4%) 400 267 2018-01-10 2018-01-23    It is derived from the historical, internal gardener-ui repository at commit eeb623d60c86e6037c0e1dc2bdd9e54663bf41a8.\nLicense Apache License 2.0\nCopyright 2020 The Gardener Authors\n"
},
{
	"uri": "https://gardener.cloud/documentation/045_contribute/10_code/20_dependencies/",
	"title": "Dependencies",
	"tags": [],
	"description": "",
	"content": "Testing We follow the BDD-style testing principles and are leveraging the Ginkgo framework along with Gomega as matcher library. In order to execute the existing tests, you can use\nmake test # runs tests\rmake verify # runs static code checks and test\rThere is an additional command for analyzing the code coverage of the tests. Ginkgo will generate standard Golang cover profiles which will be translated into a HTML file by the Go Cover Tool. Another command helps you to clean up the filesystem from the temporary cover profile files and the HTML report:\nmake test-cov\ropen gardener.coverage.html\rmake test-clean\rDependency Management We are using go modules for depedency management. In order to add a new package dependency to the project, you can perform go get \u0026lt;PACKAGE\u0026gt;@\u0026lt;VERSION\u0026gt; or edit the go.mod file and append the package along with the version you want to use.\nUpdating Dependencies The Makefile contains a rule called revendor which performs go mod vendor and go mod tidy. go mod vendor resets the main module's vendor directory to include all packages needed to build and test all the main module's packages. It does not include test code for vendored packages. go mod tidy makes sure go.mod matches the source code in the module. It adds any missing modules necessary to build the current module's packages and dependencies, and it removes unused modules that don't provide any relevant packages.\nmake revendor\rThe dependencies are installed into the vendor folder which should be added to the VCS.\n:warning: Make sure that you test the code after you have updated the dependencies!\n"
},
{
	"uri": "https://gardener.cloud/documentation/045_contribute/10_code/27_deploy_into_cluster/",
	"title": "Deploy into a Cluster",
	"tags": [],
	"description": "",
	"content": "Deploying the Gardener into a Kubernetes cluster Similar to Kubernetes, Gardener consists out of control plane components (Gardener API server, Gardener controller manager, Gardener scheduler), and an agent component (Gardenlet). The control plane is deployed in the so-called garden cluster while the agent is installed into every seed cluster. Please note that it is possible to use the garden cluster as seed cluster by simply deploying the Gardenlet into it.\nWe are providing Helm charts in order to manage the various resources of the components. Please always make sure that you use the Helm chart version that matches the Gardener version you want to deploy.\nDeploying the Gardener control plane (API server, controller manager, scheduler) The configuration values depict the various options to configure the different components. Please consult this document to get a detailed explanation of what can be configured for which component. Also note that all resources and deployments need to be created in the garden namespace (not overrideable).\nAfter preparing your values in a separate controlplane-values.yaml file, you can run the following command against your garden cluster:\nhelm install charts/gardener/controlplane \\\r --namespace garden \\\r --name gardener-controlplane \\\r -f gardener-values.yaml \\\r --wait\rDeploying Gardener extensions Gardener is an extensible system that does not contain the logic for provider-specific things like DNS management, cloud infrastructures, network plugins, operating system configs, and many more.\nYou have to install extension controllers for these parts. Please consult the documentation regarding extensions to get more information.\nDeploying the Gardener agent (Gardenlet) The Gardenlet requires a bootstrap token as well as a bootstrap kubeconfig in order to properly register itself with the Gardener control plane.\nThe configuration values depict the various options to configure it. Please consult this document to get a detailed explanation of what can be configured.\nPrepare your values in a separate gardenlet-values.yaml file:\n Create a bootstrap token secret in the kube-system namespace of the garden cluster (see this and this). Create a bootstrap kubeconfig containing this token:  apiVersion: v1\rkind: Config\rcurrent-context: gardenlet-bootstrap@default\rclusters:\r- cluster:\rcertificate-authority-data: \u0026lt;ca-of-garden-cluster\u0026gt;\rserver: https://\u0026lt;endpoint-of-garden-cluster\u0026gt;\r name: default\rcontexts:\r- context:\rcluster: default\ruser: gardenlet-bootstrap\rname: gardenlet-bootstrap@default\rusers:\r- name: gardenlet-bootstrap\ruser:\rtoken: \u0026lt;bootstrap-token\u0026gt;\rProvide this bootstrap kubeconfig together with a desired name and namespace to the Gardenlet Helm chart values here:  gardenClientConnection:\rbootstrapKubeconfig:\rname: gardenlet-kubeconfig-bootstrap\rnamespace: garden\rkubeconfig: |\r\u0026lt;bootstrap-kubeconfig\u0026gt;\r Define a name and namespace where the Gardenlet shall store the real kubeconfig it creates during the bootstrap process here:  gardenClientConnection:\rkubeconfigSecret:\rname: gardenlet-kubeconfig\rnamespace: garden\rDefine either seedSelector or seedConfig (see this document  Now you are ready to deploy the Helm chart:\nhelm install charts/gardener/gardenlet \\\r --namespace garden \\\r --name gardenlet \\\r -f gardenlet-values.yaml \\\r --wait\r:warning: A current prerequisite of Kubernetes clusters that are used as seeds is to have a pre-deployed nginx-ingress-controller to make the Gardener work properly. Moreover, there should exist a DNS record *.ingress.\u0026lt;SEED-CLUSTER-DOMAIN\u0026gt; where \u0026lt;SEED-CLUSTER-DOMAIN\u0026gt; is the value of the .dns.ingressDomain field of a Seed cluster resource (or the respective Gardenlet configuration).\n"
},
{
	"uri": "https://gardener.cloud/documentation/045_contribute/10_code/30_deploy_seed_into_aks/",
	"title": "Deploy into AKS",
	"tags": [],
	"description": "",
	"content": "Deploying the previous Gardener versions and a Seed into an AKS cluster This document demonstrates how to install Gardener into an existing AKS cluster. We'll use a single cluster to host both Gardener and a Seed to the same cluster for the sake of simplicity .\nPlease note that this document is to provide you an example installation and is not to be used in a production environment since there are some certificates hardcoded, non-HA and non-TLS-enabled etcd setup.\nHigh Level Overview In this example we'll follow these steps to create a Seed cluster on AKS:\n Deploying the Gardener and a Seed into an AKS cluster High Level Overview Prerequisites  AWS credentials for Route 53 Hosted Zone Deploy AKS cluster  Initialize Helm on the Cluster Deploy stable/nginx-ingress chart to AKS Create wildcard DNS record for the ingress   Create Azure Service Principle to get Azure credentials Install gardenctl   Install Gardener  Create garden namespace Deploy etcd Deploy Gardener Helm Chart   Create a CloudProfile Define Seed cluster in Gardener  Create the Seed resource definition with its Secret   Create a Shoot cluster  Create a Project (namespace) for Shoots Create a SecretBinding and related Secret Create the Shoot resource  Cluster Resources After Shoot is Created Troubleshooting Shoot Creation Issues     Access Shoot cluster Delete Shoot cluster  Prerequisites Summary of prerequisites:\n An Azure AKS cluster with:  Helm initialized, an ingress controller deployed, a wildcard DNS record pointing the ingress, az command line client configured for Azure subscription,   An Azure service principle to provide Azure credentials to Gardener, A Route53 Hosted Zone and AWS account credentials with permissions on that Route53 Zone,  aws command line client configured for this account,   gardenctl command line client configured for the AKS cluster's kubeconfig  Note: Gardener doesn't have support for Azure DNS yet (see #494). So, we use a Route53 Hosted Zone even if we are deploying on Azure.\nAWS credentials for Route 53 Hosted Zone You need to provide credentials for AWS with permission to access Route53 Hosted Zone. In this example we'll assume your domain for the Hosted Zone is .your.domain.here.\nHOSTED_ZONE_ID= # place your AWS Route53 hostedZoneID here\rCreate an AWS user, define policy to allow permission for the Hosted Zone and note the hostedZoneID, accessKeyID and secretAccessKey for later use.\nDeploy AKS cluster Here you can find a summary for creating an AKS cluster, if you already have one, skip this step.\naz group create --name garden-1 --location eastus\raz aks create --resource-group garden-1 --name garden-1 \\\r--kubernetes-version 1.11.5 \\\r--node-count 2 --node-vm-size Standard_DS4_v2 \\\r--generate-ssh-keys\raz aks get-credentials --resource-group garden-1 --name garden-1 --admin\rInitialize Helm on the Cluster Since RBAC is enabled by default we need to deploy helm with an RBAC config.\nkubectl apply -f https://raw.githubusercontent.com/Azure/helm-charts/master/docs/prerequisities/helm-rbac-config.yaml\rhelm init --service-account tiller\rDeploy stable/nginx-ingress chart to AKS At the moment the Ingress resources created by the Gardener are expecting the nginx-ingress style annotations to work.\nhelm upgrade --install \\\r--namespace kube-system \\\rnginx-ingress stable/nginx-ingress\rCreate wildcard DNS record for the ingress You need to pick a wildcard subdomain matching your Route53 Hosted Zone here. This ingress wildcard record is supposed to be part of the Seed cluster rather than Gardener cluster, in our example we'll use *.seed-1.your.domain.here.\nAssuming you have the AWS cli for your Route53 Hosted Zone is configured on your local, here we'll create the wildcard DNS record using the awless. You can also use the AWS console or any other tool of your choice to create the wildcard record:\nHOSTED_ZONE_DOMAIN=$(aws route53 get-hosted-zone --id /hostedzone/${HOSTED_ZONE_ID:?\u0026quot;HOSTED_ZONE_ID is missing\u0026quot;} --query 'HostedZone.Name' --output text)\rINGRESS_DOMAIN=\u0026quot;seed-1.${HOSTED_ZONE_DOMAIN%%.}\u0026quot;\r# Get LB IP address from `kubectl -n kube-system get svc shared-ingress-nginx-ingress-controller`\rLB_IP=$(kubectl -n kube-system get svc nginx-ingress-controller --template '{{(index .status.loadBalancer.ingress 0).ip}}')\rawless create record \\\rzone=$HOSTED_ZONE_ID \\\rname=\u0026quot;*.$INGRESS_DOMAIN\u0026quot; \\\rvalue=$LB_IP \\\rtype=A \\\rttl=300\rCreate Azure Service Principle to get Azure credentials We need client_id and client_secret to allow Gardener to reach Azure services, we can generate a pair by creating a Service Principle on Azure:\n$ az ad sp create-for-rbac --role=\u0026quot;Contributor\u0026quot;\rRetrying role assignment creation: 1/36\r{\r\u0026quot;appId\u0026quot;: \u0026quot;xxxxxx-xxx-xxxx-xxx-xxxxx\u0026quot;, #az_client_id\r\u0026quot;displayName\u0026quot;: \u0026quot;azure-cli-2018-05-23-16-15-49\u0026quot;,\r\u0026quot;name\u0026quot;: \u0026quot;http://azure-cli-2018-05-23-16-15-49\u0026quot;,\r\u0026quot;password\u0026quot;: \u0026quot;xxxxxx-xxx-xxxx-xxx-xxxxx\u0026quot;, #az_client_secret\r\u0026quot;tenant\u0026quot;: \u0026quot;xxxxxx-xxx-xxxx-xxx-xxxxx\u0026quot; #az_tenant_id\r}\rLet's define some env variables for later use\nCLIENT_ID= # place your Azure Service Principal appId\rCLIENT_SECRET= # place your Azure Service Principal password here\rInstall gardenctl In this example we'll be using gardenctl to interact with Gardener. You can install gardenctl following instruction in its repo: https://github.com/gardener/gardenctl\nHere is a sample configuration for gardenctl:\n$ cat ~/.garden/config\rgardenClusters:\r- name: dev\rkubeConfig: ~/.kube/config\rInstall Gardener Create garden namespace This is where we deploy Gardener components.\nkubectl apply -f example/00-namespace-garden.yaml\rDeploy etcd Since Gardener is an extension API Server, it can share the etcd backing native Kubernetes cluster's API Server, and hence explicit etcd installation is optional. But in our case we have no access to the control plane components of the AKS cluster and we have to deploy our own etcd ourselves for Gardener. Lets deploy an etcd using the gardener/etcd-backup-restore project, which is also used by the Gardener for Shoot control plane.\n# pull the etcd-backup-restore\rgit clone https://github.com/gardener/etcd-backup-restore.git\r# deploy etcd\rhelm upgrade --install \\\r--namespace garden \\\retcd etcd-backup-restore/chart \\\r--set tls=\rNote: This etcd installation doesn't provide HA. But etcd will be auto recovered by the Deployment. This could be sufficient for some deployments but may not be suitable for production usage. Also note that this etcd is not deployed with TLS enabled and doesn't use certificates for authentication.\nCheck etcd pod's health, it should have READY:2/2 and STATUS:Running:\n$ kubectl -n garden get pods\rNAME READY STATUS RESTARTS AGE\retcd-for-test-0 2/2 Running 0 1m\rDeploy Gardener Helm Chart Check (current releases)[https://github.com/gardener/gardener/releases] and pick a suitable one to install.\nGARDENER_RELEASE=0.17.1\rgardener-controller-manager will need to maintain some DNS records for Seed. So, you need to provide Route53 credentials in the values.yaml file:\n global.controller.internalDomain.hostedZoneID global.controller.internalDomain.domain: Here pick a subdomain for your Gardener to maintain DNS records for your Shoot clusters. This domain has to be within your Route53 Hosted Zone. e.g. garden-1.your.domain.here global.controller.internalDomain.credentials global.controller.internalDomain.secretAccessKey  HOSTED_ZONE_DOMAIN=$(\raws route53 get-hosted-zone \\\r--id /hostedzone/${HOSTED_ZONE_ID:?\u0026quot;HOSTED_ZONE_ID is missing\u0026quot;} \\\r--query 'HostedZone.Name' \\\r--output text)\rHOSTED_ZONE_DOMAIN=${HOSTED_ZONE_DOMAIN%%.}\rGARDENER_DOMAIN=\u0026quot;garden-1.${HOSTED_ZONE_DOMAIN}\u0026quot;\rACCESS_KEY_ID=$(aws configure get aws_access_key_id)\rSECRET_ACCESS_KEY=$(aws configure get aws_secret_access_key)\rcat \u0026lt;\u0026lt;EOF \u0026gt; gardener-values.yaml\rglobal:\rapiserver:\rimage:\rtag: ${GARDENER_RELEASE:?\u0026quot;GARDENER_RELEASE is missing\u0026quot;}\retcd:\rservers: http://etcd-for-test-client:2379\ruseSidecar: false\rcontroller:\rimage:\rtag: ${GARDENER_RELEASE:?\u0026quot;GARDENER_RELEASE is missing\u0026quot;}\rinternalDomain:\rprovider: aws-route53\rhostedZoneID: ${HOSTED_ZONE_ID}\rdomain: ${HOSTED_ZONE_DOMAIN}\rcredentials:\rAWS_ACCESS_KEY_ID: ${ACCESS_KEY_ID}\rAWS_SECRET_ACCESS_KEY: ${SECRET_ACCESS_KEY}\rEOF\rAfter creating the gardener-values.yaml file, since chart definition in master branch can have breaking changes after the release, checkout the gardener tag for that release, and run:\ngit checkout ${GARDENER_RELEASE:?\u0026quot;GARDENER_RELEASE is missing\u0026quot;}\rhelm upgrade --install \\\r--namespace garden \\\rgarden charts/gardener \\\r-f charts/gardener/local-values.yaml \\\r-f gardener-values.yaml\rValidate the Gardener is deployed:\nhelm status garden # Wait for `STATUS: DEPLOYED`\rkubectl -n garden get deploy,pod -l app=gardener\r# Better if you leave two terminals open in for below commands, and\r# keep an eye on whats going on behind the scenes as you create/delete\r# Gardener specific resources (Seed, CloudProfile, SecretBinding, Shoot).\rkubectl -n garden logs -f deployment/gardener-apiserver # confirm no issues\rkubectl -n garden logs -f deployment/gardener-controller-manager # confirm no issues, except some \u0026quot;Failed to list *v1beta1...\u0026quot; messages\rNote: This is not meant to be used in production. You may not want to use apiserver.insecureSkipTLSVerify=true, the hardcoded apiserver certificates, and insecure (non-tls enabled) etcd. But for the sake of keeping this example simple you can just keep those values as they are.\nCreate a CloudProfile We need to create a CloudProfile to be referred from the Shoot (example/30-cloudprofile-azure.yaml):\nkubectl apply -f example/30-cloudprofile-azure.yaml\rValidate that CloudProfile is created:\nkubectl describe -f example/30-cloudprofile-azure.yaml\rDefine Seed cluster in Gardener In our setup we'll use the cluster for Gardener also as a Seed, this saves us from creating a new Kubernetes cluster. But you can also create an explicit cluster for the Seed. Seed cluster can also be placed into any other cloud provider or on prem. But keep in mind that below steps may differ if you use a different cluster for seed.\nCurrently, a Seed cluster is just a Kubeconfig for the Gardener. The seed cluster could have been created by any tool, Gardener only cares about having a valid Kubeconfig to talk to its API.\nCreate the Seed resource definition with its Secret Lets start with the required seed secret first. Here we need to provide it's cloud provider credentials and kubeconfig in the seed secret. Update example/40-secret-seed-azure.yaml and place the secrets for your environment:\n data.subscriptionID: you can learn this one with az account show data.tenantID: from az ad sp create-for-rbac output as you can see above data.clientID: from az ad sp create-for-rbac output as you can see above data.clientSecret: from az ad sp create-for-rbac output as you can see above data.kubeconfig: you can get this one with az aks get-credentials --resource-group garden-1 --name garden-1 -f - | base64)  Note: All of the above values must be base64 encoded. If you skip this it will hurt you later.\nSUBSCRIPTION_ID=$(az account list -o json | jq -r '.[] | select(.isDefault == true) | .id')\rTENANT_ID=$(az account show -o tsv --query 'tenantId')\rKUBECONFIG_FOR_SEED_CLUSTER=$(az aks get-credentials --resource-group garden-1 --name garden-1 -f -)\rsed -i \\\r-e \u0026quot;s@base64(uuid-of-subscription)@$(echo $SUBSCRIPTION_ID | tr -d '\\n' | base64)@\u0026quot; \\\r-e \u0026quot;s@base64(uuid-of-tenant)@$(echo \u0026quot;$TENANT_ID\u0026quot; | tr -d '\\n' | base64)@\u0026quot; \\\r-e \u0026quot;s@base64(uuid-of-client)@$(echo \u0026quot;${CLIENT_ID:?\u0026quot;CLIENT_ID is missing\u0026quot;}\u0026quot; | tr -d '\\n' | base64)@\u0026quot; \\\r-e \u0026quot;s@base64(client-secret)@$(echo \u0026quot;${CLIENT_SECRET:?\u0026quot;CLIENT_SECRET is missing\u0026quot;}\u0026quot; | tr -d '\\n' | base64)@\u0026quot; \\\r-e \u0026quot;s@base64(kubeconfig-for-seed-cluster)@$(echo \u0026quot;$KUBECONFIG_FOR_SEED_CLUSTER\u0026quot; | base64 -w 0)@\u0026quot; \\\rexample/40-secret-seed-azure.yaml\rAfter updating the fields, create the Seed secret:\nkubectl apply -f example/40-secret-seed-azure.yaml\rBefore creating Seed, we need to update the example/50-seed-azure.yaml file and update:\n spec.networks: IP ranges used in your AKS cluster. spec.ingressDomain: Place here the wildcard domain you have for the ingress controller (we created this record in prerequisites). Gardener doesn't create this DNS records but assumes its created ahead of time, Seed clusters are not provisioned by Gardener. spec.cloud.region: eastus (the region of the existing AKS cluster)  HOSTED_ZONE_DOMAIN=$(aws route53 get-hosted-zone --id /hostedzone/${HOSTED_ZONE_ID:?\u0026quot;HOSTED_ZONE_ID is missing\u0026quot;} --query 'HostedZone.Name' --output text)\rINGRESS_DOMAIN=\u0026quot;seed-1.${HOSTED_ZONE_DOMAIN%%.}\u0026quot;\r# discover AKS CIDRs\rNODE_CIDR=$(az network vnet list -g MC_garden-1_garden-1_eastus -o json | jq -r '.[] | .subnets[] | .addressPrefix')\rPOD_CIDR=$(kubectl -n kube-system get daemonset/kube-proxy -o yaml | grep cluster-cidr= | grep -v annotations | cut -d = -f2)\rSERVICE_CIDR=10.0.0.0/16 # This one is hardcoded for now, not easy to discover\rsed -i \\\r-e \u0026quot;s/ingressDomain: dev.azure.seed.example.com/ingressDomain: $INGRESS_DOMAIN/\u0026quot; \\\r-e \u0026quot;s/region: westeurope/region: eastus/\u0026quot; \\\r-e \u0026quot;s@nodes: 10.240.0.0/16@nodes: $NODE_CIDR@\u0026quot; \\\r-e \u0026quot;s@pods: 10.241.128.0/17@pods: $POD_CIDR@\u0026quot; \\\r-e \u0026quot;s@services: 10.241.0.0/17@services: $SERVICE_CIDR@\u0026quot; \\\rexample/50-seed-azure.yaml\rNow we are ready to create the seed:\nkubectl apply -f example/50-seed-azure.yaml\rCheck the logs in gardener-controller-manager and also wait for seed to be Ready: True. This means gardener-controller-manager is able to reach the Seed cluster with the credentials you provide.\n$ gardenctl target garden dev\rKUBECONFIG=/Users/user/.kube/config\r$ kubectl get seed azure\rNAME CLOUDPROFILE REGION INGRESS DOMAIN AVAILABLE AGE\razure azure eastus seed-1.your.domain.here True 1m\r$ gardenctl ls seeds\rseeds:\r- seed: azure\rIf something goes wrong verify that you provided right credentials, and base64 encoded strings of those in the secret. Also check the status field in the Seed resource and gardener-controller-manager logs:\n$ kubectl get seed azure -o json | jq .status\r{\r\u0026quot;conditions\u0026quot;: [\r{\r\u0026quot;lastTransitionTime\u0026quot;: \u0026quot;2018-05-31T14:56:49Z\u0026quot;,\r\u0026quot;message\u0026quot;: \u0026quot;all checks passed\u0026quot;,\r\u0026quot;reason\u0026quot;: \u0026quot;Passed\u0026quot;,\r\u0026quot;status\u0026quot;: \u0026quot;True\u0026quot;,\r\u0026quot;type\u0026quot;: \u0026quot;Available\u0026quot;\r}\r]\r}\rCreate a Shoot cluster Create a Project (namespace) for Shoots In this step we create a namespace in Gardener cluster to keep Shoot resource definitions. A project in Gardener terminology is simply a namespace that holds group of Shoots, during this example we'll deploy a single Shoot. (Mind the extra labels defined in example/00-namespace-garden-dev.yaml).\nkubectl apply -f example/05-project-dev.yaml\rYou can check the projects via gardenctl:\n$ gardenctl target garden dev\r$ kubectl get project dev\rNAME NAMESPACE STATUS OWNER CREATOR AGE\rdev garden-dev Ready john.doe@example.com client 1m\r$ kubectl get ns garden-dev\rNAME STATUS AGE\rgarden-dev Active 1m\r$ gardenctl ls projects\rprojects:\r- project: garden-dev\rCreate a SecretBinding and related Secret We'll use same Azure credentials with example/40-secret-seed-azure.yaml, this is due to the fact that we use the same Azure Subscription for the Shoot and Seed clusters. Differently from the Seed secret, in this one we don't need to provide kubeconfig since the Shoot cluster will be provisioned by Gardener, and we need to provide credentials for Route53 DNS records management.\nUpdate example/70-secret-cloudprovider-azure.yaml and place the secrets for your environment:\n data.subscriptionID: you can learn this one with az account show data.tenantID: from az ad sp create-for-rbac output as you can see above data.clientID: from az ad sp create-for-rbac output as you can see above data.clientSecret: from az ad sp create-for-rbac output as you can see above data.accessKeyID: You need to add this field for Route53 records to be updated. data.secretAccessKey: You need to add this field for Route53 records to be updated.  Note: All of the above values must be base64 encoded. If you skip this it will hurt you later.\nSUBSCRIPTION_ID=$(az account list -o json | jq -r '.[] | select(.isDefault == true) | .id')\rTENANT_ID=$(az account show -o tsv --query 'tenantId')\rACCESS_KEY_ID=$(aws configure get aws_access_key_id)\rSECRET_ACCESS_KEY=$(aws configure get aws_secret_access_key)\rsed -i \\\r-e \u0026quot;s@base64(uuid-of-subscription)@$(echo $SUBSCRIPTION_ID | tr -d '\\n' | base64)@\u0026quot; \\\r-e \u0026quot;s@base64(uuid-of-tenant)@$(echo \u0026quot;$TENANT_ID\u0026quot; | tr -d '\\n' | base64)@\u0026quot; \\\r-e \u0026quot;s@base64(uuid-of-client)@$(echo \u0026quot;${CLIENT_ID:?\u0026quot;CLIENT_ID is missing\u0026quot;}\u0026quot; | tr -d '\\n' | base64)@\u0026quot; \\\r-e \u0026quot;s@base64(client-secret)@$(echo \u0026quot;${CLIENT_SECRET:?\u0026quot;CLIENT_SECRET is missing\u0026quot;}\u0026quot; | tr -d '\\n' | base64)@\u0026quot; \\\r-e \u0026quot;\\$a\\ \\ accessKeyID: $(echo $ACCESS_KEY_ID | tr -d '\\n' | base64 )\u0026quot; \\\r-e \u0026quot;\\$a\\ \\ secretAccessKey: $(echo $SECRET_ACCESS_KEY | tr -d '\\n' | base64 )\u0026quot; \\\rexample/70-secret-cloudprovider-azure.yaml\rAfter updating the fields, create the cloud provider secret:\nkubectl apply -f example/70-secret-cloudprovider-azure.yaml\rAnd create the SecretBinding resource to allow Gardener use that secret (example/80-secretbinding-cloudprovider-azure.yaml):\nsed -i \\\r-e 's/# namespace: .*/ namespace: garden-dev/' \\\rexample/80-secretbinding-cloudprovider-azure.yaml\rkubectl apply -f example/80-secretbinding-cloudprovider-azure.yaml\rCheck the logs in gardener-controller-manager, there should not be any problems reported.\nCreate the Shoot resource Update the fields in example/90-deprecated-shoot-azure.yaml:\n spec.cloud.region: eastus (this must match the seed cluster's region) spec.dns.domain: This is used to specify the base domain for your api (and other in the future) endpoint(s). For example when johndoe-azure.garden-dev.your.domain.here is used as a value, then your apiserver is available at api.johndoe-azure.garden-dev.your.domain.here spec.dns.hostedZoneID: This field doesn't exist in the example you need to add this field and place the Route53 Hosted Zone ID.  HOSTED_ZONE_DOMAIN=$(aws route53 get-hosted-zone --id /hostedzone/${HOSTED_ZONE_ID:?\u0026quot;HOSTED_ZONE_ID is missing\u0026quot;} --query 'HostedZone.Name' --output text)\rSHOOT_DOMAIN=\u0026quot;johndoe-azure.garden-dev.${HOSTED_ZONE_DOMAIN%%.}\u0026quot;\rKUBE_LEGO_EMAIL=$(git config user.email)\rsed -i \\\r-e \u0026quot;s/region: westeurope/region: eastus/\u0026quot; \\\r-e \u0026quot;s/domain: johndoe-azure.garden-dev.example.com/domain: $SHOOT_DOMAIN/\u0026quot; \\\r-e \u0026quot;/domain:/a\\ \\ \\ \\ hostedZoneID: $HOSTED_ZONE_ID\u0026quot; \\\r-e \u0026quot;s/email: john.doe@example.com/email: $KUBE_LEGO_EMAIL/\u0026quot; \\\rexample/90-deprecated-shoot-azure.yaml\rAnd let's create the Shoot resource:\nkubectl apply -f example/90-deprecated-shoot-azure.yaml\rAfter creating the Shoot resource, gardener-controller-manager will pick it up and start provisioning the Shoot cluster.\n$ kubectl get -f example/90-deprecated-shoot-azure.yaml\rNAME CLOUDPROFILE VERSION SEED DOMAIN OPERATION PROGRESS APISERVER CONTROL NODES SYSTEM AGE\rjohndoe-azure azure 1.12.3 azure johndoe-azure.garden-dev.your.domain.here Processing 15 \u0026lt;unknown\u0026gt; \u0026lt;unknown\u0026gt; \u0026lt;unknown\u0026gt; \u0026lt;unknown\u0026gt; 16s\rFollow the logs in your console with gardener-controller-manager, starting like below you'll see plenty of Waiting and Executing, etc. logs and many tasks will keep repeating:\ntime=\u0026quot;2018-06-09T07:35:45Z\u0026quot; level=info msg=\u0026quot;[SHOOT RECONCILE] garden-dev/johndoe-azure\u0026quot;\rtime=\u0026quot;2018-06-09T07:35:46Z\u0026quot; level=info msg=\u0026quot;Starting flow Shoot cluster creation\u0026quot; opid=VIBBBGFx shoot=garden-dev/johndoe-azure\rtime=\u0026quot;2018-06-09T07:35:46Z\u0026quot; level=info msg=\u0026quot;Executing (*Botanist).DeployExternalDomainDNSRecord\u0026quot; opid=VIBBBGFx shoot=garden-dev/johndoe-azure\rtime=\u0026quot;2018-06-09T07:35:46Z\u0026quot; level=info msg=\u0026quot;Executing (*Botanist).DeployNamespace\u0026quot; opid=VIBBBGFx shoot=garden-dev/johndoe-azure\rtime=\u0026quot;2018-06-09T07:35:46Z\u0026quot; level=info msg=\u0026quot;Executing (*Botanist).DeployKubeAPIServerService\u0026quot; opid=VIBBBGFx shoot=garden-dev/johndoe-azure\rtime=\u0026quot;2018-06-09T07:35:46Z\u0026quot; level=info msg=\u0026quot;Executing (*Botanist).DeployBackupNamespaceFromShoot\u0026quot; opid=VIBBBGFx shoot=garden-dev/johndoe-azure\rtime=\u0026quot;2018-06-09T07:35:46Z\u0026quot; level=info msg=\u0026quot;Waiting for Terraform validation Pod 'johndoe-azure.external-dns.tf-pod-d8f66' to be completed...\u0026quot; opid=VIBBBGFx shoot=garden-dev/johndoe-azure\rtime=\u0026quot;2018-06-09T07:35:51Z\u0026quot; level=info msg=\u0026quot;Waiting for Terraform validation Pod 'johndoe-azure.external-dns.tf-pod-d8f66' to be completed...\u0026quot; opid=VIBBBGFx shoot=garden-dev/johndoe-azure\rtime=\u0026quot;2018-06-09T07:35:51Z\u0026quot; level=info msg=\u0026quot;Executing (*Botanist).MoveBackupTerraformResources\u0026quot; opid=VIBBBGFx shoot=garden-dev/johndoe-azure\rtime=\u0026quot;2018-06-09T07:35:52Z\u0026quot; level=info msg=\u0026quot;Executing (*Botanist).WaitUntilKubeAPIServerServiceIsReady\u0026quot; opid=VIBBBGFx shoot=garden-dev/johndoe-azure\rtime=\u0026quot;2018-06-09T07:35:52Z\u0026quot; level=info msg=\u0026quot;Waiting until the kube-apiserver service is ready...\u0026quot; opid=VIBBBGFx shoot=garden-dev/johndoe-azure\rtime=\u0026quot;2018-06-09T07:35:52Z\u0026quot; level=info msg=\u0026quot;Waiting until the backup-infrastructure has been reconciled in the Garden cluster...\u0026quot; opid=VIBBBGFx shoot=garden-dev/johndoe-azure\rtime=\u0026quot;2018-06-09T07:35:56Z\u0026quot; level=info msg=\u0026quot;Waiting for Terraform validation Pod 'johndoe-azure.external-dns.tf-pod-d8f66' to be completed...\u0026quot; opid=VIBBBGFx shoot=garden-dev/johndoe-azure\rtime=\u0026quot;2018-06-09T07:35:57Z\u0026quot; level=info msg=\u0026quot;Waiting until the kube-apiserver service is ready...\u0026quot; opid=VIBBBGFx shoot=garden-dev/johndoe-azure\rtime=\u0026quot;2018-06-09T07:35:57Z\u0026quot; level=info msg=\u0026quot;Waiting until the backup-infrastructure has been reconciled in the Garden cluster...\u0026quot; opid=VIBBBGFx shoot=garden-dev/johndoe-azure\rtime=\u0026quot;2018-06-09T07:36:01Z\u0026quot; level=info msg=\u0026quot;Waiting for Terraform validation Pod 'johndoe-azure.external-dns.tf-pod-d8f66' to be completed...\u0026quot; opid=VIBBBGFx shoot=garden-dev/johndoe-azure\rtime=\u0026quot;2018-06-09T07:36:02Z\u0026quot; level=info msg=\u0026quot;Waiting until the kube-apiserver service is ready...\u0026quot; opid=VIBBBGFx shoot=garden-dev/johndoe-azure\rtime=\u0026quot;2018-06-09T07:36:02Z\u0026quot; level=info msg=\u0026quot;Waiting until the backup-infrastructure has been reconciled in the Garden cluster...\u0026quot; opid=VIBBBGFx shoot=garden-dev/johndoe-azure\r...\rAt this stage you should be waiting for a while until the Shoot cluster is provisioned and initial resources are deployed.\nDuring the provisioning you can also check output of these commands to have a better understanding about what's going on in the seed cluster:\n$ gardenctl ls shoots\rprojects:\r- project: garden-dev\rshoots:\r- johndoe-azure\r$ gardenctl ls issues\rissues:\r- project: garden-dev\rseed: azure\rshoot: johndoe-azure\rhealth: Unknown\rstatus:\rlastOperation:\rdescription: Executing DeployKubeAddonManager, ReconcileMachines.\rlastUpdateTime: 2018-06-09 08:40:20 +0100 IST\rprogress: 74\rstate: Processing\rtype: Create\r$ kubectl -n garden-dev get shoot johndoe-azure\rNAMESPACE NAME SEED DOMAIN VERSION CONTROL NODES SYSTEM LATEST\rgarden-dev johndoe-azure azure johndoe-azure.garden-dev.your.domain.here 1.10.1 True True True Succeeded\r$ kubectl -n garden-dev describe shoot johndoe-azure\r...\rEvents:\rType Reason Age From Message\r---- ------ ---- ---- -------\rNormal Reconciling 1h gardener-controller-manager [BrXWiztO] Reconciling Shoot cluster state\rNormal Reconciling 59m gardener-controller-manager [rBFsfwU5] Reconciling Shoot cluster state\rNormal Reconciling 59m gardener-controller-manager [2HAbm45D] Reconciling Shoot cluster state\rWarning ReconcileError 48m gardener-controller-manager [2HAbm45D] Failed to reconcile Shoot cluster state: Errors occurred during flow execution: '(*Botanist).EnsureIngressDNSRecord' returned '`.status.loadBalancer.ingress[]` has no elements yet, i.e. external load balancer has not been created (is your quota limit exceeded/reached?)'\rNormal Reconciling 48m gardener-controller-manager [S1QA0ksz] Reconciling Shoot cluster state\rNormal Reconciling 47m gardener-controller-manager [lvcSKy1Q] Reconciling Shoot cluster state\rNormal Reconciling 47m gardener-controller-manager [MddMyk8W] Reconciling Shoot cluster state\rNormal Reconciling 47m gardener-controller-manager [XDAAWABd] Reconciling Shoot cluster state\rNormal Reconciling 46m gardener-controller-manager [6HYH9Psz] Reconciling Shoot cluster state\rNormal Reconciling 46m gardener-controller-manager [rhL38ym4] Reconciling Shoot cluster state\rWarning ReconcileError 35m gardener-controller-manager [rhL38ym4] Failed to reconcile Shoot cluster state: Errors occurred during flow execution: '(*Botanist).EnsureIngressDNSRecord' returned '`.status.loadBalancer.ingress[]` has no elements yet, i.e. external load balancer has not been created (is your quota limit exceeded/reached?)'\rNormal Reconciling 35m gardener-controller-manager [BOt4Nvso] Reconciling Shoot cluster state\rNormal Reconciling 35m gardener-controller-manager [JPtmXmxD] Reconciling Shoot cluster state\rNormal Reconciling 34m gardener-controller-manager [ldHsVA6G] Reconciling Shoot cluster state\rNormal Reconciled 31m gardener-controller-manager [ldHsVA6G] Reconciled Shoot cluster state\rNormal Reconciling 26m gardener-controller-manager [yBh2IBOF] Reconciling Shoot cluster state\rNormal Reconciled 24m gardener-controller-manager [yBh2IBOF] Reconciled Shoot cluster state\rNormal Reconciling 16m gardener-controller-manager [bqmFtHUA] Reconciling Shoot cluster state\rNormal Reconciled 14m gardener-controller-manager [bqmFtHUA] Reconciled Shoot cluster state\rNormal Reconciling 6m gardener-controller-manager [7QgHE5CH] Reconciling Shoot cluster state\rNormal Reconciled 3m gardener-controller-manager [7QgHE5CH] Reconciled Shoot cluster state\rCheck Shoot cluster:\n$ gardenctl target garden dev\rKUBECONFIG=/Users/user/.kube/config\r$ gardenctl target project garden-dev\r$ gardenctl target shoot johndoe-azure\rKUBECONFIG=/Users/user/.garden/cache/projects/garden-dev/johndoe-azure/kubeconfig.yaml\r$ gardenctl kubectl cluster-info\rKubernetes master is running at https://api.johndoe-azure.garden-dev.your.domain.here\rCoreDNS is running at https://api.johndoe-azure.garden-dev.your.domain.here/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\rkubernetes-dashboard is running at https://api.johndoe-azure.garden-dev.your.domain.here/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy\rTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\rCluster Resources After Shoot is Created After the Shoot has been created the summary of the resources in the AKS cluster handled by Gardener will be something like this:\nnon-namespaced resources\rCloudProfile: azure\rProject: dev\rNamespace: garden-dev\rSeed: azure # cloud.profile:azure, cloud.region:eastus, secretRef.name:seed-azure, secretRef.namespace: garden\rNamespace: garden\rSecret: seed-azure # aks credentials, kubeconfig\r# No other resources with any kind handled by Gardener\r# Gardener components as well lives in this namespace\rNamespace: garden-dev # maps to \u0026quot;project:dev\u0026quot; in Gardener\rSecret: core-azure # credentials for aks + aws (for route53)\rSecretBinding: core-azure # secretRef.name:core-azure\rShoot: johndoe-azure # seed:azure, secretBindingRef.name:core-azure\rNamespace: shoot--dev--johndoe-azure\r# These are automatically created once Shoot resource is created\rAzureMachineClass: shoot--dev--johndoe-azure-cpu-worker-8506a\rMachineDeployment: shoot--dev--johndoe-azure-cpu-worker\rMachineSet: shoot--dev--johndoe-azure-cpu-worker-849bbbf75\rMachine: shoot--dev--johndoe-azure-cpu-worker-849bbbf75-b42vh\rBackupInfra: shoot--dev--johndoe-azure--c1b3b # seed:azure, shootUID: shoot.status.UID.\r# Many other resources created as part of shoot cluster,\r# but only above ones are handled by Gardener\rNamespace: backup--shoot--dev--johndoe-azure--c1b3b\r# Secrets and configMap having info related to backup infrastructure\r# are created by Gardener.\rTroubleshooting Shoot Creation Issues For any issue happening during Shoot provisioning, you can consult the gardener-controller-manager logs, or the state in the shoot resource, gardenctl also provides a command to check Shoot cluster states:\n# check gardener-controller-manager logs\rkubectl -n garden logs -f deployment/gardener-controller-manager\r# kubectl describe can provide you a human readable output of\r# same information in below gardenctl command.\rkubectl -n garden-dev describe shoot johndoe-azure\r# also try cheking the machine-controller-manager logs of the shoot\rkubectl logs -n shoot--dev--johndoe-azure deployment/machine-controller-manager\rWith gardenctl:\n$ gardenctl ls issues\rissues:\r- project: garden-dev\rseed: azure\rshoot:\rhealth: Ready\rstatus: johndoe-azure\rlastError: \u0026quot;Failed to reconcile Shoot cluster state: Errors occurred during flow\rexecution: '(*Botanist).DeployExternalDomainDNSRecord' returned 'Terraform execution\r...\rlastOperation:\rdescription: \u0026quot;Failed to reconcile Shoot cluster state: Errors occurred during\rflow execution: '(*Botanist).DeployExternalDomainDNSRecord' returned 'Terraform\r...\rlastUpdateTime: 2018-06-03 09:48:00 +0100 IST\rprogress: 100\rstate: Failed\rtype: Reconcile\rAccess Shoot cluster The gardenctl tool provides a convenient wrapper to operate on both cluster and cloud providers, here are some commands you can run\n# select target shoot cluster\rgardenctl ls gardens\rgardenctl target garden dev\rgardenctl ls projects\rgardenctl target shoot johndoe-azure\r# issue Azure client (az) commands on target shoot\rgardenctl az aks list\r# issue kubectl commands on target shoot\rgardenctl kubectl -- version --short # '--' is required if you want to\r# pass any args starting with '-'\r# open prometheus, alertmanager, grafana without having to find\r# the user/pass for each\rgardenctl show prometheus\rgardenctl show grafana\rgardenctl show alertmanager\rEasiest way to obtain kubeconfig of the shoot cluster:\n$ gardenctl target shoot johndoe-azure\rKUBECONFIG=/Users/user/.garden/cache/projects/garden-dev/johndoe-azure/kubeconfig.yaml\r$ export KUBECONFIG=/Users/user/.garden/cache/projects/garden-dev/johndoe-azure/kubeconfig.yaml\r$ # From now on your local kubectl will be operating on target shoot\r$ kubectl cluster-info # will show your shoot cluster info\r$ unset KUBECONFIG # reset to your default kubectl\rThe shoot cluster's kubeconfig is being kept in a secret in the project namespace:\nkubectl -n shoot--dev--johndoe-azure get secret kubecfg -o jsonpath='{.data.kubeconfig}' | base64 -D \u0026gt; /tmp/johndoe-azure-kubeconfig.yaml\rexport KUBECONFIG=/tmp/johndoe-azure-kubeconfig.yaml\rDelete Shoot cluster Deleting a Shoot cluster is not straight forward, and this is to protect users from undesired/accidental cluster deletion. One has to place some special annotations to get a Shoot cluster removed. We use the hack/usage/delete script for this purpose.\nPlease refer to Creating / Deleting a Shoot cluster document for more details.\nhack/delete shoot johndoe-azure garden-dev\r"
},
{
	"uri": "https://gardener.cloud/components/dns-cm/",
	"title": "DNS Controller Manager",
	"tags": [],
	"description": "",
	"content": "External DNS Management The main artefact of this project is the DNS controller manager for managing DNS records, also nicknamed as the Gardener \u0026ldquo;DNS Controller\u0026rdquo;.\nIt contains provisioning controllers for creating DNS records in one of the DNS cloud services\n Amazon Route53, Google CloudDNS, AliCloud DNS, Azure DNS, OpenStack Designate, Cloudflare DNS  and source controllers for services and ingresses to create DNS entries by annotations.\nThe configuration for the external DNS service is specified in a custom resource DNSProvider. Multiple DNSProvider can be used simultaneously and changed without restarting the DNS controller.\nDNS records are either created directly for a corresponding custom resource DNSEntry or by annotating a service or ingress.\nFor a detailed explanation of the model, see section The Model.\nFor extending or adapting this project with your own source or provisioning controllers, see section Extensions\nQuick start To install the DNS controller manager in your Kubernetes cluster, follow these steps.\n  Prerequisites\n  Check out or download the project to get a copy of the Helm charts. It is recommended to check out the tag of the last release, so that Helm values reference the newest released container image for the deployment.\n  Make sure, that you have installed Helm client (helm) locally and Helm server (tiller) on the Kubernetes cluster. See e.g. Helm installation for more details.\n    Install the DNS controller manager\nAs multiple Gardener DNS controllers can act on the same DNS Hosted Zone concurrently, each instance needs an owner identifier. Therefore choose an identifier sufficiently unique across these instances.\nThen install the DNS controller manager with\nhelm install charts/external-dns-management --name dns-controller --namespace=\u0026lt;my-namespace\u0026gt; --set configuration.identifier=\u0026lt;my-identifier\u0026gt;\rThis will use the default configuration with all source and provisioning controllers enabled. The complete set of configuration variables can be found in charts/external-dns-management/values.yaml. Their meaning is explained by their corresponding command line options in section Using the DNS controller manager\nBy default, the DNS controller looks for custom resources in all namespaces. The choosen namespace is only relevant for the deployment itself.\n  Create a DNSProvider\nTo specify a DNS provider, you need to create a custom resource DNSProvider and a secret containing the credentials for your account at the provider. E.g. if you want to use AWS Route53, create a secret and provider with\ncat \u0026lt;\u0026lt; EOF | kubectl apply -f -\rapiVersion: v1\rkind: Secret\rmetadata:\rname: aws-credentials\rnamespace: default\rtype: Opaque\rdata:\r# replace \u0026#39;...\u0026#39; with values encoded as base64\r# see https://docs.aws.amazon.com/general/latest/gr/managing-aws-access-keys.html\rAWS_ACCESS_KEY_ID: ...\rAWS_SECRET_ACCESS_KEY: ...\rEOF\rand\ncat \u0026lt;\u0026lt; EOF | kubectl apply -f -\rapiVersion: dns.gardener.cloud/v1alpha1\rkind: DNSProvider\rmetadata:\rname: aws\rnamespace: default\rspec:\rtype: aws-route53\rsecretRef:\rname: aws-credentials\rdomains:\rinclude:\r# this must be replaced with a (sub)domain of the hosted zone\r- my.own.domain.com\rEOF\rCheck the successful creation with\nkubectl get dnspr\rYou should see something like\nNAME TYPE STATUS AGE\raws aws-route53 Ready 12s\r  Create a DNSEntry\nCreate an DNS entry with\ncat \u0026lt;\u0026lt; EOF | kubectl apply -f -\rapiVersion: dns.gardener.cloud/v1alpha1\rkind: DNSEntry\rmetadata:\rname: mydnsentry\rnamespace: default\rspec:\rdnsName: \u0026#34;myentry.my-own-domain.com\u0026#34;\rttl: 600\rtargets:\r- 1.2.3.4\rEOF\rCheck the status of the DNS entry with\nkubectl get dnsentry\rYou should see something like\nNAME DNS TYPE PROVIDER STATUS AGE\rmydnsentry myentry.my-own-domain.com aws-route53 default/aws Ready 24s\rAs soon as the status of the entry is Ready, the provider has accepted the new DNS record. Depending on the provider and your DNS settings and cache, it may take up to a few minutes before the domain name can be resolved.\n  Wait for/check DNS record\nTo check the DNS resolution, use nslookup or dig.\nnslookup myentry.my-own-domain.com\ror with dig\n# or with dig\rdig +short myentry.my-own-domain.com\rDepending on your network settings, you may get a successful response faster using a public DNS server (e.g. 8.8.8.8, 8.8.4.4, or 1.1.1.1)\ndig @8.8.8.8 +short myentry.my-own-domain.com\r  For more examples about the custom resources and the annotations for services and ingresses see the examples directory.\nThe Model This project provides a flexible model allowing to add DNS source objects and DNS provisioning environments by adding new independent controllers.\nThere is no single DNS controller anymore. The decoupling between the handling of DNS source objects, like ingresses or services, and the provisioning of DNS entries in an external DNS provider like Route53 or CloudDNS is achieved by introducing a new custom resource DNSEntry.\nThese objects can either be explicitly created to request dedicated DNS entries, or they are managed based on other resources like ingresses or services. For the latter dedicated DNS Source Controllers are used. There might be any number of such source controllers. They do not need to know anything about the various DNS environments. Their task is to figure out which DNS entries are required in their realm and manage appropriate DNSEntry objects. From these objects they can also read the provisioning status and report it back to the original source.\nProvisioning of DNS entries in external DNS providers is done by DNS Provisioning Controllers. They don't need to know anything about the various DNS source objects. They watch DNSEntry objects and check whether they are responsible for such an object. If a provisioning controller feels responsible for an entry it manages the corresponding settings in the external DNS environment and reports the provisioning status back to the corresponding DNSEntry object.\nTo do this a provisioning controller is responsible for a dedicated environment (for example Route53). For every such environment the controller uses a dedicated type key. This key is used to look for DNSProvider objects. There might be multiple such objects per environment, specifying the credentials needed to access different external accounts. These accounts are then scanned for DNS zones and domain names they support. This information is then used to dynamically assign DNSEntry objects to dedicated DNSProvider objects. If such an assignment can be done by a provisioning controller then it is responsible for this entry and manages the corresponding entries in the external environment. DNSProvider objects can specify explicit inclusion and exclusion sets of domain names and/or DNS zone identifiers to override the scanning results of the account.\nOwner Identifiers Every DNS Provisioning Controller is responsible for a set of Owner Identifiers. DNS records in an external DNS environment are attached to such an identifier. This is used to identify the records in the DNS environment managed by a dedicated controller (manager). Every controller manager hosting DNS Provisioning Controllers offers an option to specify a default identifier. Additionally there might be dedicated DNSOwner objects that enable or disable additional owner ids.\nEvery DNSEntry object may specify a dedicated owner that is used to tag the records in the DNS environment. A DNS provisioning controller only acts of DNS entries it is responsible for. Other resources in the external DNS environment are not touched at all.\nThis way it is possbible to\n identify records in the external DNS management environment that are managed by the actual controller instance distinguish different DNS source environments sharing the same hosted zones in the external management environment cleanup unused entries, even if the whole resource set is already gone move the responsibility for dedicated sets of DNS entries among different kubernetes clusters or DNS source environments running different DNS Provisioning Controller without loosing the entries during the migration process.  If multiple DNS controller instances have access to the same DNS zones, it is very important, that every instance uses a unique owner identifier! Otherwise the cleanup of stale DNS record will delete entries created by another instance if they use the same identifier.\nDNS Classes Multiple sets of controllers of the DNS ecosystem can run in parallel in a kubernetes cluster working on different object set. They are separated by using different DNS Classes. Adding a DNS class annotation to an object of the DNS ecosytems assigns this object to such a dedicated set of DNS controllers. This way it is possible to maintain clearly separated set of DNS objects in a single kubernetes cluster.\nUsing the DNS controller manager The controllers to run can be selected with the --controllers option. Here the following controller groups can be used:\n  dnssources: all DNS Source Controllers. It includes the conrollers\n ingress-dns: handle DNS annotations for the standard kubernetes ingress resource service-dns: handle DNS annotations for the standard kubernetes service resource    dnscontrollers: all DNS Provisioning Controllers. It includes the controllers\n alicloud-dns: aws-route53: azure-dns: google-clouddns: openstack-designate: cloudflare-dns    all: (default) all controllers\n  It is also possible to list dedicated controllers by their name.\nIf a DNS Provisioning Controller is enabled it is important to specify a unique controller identity using the --identifier option. This identifier is stored in the DNS system to identify the DNS entries managed by a dedicated controller. There should never be two DNS controllers with the same identifier running at the same time for the same DNS domains/accounts.\nHere is the complete list of options provided:\nUsage:\rdns-controller-manager [flags]\rFlags:\r--alicloud-dns.cache-dir string Directory to store zone caches (for reload after restart)\r--alicloud-dns.cache-ttl int Time-to-live for provider hosted zone cache\r--alicloud-dns.default.pool.size int Worker pool size for pool default of controller alicloud-dns (default: 2)\r--alicloud-dns.disable-zone-state-caching disable use of cached dns zone state on changes\r--alicloud-dns.dns-class string Identifier used to differentiate responsible controllers for entries\r--alicloud-dns.dns-delay duration delay between two dns reconciliations\r--alicloud-dns.dns.pool.resync-period duration Period for resynchronization of pool dns of controller alicloud-dns (default: 15m0s)\r--alicloud-dns.dns.pool.size int Worker pool size for pool dns of controller alicloud-dns (default: 1)\r--alicloud-dns.dry-run just check, don\u0026#39;t modify\r--alicloud-dns.identifier string Identifier used to mark DNS entries\r--alicloud-dns.ownerids.pool.size int Worker pool size for pool ownerids of controller alicloud-dns (default: 1)\r--alicloud-dns.providers.pool.resync-period duration Period for resynchronization of pool providers of controller alicloud-dns (default: 10m0s)\r--alicloud-dns.providers.pool.size int Worker pool size for pool providers of controller alicloud-dns (default: 2)\r--alicloud-dns.reschedule-delay duration reschedule delay after losing provider\r--alicloud-dns.secrets.pool.size int Worker pool size for pool secrets of controller alicloud-dns (default: 2)\r--alicloud-dns.setup int number of processors for controller setup\r--alicloud-dns.ttl int Default time-to-live for DNS entries\r--aws-route53.cache-dir string Directory to store zone caches (for reload after restart)\r--aws-route53.cache-ttl int Time-to-live for provider hosted zone cache\r--aws-route53.default.pool.size int Worker pool size for pool default of controller aws-route53 (default: 2)\r--aws-route53.disable-zone-state-caching disable use of cached dns zone state on changes\r--aws-route53.dns-class string Identifier used to differentiate responsible controllers for entries\r--aws-route53.dns-delay duration delay between two dns reconciliations\r--aws-route53.dns.pool.resync-period duration Period for resynchronization of pool dns of controller aws-route53 (default: 15m0s)\r--aws-route53.dns.pool.size int Worker pool size for pool dns of controller aws-route53 (default: 1)\r--aws-route53.dry-run just check, don\u0026#39;t modify\r--aws-route53.identifier string Identifier used to mark DNS entries\r--aws-route53.ownerids.pool.size int Worker pool size for pool ownerids of controller aws-route53 (default: 1)\r--aws-route53.providers.pool.resync-period duration Period for resynchronization of pool providers of controller aws-route53 (default: 10m0s)\r--aws-route53.providers.pool.size int Worker pool size for pool providers of controller aws-route53 (default: 2)\r--aws-route53.reschedule-delay duration reschedule delay after losing provider\r--aws-route53.secrets.pool.size int Worker pool size for pool secrets of controller aws-route53 (default: 2)\r--aws-route53.setup int number of processors for controller setup\r--aws-route53.ttl int Default time-to-live for DNS entries\r--azure-dns.cache-dir string Directory to store zone caches (for reload after restart)\r--azure-dns.cache-ttl int Time-to-live for provider hosted zone cache\r--azure-dns.default.pool.size int Worker pool size for pool default of controller azure-dns (default: 2)\r--azure-dns.disable-zone-state-caching disable use of cached dns zone state on changes\r--azure-dns.dns-class string Identifier used to differentiate responsible controllers for entries\r--azure-dns.dns-delay duration delay between two dns reconciliations\r--azure-dns.dns.pool.resync-period duration Period for resynchronization of pool dns of controller azure-dns (default: 15m0s)\r--azure-dns.dns.pool.size int Worker pool size for pool dns of controller azure-dns (default: 1)\r--azure-dns.dry-run just check, don\u0026#39;t modify\r--azure-dns.identifier string Identifier used to mark DNS entries\r--azure-dns.ownerids.pool.size int Worker pool size for pool ownerids of controller azure-dns (default: 1)\r--azure-dns.providers.pool.resync-period duration Period for resynchronization of pool providers of controller azure-dns (default: 10m0s)\r--azure-dns.providers.pool.size int Worker pool size for pool providers of controller azure-dns (default: 2)\r--azure-dns.reschedule-delay duration reschedule delay after losing provider\r--azure-dns.secrets.pool.size int Worker pool size for pool secrets of controller azure-dns (default: 2)\r--azure-dns.setup int number of processors for controller setup\r--azure-dns.ttl int Default time-to-live for DNS entries\r--cloudflare-dns.cache-dir string Directory to store zone caches (for reload after restart)\r--cloudflare-dns.cache-ttl int Time-to-live for provider hosted zone cache\r--cloudflare-dns.default.pool.size int Worker pool size for pool default of controller cloudflare-dns (default: 2)\r--cloudflare-dns.disable-zone-state-caching disable use of cached dns zone state on changes\r--cloudflare-dns.dns-class string Identifier used to differentiate responsible controllers for entries\r--cloudflare-dns.dns-delay duration delay between two dns reconciliations\r--cloudflare-dns.dns.pool.resync-period duration Period for resynchronization of pool dns of controller cloudflare-dns (default: 15m0s)\r--cloudflare-dns.dns.pool.size int Worker pool size for pool dns of controller cloudflare-dns (default: 1)\r--cloudflare-dns.dry-run just check, don\u0026#39;t modify\r--cloudflare-dns.identifier string Identifier used to mark DNS entries\r--cloudflare-dns.ownerids.pool.size int Worker pool size for pool ownerids of controller cloudflare-dns (default: 1)\r--cloudflare-dns.providers.pool.resync-period duration Period for resynchronization of pool providers of controller cloudflare-dns (default: 10m0s)\r--cloudflare-dns.providers.pool.size int Worker pool size for pool providers of controller cloudflare-dns (default: 2)\r--cloudflare-dns.reschedule-delay duration reschedule delay after losing provider\r--cloudflare-dns.secrets.pool.size int Worker pool size for pool secrets of controller cloudflare-dns (default: 2)\r--cloudflare-dns.setup int number of processors for controller setup\r--cloudflare-dns.ttl int Default time-to-live for DNS entries\r--cache-dir string default for all controller \u0026#34;cache-dir\u0026#34; options\r--cache-ttl int default for all controller \u0026#34;cache-ttl\u0026#34; options\r-c, --controllers string comma separated list of controllers to start (\u0026lt;name\u0026gt;,source,target,all) (default \u0026#34;all\u0026#34;)\r--cpuprofile string set file for cpu profiling\r--disable-namespace-restriction disable access restriction for namespace local access only\r--disable-zone-state-caching default for all controller \u0026#34;disable-zone-state-caching\u0026#34; options\r--dns-class string default for all controller \u0026#34;dns-class\u0026#34; options\r--dns-delay duration default for all controller \u0026#34;dns-delay\u0026#34; options\r--dns-target-class string default for all controller \u0026#34;dns-target-class\u0026#34; options\r--dnsentry-source.default.pool.resync-period duration Period for resynchronization of pool default of controller dnsentry-source (default: 2m0s)\r--dnsentry-source.default.pool.size int Worker pool size for pool default of controller dnsentry-source (default: 2)\r--dnsentry-source.dns-class string identifier used to differentiate responsible controllers for entries\r--dnsentry-source.dns-target-class string identifier used to differentiate responsible dns controllers for target entries\r--dnsentry-source.exclude-domains stringArray excluded domains\r--dnsentry-source.key string selecting key for annotation\r--dnsentry-source.target-creator-label-name string label name to store the creator for generated DNS entries\r--dnsentry-source.target-creator-label-value string label value for creator label\r--dnsentry-source.target-name-prefix string name prefix in target namespace for cross cluster generation\r--dnsentry-source.target-namespace string target namespace for cross cluster generation\r--dnsentry-source.target-owner-id string owner id to use for generated DNS entries\r--dnsentry-source.target-realms string realm(s) to use for generated DNS entries\r--dnsentry-source.target-set-ignore-owners mark generated DNS entries to omit owner based access control\r--dnsentry-source.targets.pool.size int Worker pool size for pool targets of controller dnsentry-source (default: 2)\r--dry-run default for all controller \u0026#34;dry-run\u0026#34; options\r--exclude-domains stringArray default for all controller \u0026#34;exclude-domains\u0026#34; options\r--google-clouddns.cache-dir string Directory to store zone caches (for reload after restart)\r--google-clouddns.cache-ttl int Time-to-live for provider hosted zone cache\r--google-clouddns.default.pool.size int Worker pool size for pool default of controller google-clouddns (default: 2)\r--google-clouddns.disable-zone-state-caching disable use of cached dns zone state on changes\r--google-clouddns.dns-class string Identifier used to differentiate responsible controllers for entries\r--google-clouddns.dns-delay duration delay between two dns reconciliations\r--google-clouddns.dns.pool.resync-period duration Period for resynchronization of pool dns of controller google-clouddns (default: 15m0s)\r--google-clouddns.dns.pool.size int Worker pool size for pool dns of controller google-clouddns (default: 1)\r--google-clouddns.dry-run just check, don\u0026#39;t modify\r--google-clouddns.identifier string Identifier used to mark DNS entries\r--google-clouddns.ownerids.pool.size int Worker pool size for pool ownerids of controller google-clouddns (default: 1)\r--google-clouddns.providers.pool.resync-period duration Period for resynchronization of pool providers of controller google-clouddns (default: 10m0s)\r--google-clouddns.providers.pool.size int Worker pool size for pool providers of controller google-clouddns (default: 2)\r--google-clouddns.reschedule-delay duration reschedule delay after losing provider\r--google-clouddns.secrets.pool.size int Worker pool size for pool secrets of controller google-clouddns (default: 2)\r--google-clouddns.setup int number of processors for controller setup\r--google-clouddns.ttl int Default time-to-live for DNS entries\r--grace-period duration inactivity grace period for detecting end of cleanup for shutdown\r-h, --help help for dns-controller-manager\r--identifier string default for all controller \u0026#34;identifier\u0026#34; options\r--ingress-dns.default.pool.resync-period duration Period for resynchronization of pool default of controller ingress-dns (default: 2m0s)\r--ingress-dns.default.pool.size int Worker pool size for pool default of controller ingress-dns (default: 2)\r--ingress-dns.dns-class string identifier used to differentiate responsible controllers for entries\r--ingress-dns.dns-target-class string identifier used to differentiate responsible dns controllers for target entries\r--ingress-dns.exclude-domains stringArray excluded domains\r--ingress-dns.key string selecting key for annotation\r--ingress-dns.target-creator-label-name string label name to store the creator for generated DNS entries\r--ingress-dns.target-creator-label-value string label value for creator label\r--ingress-dns.target-name-prefix string name prefix in target namespace for cross cluster generation\r--ingress-dns.target-namespace string target namespace for cross cluster generation\r--ingress-dns.target-owner-id string owner id to use for generated DNS entries\r--ingress-dns.target-realms string realm(s) to use for generated DNS entries\r--ingress-dns.target-set-ignore-owners mark generated DNS entries to omit owner based access control\r--ingress-dns.targets.pool.size int Worker pool size for pool targets of controller ingress-dns (default: 2)\r--key string default for all controller \u0026#34;key\u0026#34; options\r--kubeconfig string default cluster access\r--kubeconfig.disable-deploy-crds disable deployment of required crds for cluster default\r--kubeconfig.id string id for cluster default\r-D, --log-level string logrus log level\r--name string name used for controller manager\r--namespace string namespace for lease\r-n, --namespace-local-access-only enable access restriction for namespace local access only (deprecated)\r--omit-lease omit lease for development\r--openstack-designate.cache-dir string Directory to store zone caches (for reload after restart)\r--openstack-designate.cache-ttl int Time-to-live for provider hosted zone cache\r--openstack-designate.default.pool.size int Worker pool size for pool default of controller openstack-designate (default: 2)\r--openstack-designate.disable-zone-state-caching disable use of cached dns zone state on changes\r--openstack-designate.dns-class string Identifier used to differentiate responsible controllers for entries\r--openstack-designate.dns-delay duration delay between two dns reconciliations\r--openstack-designate.dns.pool.resync-period duration Period for resynchronization of pool dns of controller openstack-designate (default: 15m0s)\r--openstack-designate.dns.pool.size int Worker pool size for pool dns of controller openstack-designate (default: 1)\r--openstack-designate.dry-run just check, don\u0026#39;t modify\r--openstack-designate.identifier string Identifier used to mark DNS entries\r--openstack-designate.ownerids.pool.size int Worker pool size for pool ownerids of controller openstack-designate (default: 1)\r--openstack-designate.providers.pool.resync-period duration Period for resynchronization of pool providers of controller openstack-designate (default: 10m0s)\r--openstack-designate.providers.pool.size int Worker pool size for pool providers of controller openstack-designate (default: 2)\r--openstack-designate.reschedule-delay duration reschedule delay after losing provider\r--openstack-designate.secrets.pool.size int Worker pool size for pool secrets of controller openstack-designate (default: 2)\r--openstack-designate.setup int number of processors for controller setup\r--openstack-designate.ttl int Default time-to-live for DNS entries\r--plugin-dir string directory containing go plugins\r--pool.resync-period duration default for all controller \u0026#34;pool.resync-period\u0026#34; options\r--pool.size int default for all controller \u0026#34;pool.size\u0026#34; options\r--providers string cluster to look for provider objects\r--providers.disable-deploy-crds disable deployment of required crds for cluster provider\r--providers.id string id for cluster provider\r--reschedule-delay duration default for all controller \u0026#34;reschedule-delay\u0026#34; options\r--server-port-http int HTTP server port (serving /healthz, /metrics, ...)\r--service-dns.default.pool.resync-period duration Period for resynchronization of pool default of controller service-dns (default: 2m0s)\r--service-dns.default.pool.size int Worker pool size for pool default of controller service-dns (default: 2)\r--service-dns.dns-class string identifier used to differentiate responsible controllers for entries\r--service-dns.dns-target-class string identifier used to differentiate responsible dns controllers for target entries\r--service-dns.exclude-domains stringArray excluded domains\r--service-dns.key string selecting key for annotation\r--service-dns.target-creator-label-name string label name to store the creator for generated DNS entries\r--service-dns.target-creator-label-value string label value for creator label\r--service-dns.target-name-prefix string name prefix in target namespace for cross cluster generation\r--service-dns.target-namespace string target namespace for cross cluster generation\r--service-dns.target-owner-id string owner id to use for generated DNS entries\r--service-dns.target-realms string realm(s) to use for generated DNS entries\r--service-dns.target-set-ignore-owners mark generated DNS entries to omit owner based access control\r--service-dns.targets.pool.size int Worker pool size for pool targets of controller service-dns (default: 2)\r--setup int default for all controller \u0026#34;setup\u0026#34; options\r--target string target cluster for dns requests\r--target-creator-label-name string default for all controller \u0026#34;target-creator-label-name\u0026#34; options\r--target-creator-label-value string default for all controller \u0026#34;target-creator-label-value\u0026#34; options\r--target-name-prefix string default for all controller \u0026#34;target-name-prefix\u0026#34; options\r--target-namespace string default for all controller \u0026#34;target-namespace\u0026#34; options\r--target-owner-id string default for all controller \u0026#34;target-owner-id\u0026#34; options\r--target-realms string default for all controller \u0026#34;target-realms\u0026#34; options\r--target-set-ignore-owners default for all controller \u0026#34;target-set-ignore-owners\u0026#34; options\r--target.disable-deploy-crds disable deployment of required crds for cluster target\r--target.id string id for cluster target\r--ttl int default for all controller \u0026#34;ttl\u0026#34; options\rExtensions This project can also be used as library to implement own source and provisioning controllers.\nHow to implement Source Controllers Based on the provided source controller library a source controller must implement the source.DNSSource interface and provide an appropriate creator function.\nA source controller can be implemented following this example:\npackage service\rimport (\r\u0026#34;github.com/gardener/controller-manager-library/pkg/resources\u0026#34;\r\u0026#34;github.com/gardener/external-dns-management/pkg/dns/source\u0026#34;\r)\rvar _MAIN_RESOURCE = resources.NewGroupKind(\u0026#34;core\u0026#34;, \u0026#34;Service\u0026#34;)\rfunc init() {\rsource.DNSSourceController(source.NewDNSSouceTypeForExtractor(\u0026#34;service-dns\u0026#34;, _MAIN_RESOURCE, GetTargets),nil).\rFinalizerDomain(\u0026#34;dns.gardener.cloud\u0026#34;).\rMustRegister(source.CONTROLLER_GROUP_DNS_SOURCES)\r}\rComplete examples can be found in the sub packages of pkg/controller/source.\nHow to implement Provisioning Controllers Provisioning controllers can be implemented based on the provisioning controller library in this repository and must implement the provider.DNSHandlerFactory interface. This factory returns implementations of the provider.DNSHandler interface that does the effective work for a dedicated set of hosted zones.\nThese factories can be embedded into a final controller manager (the runnable instance) in several ways:\n The factory can be used to create a dedicated controller. This controller can then be embedded into a controller manager, either in its own controller manger or together with other controllers. The factory can be added to a compound factory, able to handle multiple infrastructures. This one can then be used to create a dedicated controller, again.  Embedding a Factory into a Controller A provisioning controller can be implemented following this example:\npackage controller\rimport (\r\u0026#34;github.com/gardener/external-dns-management/pkg/dns/provider\u0026#34;\r)\rconst CONTROLLER_NAME = \u0026#34;route53-dns-controller\u0026#34;\rfunc init() {\rprovider.DNSController(CONTROLLER_NAME, \u0026amp;Factory{}).\rFinalizerDomain(\u0026#34;dns.gardener.cloud\u0026#34;).\rMustRegister(provider.CONTROLLER_GROUP_DNS_CONTROLLERS)\r}\rThis controller can be embedded into a controller manager just by using an anonymous import of the controller package in the main package of a dedicated controller manager.\nComplete examples are available in the sub packages of pkg/controller/provider. They also show a typical set of implementation structures that help to structure the implementation of such controllers.\nThe provider implemented in this project always follow the same structure:\n the provider package contains the provider code the factory source file registers the factory at a default compound factory it contains a sub package controller, which contains the embedding of the factory into a dedicated controller  Embedding a Factory into a Compound Factory A provisioning controller based on a Compound Factory can be extended by a new provider factory by registering this factory at the compound factory. This could be done, for example, by using the default compound factory provided in package pkg/controller/provider/compound as shown here, where NewHandler is a function creating a dedicated handler for a dedicated provider type:\npackage aws\rimport (\r\u0026#34;github.com/gardener/external-dns-management/pkg/controller/provider/compound\u0026#34;\r\u0026#34;github.com/gardener/external-dns-management/pkg/dns/provider\u0026#34;\r)\rconst TYPE_CODE = \u0026#34;aws-route53\u0026#34;\rvar Factory = provider.NewDNSHandlerFactory(TYPE_CODE, NewHandler)\rfunc init() {\rcompound.MustRegister(Factory)\r}\rThe compound factory is then again embedded into a provisioning controller as shown in the previous section (see the controllersub package).\nSetting Up a Controller Manager One or multiple controller packages can be bundled into a controller manager, by implementing a main package like this:\npackage main\rimport (\r\u0026#34;github.com/gardener/controller-manager-library/pkg/controllermanager\u0026#34;\r_ \u0026#34;github.com/\u0026lt;your controller package\u0026gt;\u0026#34;\r...\r)\rfunc main() {\rcontrollermanager.Start(\u0026#34;my-dns-controller-manager\u0026#34;, \u0026#34;dns controller manager\u0026#34;, \u0026#34;some description\u0026#34;)\r}\rUsing the standard Compound Provisioning Controller If the standard Compound Provisioning Controller should be used it is required to additionally add the anonymous imports for the providers intended to be embedded into the compound factory like this:\n\rExample Coding\rpackage main\rimport (\r\u0026#34;fmt\u0026#34;\r\u0026#34;os\u0026#34;\r\u0026#34;github.com/gardener/controller-manager-library/pkg/controllermanager\u0026#34;\r_ \u0026#34;github.com/gardener/external-dns-management/pkg/controller/provider/compound/controller\u0026#34;\r_ \u0026#34;github.com/gardener/external-dns-management/pkg/controller/provider/\u0026lt;your provider\u0026gt;\u0026#34;\r...\r)\rfunc main() {\rcontrollermanager.Start(\u0026#34;dns-controller-manager\u0026#34;, \u0026#34;dns controller manager\u0026#34;, \u0026#34;nothing\u0026#34;)\r}\r\rMultiple Cluster Support The controller implementations provided in this project are prepared to work with multiple clusters by using the features of the used controller manager library.\nThe DNS Source Controllers support two clusters:\n the default cluster is used to scan for source objects the logical cluster target is used to maintain the DNSEnry objects.  The DNS Provisioning Controllers also support two clusters:\n the default cluster is used to scan for DNSEntry objects. It is mapped to the logical cluster target the logical cluster provider is used to look to the DNSProvider objects and their related secrets.  If those controller types should be combined in a single controller manager, it can be configured to support three potential clusters with the source objects, the one for the entry objects and the one with provider objects using cluster mappings.\nThis is shown in a complete example using the dns source controllers, the compound provisioning controller configured to support all the included DNS provider type factories:\n\rExample Coding\rpackage main\rimport (\r\u0026#34;fmt\u0026#34;\r\u0026#34;os\u0026#34;\r\u0026#34;github.com/gardener/controller-manager-library/pkg/controllermanager\u0026#34;\r\u0026#34;github.com/gardener/controller-manager-library/pkg/controllermanager/cluster\u0026#34;\r\u0026#34;github.com/gardener/controller-manager-library/pkg/controllermanager/controller\u0026#34;\r\u0026#34;github.com/gardener/controller-manager-library/pkg/controllermanager/controller/mappings\u0026#34;\rdnsprovider \u0026#34;github.com/gardener/external-dns-management/pkg/dns/provider\u0026#34;\rdnssource \u0026#34;github.com/gardener/external-dns-management/pkg/dns/source\u0026#34;\r_ \u0026#34;github.com/gardener/external-dns-management/pkg/controller/provider/compound/controller\u0026#34;\r_ \u0026#34;github.com/gardener/external-dns-management/pkg/controller/provider/alicloud\u0026#34;\r_ \u0026#34;github.com/gardener/external-dns-management/pkg/controller/provider/aws\u0026#34;\r_ \u0026#34;github.com/gardener/external-dns-management/pkg/controller/provider/azure\u0026#34;\r_ \u0026#34;github.com/gardener/external-dns-management/pkg/controller/provider/google\u0026#34;\r_ \u0026#34;github.com/gardener/external-dns-management/pkg/controller/provider/openstack\u0026#34;\r_ \u0026#34;github.com/gardener/external-dns-management/pkg/controller/provider/cloudflare\u0026#34;\r_ \u0026#34;github.com/gardener/external-dns-management/pkg/controller/source/ingress\u0026#34;\r_ \u0026#34;github.com/gardener/external-dns-management/pkg/controller/source/service\u0026#34;\r)\rfunc init() {\r// target cluster already defined in dns source controller package\r cluster.Configure(\rdnsprovider.PROVIDER_CLUSTER,\r\u0026#34;providers\u0026#34;,\r\u0026#34;cluster to look for provider objects\u0026#34;,\r).Fallback(dnssource.TARGET_CLUSTER)\rmappings.ForControllerGroup(dnsprovider.CONTROLLER_GROUP_DNS_CONTROLLERS).\rMap(controller.CLUSTER_MAIN, dnssource.TARGET_CLUSTER).MustRegister()\r}\rfunc main() {\rcontrollermanager.Start(\u0026#34;dns-controller-manager\u0026#34;, \u0026#34;dns controller manager\u0026#34;, \u0026#34;nothing\u0026#34;)\r}\r\rThose clusters can the be separated by registering their names together with command line option names. These can be used to specify different kubeconfig files for those clusters.\nBy default all logical clusters are mapped to the default physical cluster specified via --kubeconfig or default cluster access.\nIf multiple physical clusters are defined they can be specified by a corresponding cluster option defining the kubeconfig file used to access this cluster. If no such option is specified the default is used.\nTherefore, even if the configuration is prepared for multiple clusters, such a controller manager can easily work on a single cluster if no special options are given on the command line.\nWhy not using the community external-dns solution? Some of the reasons are context-specific, i.e. relate to Gardener's highly dynamic requirements.\n Custom resource for DNS entries  DNS entries are explicitly specified as custom resources. As an important side effect, each DNS entry provides an own status. Simply by querying the Kubernetes API, a client can check if a requested DNS entry has been successfully added to the DNS backend, or if an update has already been deployed, or if not to reason about the cause. It also opens for easy extensibility, as DNS entries can be created directly via the Kubernetes API. And it simplifies Day 2 operations, e.g. automatic cleanup of unused entries if a DNS provider is deleted.\nManagement of multiple DNS providers  The Gardener DNS controller uses a custom resource DNSProvider to dynamically manage the backend DNS services. While with external-dns you have to specify the single provider during startup, in the Gardener DNS controller you can add/update/delete providers during runtime with different credentials and/or backends. This is important for a multi-tenant environment as in Gardener, where users can bring their own accounts.\nA DNS provider can also restrict its actions on subset of the DNS domains (includes and excludes) for which the credentials are capable to edit.\nEach provider can define a separate “owner” identifier, to differentiate DNS entries in the same DNS zone from different providers.\nMulti cluster support  The Gardener DNS controller distinguish three different logical Kubernetes clusters: Source cluster, target cluster and runtime cluster. The source cluster is monitored by the DNS source controllers for annotations on ingress and service resources. These controllers then create DNS entries in the target cluster. DNS entries in the target cluster are then reconciliated/synchronized with the corresponding DNS backend service by the provider controller. The runtime cluster is the cluster the DNS controller runs on. For example, this enables needed flexibility in the Gardener deployment. The DNS controller runs on the seed cluster. This is also the target cluster. DNS providers and entries resources are created in the corresponding namespace of the shoot control plane, while the source cluster is the shoot cluster itself.\nOptimizations for handling hundreds of DNS entries  Some DNS backend services are restricted on the API calls per second (e.g. the AWS Route 53 API). To manage hundreds of DNS entries it is important to minimize the number of API calls. The Gardener DNS controller heavily makes usage of caches and batch processing for this reason.\n"
},
{
	"uri": "https://gardener.cloud/documentation/050-tutorials/content/howto/dockerfile_pitfall/",
	"title": "Dockerfile pitfalls",
	"tags": [],
	"description": "Common Dockerfile pitfalls",
	"content": "Using latest tag for an image Many Dockerfiles use the FROM package:latest pattern at the top of their Dockerfiles to pull the latest image from a Docker registry.\nBad Dockerfile FROMalpine\rWhile simple, using the latest tag for an image means that your build can suddenly break if that image gets updated. This can lead to problems where everything builds fine locally (because your local cache thinks it is the latest) while a build server may fail, because some Pipelines makes a clean pull on every build. Additionally, troubleshooting can prove to be difficult, since the maintainer of the Dockerfile didn't actually make any changes.\nGood Dockerfile A digest takes the place of the tag when pulling an image. This will ensure your Dockerfile remains immutable.\nFROMalpine@sha256:7043076348bf5040220df6ad703798fd8593a0918d06d3ce30c6c93be117e430\rRunning apt/apk/yum update Running apt-get install is one of those things virtually every Debian-based Dockerfile will have to satiate some external package requirements your code needs to run. But, using apt-get as an example, comes with its own problems.\napt-get upgrade\nThis will update all your packages to their latests versions, which can be bad because it prevents your Dockerfile from creating consistent, immutable builds.\napt-get update in a different line than running your apt-get install command.\nRunning apt-get update as a single line entry will get cached by the build and won't actually run every time you need to run apt-get install. Instead, make sure you run apt-get update in the same line with all the packages to ensure all are updated correctly.\nAvoid big container images Building small container image will reduce the time needed to start or restart pods. An image based on the popular Alpine Linux project is much smaller than most distribution based images (~5MB). For most popular languages and products, there are usually an official Alpine Linux image, e.g. golang, nodejs and postgres.\n$ docker images\rREPOSITORY TAG IMAGE ID CREATED SIZE\rpostgres 9.6.9-alpine 6583932564f8 13 days ago 39.26 MB\rpostgres 9.6 d92dad241eff 13 days ago 235.4 MB\rpostgres 10.4-alpine 93797b0f31f4 13 days ago 39.56 MB\rIn addition, for compiled languages such as Go or C++ which does not requires build time tooling during runtime, it is recommended to avoid build time tooling in the final images. With Docker's support for multi-stages builds this can be easily achieved with minimal effort. Such an example can be found here.\nGoogle's distroless image is also a good base image.\n"
},
{
	"uri": "https://gardener.cloud/contribute/docs/",
	"title": "Documentation",
	"tags": [],
	"description": "",
	"content": " Contributing Documentation\rHow to Contribute to the Open Source Project Gardener\r\r\r\rYou are welcome to contribute documentation to Gardener.\nThe following rules govern documentation contributions:\n Contributions must be licensed under the Creative Commons Attribution 4.0 International License You need to sign the Contributor License Agreement. We are using CLA assistant providing a click-through workflow for accepting the CLA. For company contributors additionally the company needs to sign a corporate license agreement. See the following sections for details.  \r"
},
{
	"uri": "https://gardener.cloud/documentation/",
	"title": "Documentations",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gardener.cloud/documentation/050-tutorials/content/howto/dynamic-pvc/",
	"title": "Dynamic Volume Provisioning",
	"tags": [],
	"description": "How to dynamically provision volume",
	"content": "Introduction The example shows how to run a postgres database on Kubernetes and how to dynamically provision and mount the storage volumes needed by the database\nRun postgres database Define the following Kubernetes resources in a yaml file\n PersistentVolumeClaim (PVC) Deployment  PersistentVolumeClaim apiVersion: v1\rkind: PersistentVolumeClaim\rmetadata:\rname: postgresdb-pvc\rspec:\raccessModes:\r- ReadWriteOnce\rresources:\rrequests:\rstorage: 9Gi\rstorageClassName: \u0026#39;default\u0026#39;\rThis defines a PVC using storage class default. Storage classes abstract from the underlying storage provider as well as other parameters, like disk-type (e.g.; solid-state vs standard disks).\nThe default storage class has annotation {\u0026ldquo;storageclass.kubernetes.io/is-default-class\u0026rdquo;:\u0026ldquo;true\u0026rdquo;}.\n$ kubectl describe sc default\rName: default\rIsDefaultClass: Yes\rAnnotations: kubectl.kubernetes.io/last-applied-configuration={\u0026#34;apiVersion\u0026#34;:\u0026#34;storage.k8s.io/v1beta1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;StorageClass\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{\u0026#34;storageclass.kubernetes.io/is-default-class\u0026#34;:\u0026#34;true\u0026#34;},\u0026#34;labels\u0026#34;:{\u0026#34;addonmanager.kubernetes.io/mode\u0026#34;:\u0026#34;Exists\u0026#34;},\u0026#34;name\u0026#34;:\u0026#34;default\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;\u0026#34;},\u0026#34;parameters\u0026#34;:{\u0026#34;type\u0026#34;:\u0026#34;gp2\u0026#34;},\u0026#34;provisioner\u0026#34;:\u0026#34;kubernetes.io/aws-ebs\u0026#34;}\r,storageclass.kubernetes.io/is-default-class=true\rProvisioner: kubernetes.io/aws-ebs\rParameters: type=gp2\rAllowVolumeExpansion: \u0026lt;unset\u0026gt;\rMountOptions: \u0026lt;none\u0026gt;\rReclaimPolicy: Delete\rVolumeBindingMode: Immediate\rEvents: \u0026lt;none\u0026gt;\rA Persistent Volume is automatically created when it is dynamically provisioned. In following example, the PVC is defined as \u0026ldquo;postgresdb-pvc\u0026rdquo;, and a corresponding PV \u0026ldquo;pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb\u0026rdquo; is created and associated with pvc automatically.\n$ kubectl create -f .\\postgres_deployment.yaml\rpersistentvolumeclaim \u0026#34;postgresdb-pvc\u0026#34; created\r$ kubectl get pv\rNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE\rpvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb 9Gi RWO Delete Bound default/postgresdb-pvc default 3s\r$ kubectl get pvc\rNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE\rpostgresdb-pvc Bound pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb 9Gi RWO default 8s\rNotice that the RECLAIM POLICY is Delete (default value), which is one of the two reclaim policies, the other one is Retain. (A third policy Recycle has been deprecated). In case of Delete, the PV is deleted automatically when the PVC is removed, and the data on the PVC will also be lost.\nOn the other hand, PV with Retain policy will not be deleted when the PVC is removed, and moved to Release status, so that data can be recovered by Administrators later.\nYou can use the kubectl patch command to change the reclaim policy as described here here or use kubectl edit pv \u0026lt;pv-name\u0026gt; to edit online as below:\n$ kubectl get pv\rNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE\rpvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb 9Gi RWO Delete Bound default/postgresdb-pvc default 44m\r# change the relcaim policy from \u0026#34;Delete\u0026#34; to \u0026#34;Retain\u0026#34;\r$ kubectl edit pv pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb\rpersistentvolume \u0026#34;pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb\u0026#34; edited\r# check the reclaim policy afterwards\r$ kubectl get pv\rNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE\rpvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb 9Gi RWO Retain Bound default/postgresdb-pvc default 45m\rDeployment Once a PVC is created, you can use it in your container via volumes.persistentVolumeClaim.claimName. In below example, pvc postgresdb-pvc is mounted as readable and writable, and in volumeMounts two paths in the container are mounted to subfolders in the volume.\napiVersion: apps/v1\rkind: Deployment\rmetadata:\rname: postgres\rnamespace: default\rlabels:\rapp: postgres\rannotations:\rdeployment.kubernetes.io/revision: \u0026#34;1\u0026#34;\rspec:\rreplicas: 1\rstrategy:\rtype: RollingUpdate\rrollingUpdate:\rmaxUnavailable: 1\rmaxSurge: 1\rselector:\rmatchLabels:\rapp: postgres\rtemplate:\rmetadata:\rname: postgres\rlabels:\rapp: postgres\rspec:\rcontainers:\r- name: postgres\rimage: \u0026#34;cpettech.docker.repositories.sap.ondemand.com/jtrack_postgres:howto\u0026#34;\renv:\r- name: POSTGRES_USER\rvalue: postgres\r- name: POSTGRES_PASSWORD\rvalue: p5FVqfuJFrM42cVX9muQXxrC3r8S9yn0zqWnFR6xCoPqxqVQ\r- name: POSTGRES_INITDB_XLOGDIR\rvalue: \u0026#34;/var/log/postgresql/logs\u0026#34;\rports:\r- containerPort: 5432\rvolumeMounts:\r- mountPath: /var/lib/postgresql/data\rname: postgre-db\rsubPath: data # https://github.com/kubernetes/website/pull/2292. Solve the issue of crashing initdb due to non-empty directory (i.e. lost+found)\r - mountPath: /var/log/postgresql/logs\rname: postgre-db\rsubPath: logs\rvolumes:\r- name: postgre-db\rpersistentVolumeClaim:\rclaimName: postgresdb-pvc\rreadOnly: false\rimagePullSecrets:\r- name: cpettechregistry\rTo check the mount points in the container:\n$ kubectl get po\rNAME READY STATUS RESTARTS AGE\rpostgres-7f485fd768-c5jf9 1/1 Running 0 32m\r$ kubectl exec -it postgres-7f485fd768-c5jf9 bash\rroot@postgres-7f485fd768-c5jf9:/# ls /var/lib/postgresql/data/\rbase pg_clog pg_dynshmem pg_ident.conf pg_multixact pg_replslot pg_snapshots pg_stat_tmp pg_tblspc PG_VERSION postgresql.auto.conf postmaster.opts\rglobal pg_commit_ts pg_hba.conf pg_logical pg_notify pg_serial pg_stat pg_subtrans pg_twophase pg_xlog postgresql.conf postmaster.pid\rroot@postgres-7f485fd768-c5jf9:/# ls /var/log/postgresql/logs/\r000000010000000000000001 archive_status\rDeleting a PersistentVolumeClaim In case of \u0026ldquo;Delete\u0026rdquo; policy, deleting a PVC will also delete its associated PV. If \u0026ldquo;Retain\u0026rdquo; is the reclaim policy, the PV will change status from Bound to Released when PVC is deleted.\n# Check pvc and pv before deletion\r$ kubectl get pvc\rNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE\rpostgresdb-pvc Bound pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb 9Gi RWO default 50m\r$ kubectl get pv\rNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE\rpvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb 9Gi RWO Retain Bound default/postgresdb-pvc default 50m\r# delete pvc\r$ kubectl delete pvc postgresdb-pvc\rpersistentvolumeclaim \u0026#34;postgresdb-pvc\u0026#34; deleted\r# pv changed to status \u0026#34;Released\u0026#34;\r$ kubectl get pv\rNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE\rpvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb 9Gi RWO Retain Released default/postgresdb-pvc default 51m\r"
},
{
	"uri": "https://gardener.cloud/documentation/045_contribute/10_code/13_env/",
	"title": "Enviroment",
	"tags": [],
	"description": "",
	"content": "Preparing the Setup Conceptually, all Gardener components are designated to run inside as a Pod inside a Kubernetes cluster. The API server extends the Kubernetes API via the user-aggregated API server concepts. However, if you want to develop it, you may want to work locally with the Gardener without building a Docker image and deploying it to a cluster each and every time. That means that the Gardener runs outside a Kubernetes cluster which requires providing a Kubeconfig in your local filesystem and point the Gardener to it when starting it (see below).\nFurther details could be found in\n Principles of Kubernetes, and its components Kubernetes Development Guide Architecture of Gardener  This setup is based on minikube, a Kubernetes cluster running on a single node. Docker for Desktop and kind are also supported.\nInstalling Golang environment Install latest version of Golang. For MacOS you could use Homebrew:\nbrew install golang\rFor other OS, please check Go installation documentation.\nInstalling kubectl and helm As already mentioned in the introduction, the communication with the Gardener happens via the Kubernetes (Garden) cluster it is targeting. To interact with that cluster, you need to install kubectl. Please make sure that the version of kubectl is at least v1.11.x.\nOn MacOS run\nbrew install kubernetes-cli\rPlease check the kubectl installation documentation for other OS.\nYou may also need to develop Helm charts or interact with Tiller using the Helm CLI:\nOn MacOS run\nbrew install kubernetes-helm\rOn other OS please check the Helm installation documentation.\nInstalling git We use git as VCS which you need to install.\nOn MacOS run\nbrew install git\rOn other OS, please check the Git installation documentation.\nInstalling openvpn We use OpenVPN to establish network connectivity from the control plane running in the Seed cluster to the Shoot's worker nodes running in private networks. To harden the security we need to generate another secret to encrypt the network traffic (details). Please install the openvpn binary. On MacOS run\nbrew install openvpn\rexport PATH=$(brew --prefix openvpn)/sbin:$PATH\rOn other OS, please check the OpenVPN downloads page.\nInstalling Minikube You'll need to have minikube installed and running.\n Note: Gardener is working only with self-contained kubeconfig files because of security issue. You can configure your minikube to create self-contained kubeconfig files via:\nminikube config set embed-certs true\r Alternatively, you can also install Docker for Desktop and kind.\nIn case you want to use the \u0026ldquo;Docker for Mac Kubernetes\u0026rdquo; or if you want to build Docker images for the Gardener you have to install Docker itself. On MacOS, please use Docker for MacOS which can be downloaded here.\nOn other OS, please check the Docker installation documentation.\nInstalling iproute2 iproute2 provides a collection of utilities for network administration and configuration.\nOn MacOS run\nbrew install iproute2mac\rInstalling yaml2json and jq go get -u github.com/bronze1man/yaml2json\rbrew install jq\r[MacOS only] Install GNU core utilities When running on MacOS you have to install the GNU core utilities:\nbrew install coreutils gnu-sed\rThis will create symbolic links for the GNU utilities with g prefix in /usr/local/bin, e.g., gsed or gbase64. To allow using them without the g prefix please put /usr/local/opt/coreutils/libexec/gnubin at the beginning of your PATH environment variable, e.g., export PATH=/usr/local/opt/coreutils/libexec/gnubin:$PATH.\n[Optional] Installing gcloud SDK In case you have to create a new release or a new hotfix of the Gardener you have to push the resulting Docker image into a Docker registry. Currently, we are using the Google Container Registry (this could change in the future). Please follow the official installation instructions from Google.\nLocal Gardener setup This setup is only meant to be used for developing purposes, which means that only the control plane of the Gardener cluster is running on your machine.\nGet the sources Clone the repository from GitHub.\ngit clone git@github.com:gardener/gardener.git\rcd gardener\rStart the Gardener :warning: Before you start developing, please ensure to comply with the following requirements:\n You have understood the principles of Kubernetes, and its components, what their purpose is and how they interact with each other. You have understood the architecture of Gardener, and what the various clusters are used for.  Start a local kubernetes cluster For the development of Gardener you need some kind of Kubernetes cluster, which can be used as a \u0026ldquo;garden\u0026rdquo; cluster. I.e. you need a Kubernetes API server on which you can register a APIService Gardener's own Extension API Server.\nFor this you can use a standard tool from the community to setup a local cluster like minikube, kind or the Kubernetes Cluster feature in Docker for Desktop.\nHowever, if you develop and run Gardener's components locally, you don't actually a fully fledged Kubernetes Cluster, i.e. you don't actually need to run Pods on it. If you want to use a more lightweight approach for development purposes, you can use the \u0026ldquo;nodeless Garden cluster setup\u0026rdquo; residing in hack/local-garden. This is the easiest way to get your Gardener development setup up and running.\nUsing the nodeless cluster setup\nSetting up a local nodeless Garden cluster is quite simple. The only prerequisite is a running docker daemon. Just use the provided Makefile rules to start your local Garden:\nmake local-garden-up\r[...]\rStarting gardener-dev kube-etcd cluster..!\rStarting gardener-dev kube-apiserver..!\rStarting gardener-dev kube-controller-manager..!\rStarting gardener-dev gardener-etcd cluster..!\rnamespace/garden created\rclusterrole.rbac.authorization.k8s.io/gardener.cloud:admin created\rclusterrolebinding.rbac.authorization.k8s.io/front-proxy-client created\r[...]\rThis will start all minimally required components of a Kubernetes cluster (etcd, kube-apiserver, kube-controller-manager) and an etcd Instance for the gardener-apiserver as Docker containers.\nTo tear down the local Garden cluster and remove the Docker containers, simply run:\nmake local-garden-down\rUsing minikube\nAlternatively, spin up a cluster with minikube with this command:\nminikube start --embed-certs # `--embed-certs` can be omitted if minikube has already been set to create self-contained kubeconfig files.\r😄 minikube v1.8.2 on Darwin 10.15.3\r🔥 Creating virtualbox VM (CPUs=2, Memory=2048MB, Disk=20000MB) ...\r[...]\r🏄 Done! Thank you for using minikube!\rPrepare the Gardener Now, that you have started your local cluster, we can go ahead and register the Gardener API Server. Just point your KUBECONFIG environment variable to the local cluster you created in the previous step and run:\nmake dev-setup\rFound Minikube ...\rnamespace/garden created\rnamespace/garden-dev created\rdeployment.apps/etcd created\rservice/etcd created\rservice/gardener-apiserver created\rservice/gardener-controller-manager created\rendpoints/gardener-apiserver created\rendpoints/gardener-controller-manager created\rapiservice.apiregistration.k8s.io/v1alpha1.core.gardener.cloud created\rapiservice.apiregistration.k8s.io/v1beta1.core.gardener.cloud created\rvalidatingwebhookconfiguration.admissionregistration.k8s.io/gardener-controller-manager created\rOptionally, you can switch off the Logging feature gate of Gardenlet to save resources:\nsed -i -e \u0026#39;s/Logging: true/Logging: false/g\u0026#39; dev/20-componentconfig-gardenlet.yaml\rThe Gardener exposes the API servers of Shoot clusters via Kubernetes services of type LoadBalancer. In order to establish stable endpoints (robust against changes of the load balancer address), it creates DNS records pointing to these load balancer addresses. They are used internally and by all cluster components to communicate. You need to have control over a domain (or subdomain) for which these records will be created. Please provide an internal domain secret (see this for an example) which contains credentials with the proper privileges. Further information can be found here.\nkubectl apply -f example/10-secret-internal-domain-unmanaged.yaml\rsecret/internal-domain-unmanaged created\rRun the Gardener Next, run the Gardener API Server, the Gardener Controller Manager (optionally), the Gardener Scheduler (optionally), and the Gardenlet in different terminal windows/panes using rules in the Makefile.\nmake start-apiserver\rFound Minikube ...\rI0306 15:23:51.044421 74536 plugins.go:84] Registered admission plugin \u0026#34;ResourceReferenceManager\u0026#34;\rI0306 15:23:51.044523 74536 plugins.go:84] Registered admission plugin \u0026#34;DeletionConfirmation\u0026#34;\r[...]\rI0306 15:23:51.626836 74536 secure_serving.go:116] Serving securely on [::]:8443\r[...]\r(Optional) Now you are ready to launch the Gardener Controller Manager.\nmake start-controller-manager\rtime=\u0026#34;2019-03-06T15:24:17+02:00\u0026#34; level=info msg=\u0026#34;Starting Gardener controller manager...\u0026#34;\rtime=\u0026#34;2019-03-06T15:24:17+02:00\u0026#34; level=info msg=\u0026#34;Feature Gates: \u0026#34;\rtime=\u0026#34;2019-03-06T15:24:17+02:00\u0026#34; level=info msg=\u0026#34;Starting HTTP server on 0.0.0.0:2718\u0026#34;\rtime=\u0026#34;2019-03-06T15:24:17+02:00\u0026#34; level=info msg=\u0026#34;Acquired leadership, starting controllers.\u0026#34;\rtime=\u0026#34;2019-03-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;Starting HTTPS server on 0.0.0.0:2719\u0026#34;\rtime=\u0026#34;2019-03-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;Found internal domain secret internal-domain-unmanaged for domain nip.io.\u0026#34;\rtime=\u0026#34;2019-03-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;Successfully bootstrapped the Garden cluster.\u0026#34;\rtime=\u0026#34;2019-03-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;Gardener controller manager (version 1.0.0-dev) initialized.\u0026#34;\rtime=\u0026#34;2019-03-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;ControllerRegistration controller initialized.\u0026#34;\rtime=\u0026#34;2019-03-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;SecretBinding controller initialized.\u0026#34;\rtime=\u0026#34;2019-03-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;Project controller initialized.\u0026#34;\rtime=\u0026#34;2019-03-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;Quota controller initialized.\u0026#34;\rtime=\u0026#34;2019-03-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;CloudProfile controller initialized.\u0026#34;\r[...]\r(Optional) Now you are ready to launch the Gardener Scheduler.\nmake start-scheduler\rtime=\u0026#34;2019-05-02T16:31:50+02:00\u0026#34; level=info msg=\u0026#34;Starting Gardener scheduler ...\u0026#34;\rtime=\u0026#34;2019-05-02T16:31:50+02:00\u0026#34; level=info msg=\u0026#34;Starting HTTP server on 0.0.0.0:10251\u0026#34;\rtime=\u0026#34;2019-05-02T16:31:50+02:00\u0026#34; level=info msg=\u0026#34;Acquired leadership, starting scheduler.\u0026#34;\rtime=\u0026#34;2019-05-02T16:31:50+02:00\u0026#34; level=info msg=\u0026#34;Gardener scheduler initialized (with Strategy: SameRegion)\u0026#34;\rtime=\u0026#34;2019-05-02T16:31:50+02:00\u0026#34; level=info msg=\u0026#34;Scheduler controller initialized.\u0026#34;\r[...]\r(Optional) Now you are ready to launch the Gardenlet.\nmake start-gardenlet\rtime=\u0026#34;2019-11-06T15:24:17+02:00\u0026#34; level=info msg=\u0026#34;Starting Gardenlet...\u0026#34;\rtime=\u0026#34;2019-11-06T15:24:17+02:00\u0026#34; level=info msg=\u0026#34;Feature Gates: HVPA=true, Logging=true\u0026#34;\rtime=\u0026#34;2019-11-06T15:24:17+02:00\u0026#34; level=info msg=\u0026#34;Acquired leadership, starting controllers.\u0026#34;\rtime=\u0026#34;2019-11-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;Found internal domain secret internal-domain-unmanaged for domain nip.io.\u0026#34;\rtime=\u0026#34;2019-11-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;Gardenlet (version 1.0.0-dev) initialized.\u0026#34;\rtime=\u0026#34;2019-11-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;ControllerInstallation controller initialized.\u0026#34;\rtime=\u0026#34;2019-11-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;Shoot controller initialized.\u0026#34;\rtime=\u0026#34;2019-11-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;Seed controller initialized.\u0026#34;\r[...]\r:warning: The Gardenlet will handle all your seeds for this development scenario, although, for productive usage it is recommended to run it once per seed, see this document for more information.\nPlease checkout the Gardener Extensions Manager to install extension controllers - make sure that you install all of them required for your local development. Also, please refer to this document for further information about how extensions are registered in case you want to use other versions than the latest releases.\nThe Gardener should now be ready to operate on Shoot resources. You can use\nkubectl get shoots\rNo resources found.\rto operate against your local running Gardener API Server.\n Note: It may take several seconds until the minikube cluster recognizes that the Gardener API server has been started and is available. No resources found is the expected result of our initial development setup.\n Limitations of local development setup You can run Gardener (API server, controller manager, scheduler, gardenlet) against any local Kubernetes cluster, however, your seed and shoot clusters must be deployed to a \u0026ldquo;real\u0026rdquo; provider. Currently, it is not possible to run Gardener entirely isolated from any cloud provider. We are planning to support such a setup based on KubeVirt (see this for details), however, it does not yet exist. This means that - after you have setup Gardener - you need to register an external seed cluster (e.g., one created in AWS). Only after that step you can start creating shoot clusters with your locally running Gardener.\nSome time ago, we had a local setup based on VirtualBox/Vagrant. However, as we have progressed with the Extensibility epic we noticed that this implementation/setup does no longer fit into how we envision external providers to be. Moreover, it hid too many things and came with a bunch of limitations, making the development scenario too \u0026ldquo;artificial\u0026rdquo;:\n No integration with machine-controller-manager. The Shoot API Server is exposed via a NodePort. In a cloud setup a LoadBalancer would be used. It was not possible to create Shoot clusters consisting of more than one worker node. Cluster auto-scaling therefore is not supported. It was not possible to create two or more Shoot clusters in parallel. The communication between the Seed and the Shoot Clusters uses VPN tunnel. In this setup tunnels are not needed since all components run on localhost.  Additional information To make sure that a specific Seed cluster will be chosen, specify the .spec.seedName field (see here for an example Shoot manifest).\nPlease take a look at the example manifests folder to see which resource objects you need to install into your Garden cluster.\n"
},
{
	"uri": "https://gardener.cloud/documentation/045_contribute/20_documentation/25_markup/expand/",
	"title": "Expand",
	"tags": [],
	"description": "Displays an expandable/collapsible section of text on your page",
	"content": "The Expand shortcode displays an expandable/collapsible section of text on your page. Here is an example\n\r\r Expand me...\r\u0026lt;/span\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;div class=\u0026quot;expand-content\u0026quot; style=\u0026quot;display: none;\u0026quot;\u0026gt;\r Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n\u0026lt;/div\u0026gt;\r \rUsage this shortcode takes exactly one optional parameter to define the text that appears next to the expand/collapse icon. (default is \u0026ldquo;Expand me\u0026hellip;\u0026quot;)\n{{%expand \u0026quot;Is this learn theme rocks ?\u0026quot; %}}Yes !.{{% /expand%}}\r \r\r Is this learn theme rocks ?\r\u0026lt;/span\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;div class=\u0026quot;expand-content\u0026quot; style=\u0026quot;display: none;\u0026quot;\u0026gt;\rYes !\r\u0026lt;/div\u0026gt;\r \rDemo {{%expand%}}\rLorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod\rtempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam,\rquis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo\rconsequat. Duis aute irure dolor in reprehenderit in voluptate velit esse\rcillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non\rproident, sunt in culpa qui officia deserunt mollit anim id est laborum.\r{{% /expand%}}\r \r\r Expand me...\r\u0026lt;/span\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;div class=\u0026quot;expand-content\u0026quot; style=\u0026quot;display: none;\u0026quot;\u0026gt;\rLorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod\r tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. \n"
},
{
	"uri": "https://gardener.cloud/api-reference/extensions/",
	"title": "Extensions",
	"tags": [],
	"description": "",
	"content": "Packages:\n\r\rextensions.gardener.cloud/v1alpha1\r\r\rextensions.gardener.cloud/v1alpha1\r\rPackage v1alpha1 is the v1alpha1 version of the API.\nResource Types:\r\rBackupBucket\r\rBackupEntry\r\rCluster\r\rContainerRuntime\r\rControlPlane\r\rExtension\r\rInfrastructure\r\rNetwork\r\rOperatingSystemConfig\r\rWorker\r\rBackupBucket\r\r\rBackupBucket is a specification for backup bucket.\n\r\r\rField\rDescription\r\r\r\r\r\rapiVersion\rstring\r\r\rextensions.gardener.cloud/v1alpha1\r\r\r\r\r\rkind\rstring\r\rBackupBucket\r\r\r\rmetadata\r\rKubernetes meta/v1.ObjectMeta\r\r\r\r\r(Optional)\rRefer to the Kubernetes API documentation for the fields of the\rmetadata field.\r\r\r\r\rspec\r\rBackupBucketSpec\r\r\r\r\r\r\r\r\r\rDefaultSpec\r\rDefaultSpec\r\r\r\r\r\r(Members of DefaultSpec are embedded into this type.)\rDefaultSpec is a structure containing common fields used by all extension resources.\n\r\r\r\rregion\r\rstring\r\r\r\rRegion is the region of this bucket.\n\r\r\r\rsecretRef\r\rKubernetes core/v1.SecretReference\r\r\r\r\rSecretRef is a reference to a secret that contains the credentials to access object store.\n\r\r\r\r\r\r\rstatus\r\rBackupBucketStatus\r\r\r\r\r\r\r\r\rBackupEntry\r\r\rBackupEntry is a specification for backup Entry.\n\r\r\rField\rDescription\r\r\r\r\r\rapiVersion\rstring\r\r\rextensions.gardener.cloud/v1alpha1\r\r\r\r\r\rkind\rstring\r\rBackupEntry\r\r\r\rmetadata\r\rKubernetes meta/v1.ObjectMeta\r\r\r\r\r(Optional)\rRefer to the Kubernetes API documentation for the fields of the\rmetadata field.\r\r\r\r\rspec\r\rBackupEntrySpec\r\r\r\r\r\r\r\r\r\rDefaultSpec\r\rDefaultSpec\r\r\r\r\r\r(Members of DefaultSpec are embedded into this type.)\rDefaultSpec is a structure containing common fields used by all extension resources.\n\r\r\r\rbackupBucketProviderStatus\r\rk8s.io/apimachinery/pkg/runtime.RawExtension\r\r\r\r\r(Optional)\rBackupBucketProviderStatus contains the provider status that has\rbeen generated by the controller responsible for the BackupBucket resource.\n\r\r\r\rregion\r\rstring\r\r\r\rRegion is the region of this Entry.\n\r\r\r\rbucketName\r\rstring\r\r\r\rBucketName is the name of backup bucket for this Backup Entry.\n\r\r\r\rsecretRef\r\rKubernetes core/v1.SecretReference\r\r\r\r\rSecretRef is a reference to a secret that contains the credentials to access object store.\n\r\r\r\r\r\r\rstatus\r\rBackupEntryStatus\r\r\r\r\r\r\r\r\rCluster\r\r\rCluster is a specification for a Cluster resource.\n\r\r\rField\rDescription\r\r\r\r\r\rapiVersion\rstring\r\r\rextensions.gardener.cloud/v1alpha1\r\r\r\r\r\rkind\rstring\r\rCluster\r\r\r\rmetadata\r\rKubernetes meta/v1.ObjectMeta\r\r\r\r\rRefer to the Kubernetes API documentation for the fields of the\rmetadata field.\r\r\r\r\rspec\r\rClusterSpec\r\r\r\r\r\r\r\r\r\rcloudProfile\r\rk8s.io/apimachinery/pkg/runtime.RawExtension\r\r\r\r\rCloudProfile is a raw extension field that contains the cloudprofile resource referenced\rby the shoot that has to be reconciled.\n\r\r\r\rseed\r\rk8s.io/apimachinery/pkg/runtime.RawExtension\r\r\r\r\rSeed is a raw extension field that contains the seed resource referenced by the shoot that\rhas to be reconciled.\n\r\r\r\rshoot\r\rk8s.io/apimachinery/pkg/runtime.RawExtension\r\r\r\r\rShoot is a raw extension field that contains the shoot resource that has to be reconciled.\n\r\r\r\r\r\r\rContainerRuntime\r\r\rContainerRuntime is a specification for a container runtime resource.\n\r\r\rField\rDescription\r\r\r\r\r\rapiVersion\rstring\r\r\rextensions.gardener.cloud/v1alpha1\r\r\r\r\r\rkind\rstring\r\rContainerRuntime\r\r\r\rmetadata\r\rKubernetes meta/v1.ObjectMeta\r\r\r\r\r(Optional)\rRefer to the Kubernetes API documentation for the fields of the\rmetadata field.\r\r\r\r\rspec\r\rContainerRuntimeSpec\r\r\r\r\r\r\r\r\r\rbinaryPath\r\rstring\r\r\r\rBinaryPath is the Worker\u0026rsquo;s machine path where container runtime extensions should copy the binaries to.\n\r\r\r\rworkerPool\r\rContainerRuntimeWorkerPool\r\r\r\r\rWorkerPool identifies the worker pool of the Shoot.\rFor each worker pool and type, Gardener deploys a ContainerRuntime CRD.\n\r\r\r\rDefaultSpec\r\rDefaultSpec\r\r\r\r\r\r(Members of DefaultSpec are embedded into this type.)\rDefaultSpec is a structure containing common fields used by all extension resources.\n\r\r\r\r\r\r\rstatus\r\rContainerRuntimeStatus\r\r\r\r\r\r\r\r\rControlPlane\r\r\rControlPlane is a specification for a ControlPlane resource.\n\r\r\rField\rDescription\r\r\r\r\r\rapiVersion\rstring\r\r\rextensions.gardener.cloud/v1alpha1\r\r\r\r\r\rkind\rstring\r\rControlPlane\r\r\r\rmetadata\r\rKubernetes meta/v1.ObjectMeta\r\r\r\r\rRefer to the Kubernetes API documentation for the fields of the\rmetadata field.\r\r\r\r\rspec\r\rControlPlaneSpec\r\r\r\r\r\r\r\r\r\rDefaultSpec\r\rDefaultSpec\r\r\r\r\r\r(Members of DefaultSpec are embedded into this type.)\rDefaultSpec is a structure containing common fields used by all extension resources.\n\r\r\r\rpurpose\r\rPurpose\r\r\r\r\r(Optional)\rPurpose contains the data if a cloud provider needs additional components in order to expose the control plane.\n\r\r\r\rinfrastructureProviderStatus\r\rk8s.io/apimachinery/pkg/runtime.RawExtension\r\r\r\r\r(Optional)\rInfrastructureProviderStatus contains the provider status that has\rbeen generated by the controller responsible for the Infrastructure resource.\n\r\r\r\rregion\r\rstring\r\r\r\rRegion is the region of this control plane.\n\r\r\r\rsecretRef\r\rKubernetes core/v1.SecretReference\r\r\r\r\rSecretRef is a reference to a secret that contains the cloud provider specific credentials.\n\r\r\r\r\r\r\rstatus\r\rControlPlaneStatus\r\r\r\r\r\r\r\r\rExtension\r\r\rExtension is a specification for a Extension resource.\n\r\r\rField\rDescription\r\r\r\r\r\rapiVersion\rstring\r\r\rextensions.gardener.cloud/v1alpha1\r\r\r\r\r\rkind\rstring\r\rExtension\r\r\r\rmetadata\r\rKubernetes meta/v1.ObjectMeta\r\r\r\r\r(Optional)\rRefer to the Kubernetes API documentation for the fields of the\rmetadata field.\r\r\r\r\rspec\r\rExtensionSpec\r\r\r\r\r\r\r\r\r\rDefaultSpec\r\rDefaultSpec\r\r\r\r\r\r(Members of DefaultSpec are embedded into this type.)\rDefaultSpec is a structure containing common fields used by all extension resources.\n\r\r\r\r\r\r\rstatus\r\rExtensionStatus\r\r\r\r\r\r\r\r\rInfrastructure\r\r\rInfrastructure is a specification for cloud provider infrastructure.\n\r\r\rField\rDescription\r\r\r\r\r\rapiVersion\rstring\r\r\rextensions.gardener.cloud/v1alpha1\r\r\r\r\r\rkind\rstring\r\rInfrastructure\r\r\r\rmetadata\r\rKubernetes meta/v1.ObjectMeta\r\r\r\r\r(Optional)\rRefer to the Kubernetes API documentation for the fields of the\rmetadata field.\r\r\r\r\rspec\r\rInfrastructureSpec\r\r\r\r\r\r\r\r\r\rDefaultSpec\r\rDefaultSpec\r\r\r\r\r\r(Members of DefaultSpec are embedded into this type.)\rDefaultSpec is a structure containing common fields used by all extension resources.\n\r\r\r\rregion\r\rstring\r\r\r\rRegion is the region of this infrastructure.\n\r\r\r\rsecretRef\r\rKubernetes core/v1.SecretReference\r\r\r\r\rSecretRef is a reference to a secret that contains the actual result of the generated cloud config.\n\r\r\r\rsshPublicKey\r\r[]byte\r\r\r\r(Optional)\rSSHPublicKey is the public SSH key that should be used with this infrastructure.\n\r\r\r\r\r\r\rstatus\r\rInfrastructureStatus\r\r\r\r\r\r\r\r\rNetwork\r\r\rNetwork is the specification for cluster networking.\n\r\r\rField\rDescription\r\r\r\r\r\rapiVersion\rstring\r\r\rextensions.gardener.cloud/v1alpha1\r\r\r\r\r\rkind\rstring\r\rNetwork\r\r\r\rmetadata\r\rKubernetes meta/v1.ObjectMeta\r\r\r\r\r(Optional)\rRefer to the Kubernetes API documentation for the fields of the\rmetadata field.\r\r\r\r\rspec\r\rNetworkSpec\r\r\r\r\r\r\r\r\r\rDefaultSpec\r\rDefaultSpec\r\r\r\r\r\r(Members of DefaultSpec are embedded into this type.)\rDefaultSpec is a structure containing common fields used by all extension resources.\n\r\r\r\rpodCIDR\r\rstring\r\r\r\rPodCIDR defines the CIDR that will be used for pods.\n\r\r\r\rserviceCIDR\r\rstring\r\r\r\rServiceCIDR defines the CIDR that will be used for services.\n\r\r\r\r\r\r\rstatus\r\rNetworkStatus\r\r\r\r\r\r\r\r\rOperatingSystemConfig\r\r\rOperatingSystemConfig is a specification for a OperatingSystemConfig resource\n\r\r\rField\rDescription\r\r\r\r\r\rapiVersion\rstring\r\r\rextensions.gardener.cloud/v1alpha1\r\r\r\r\r\rkind\rstring\r\rOperatingSystemConfig\r\r\r\rmetadata\r\rKubernetes meta/v1.ObjectMeta\r\r\r\r\r(Optional)\rRefer to the Kubernetes API documentation for the fields of the\rmetadata field.\r\r\r\r\rspec\r\rOperatingSystemConfigSpec\r\r\r\r\r\r\r\r\r\rcriConfig\r\rCRIConfig\r\r\r\r\r(Optional)\rCRI config is a structure contains configurations of the CRI library\n\r\r\r\rDefaultSpec\r\rDefaultSpec\r\r\r\r\r\r(Members of DefaultSpec are embedded into this type.)\rDefaultSpec is a structure containing common fields used by all extension resources.\n\r\r\r\rpurpose\r\rOperatingSystemConfigPurpose\r\r\r\r\rPurpose describes how the result of this OperatingSystemConfig is used by Gardener. Either it\rgets sent to the Worker extension controller to bootstrap a VM, or it is downloaded by the\rcloud-config-downloader script already running on a bootstrapped VM.\n\r\r\r\rreloadConfigFilePath\r\rstring\r\r\r\r(Optional)\rReloadConfigFilePath is the path to the generated operating system configuration. If set, controllers\rare asked to use it when determining the .status.command of this resource. For example, if for CoreOS\rthe reload-path might be \u0026ldquo;/var/lib/config\u0026rdquo;; then the controller shall set .status.command to\r\u0026ldquo;/usr/bin/coreos-cloudinit \u0026ndash;from-file=/var/lib/config\u0026rdquo;.\n\r\r\r\runits\r\r[]Unit\r\r\r\r\r(Optional)\rUnits is a list of unit for the operating system configuration (usually, a systemd unit).\n\r\r\r\rfiles\r\r[]File\r\r\r\r\r(Optional)\rFiles is a list of files that should get written to the host\u0026rsquo;s file system.\n\r\r\r\r\r\r\rstatus\r\rOperatingSystemConfigStatus\r\r\r\r\r\r\r\r\rWorker\r\r\rWorker is a specification for a Worker resource.\n\r\r\rField\rDescription\r\r\r\r\r\rapiVersion\rstring\r\r\rextensions.gardener.cloud/v1alpha1\r\r\r\r\r\rkind\rstring\r\rWorker\r\r\r\rmetadata\r\rKubernetes meta/v1.ObjectMeta\r\r\r\r\r(Optional)\rRefer to the Kubernetes API documentation for the fields of the\rmetadata field.\r\r\r\r\rspec\r\rWorkerSpec\r\r\r\r\r\r\r\r\r\rDefaultSpec\r\rDefaultSpec\r\r\r\r\r\r(Members of DefaultSpec are embedded into this type.)\rDefaultSpec is a structure containing common fields used by all extension resources.\n\r\r\r\rinfrastructureProviderStatus\r\rk8s.io/apimachinery/pkg/runtime.RawExtension\r\r\r\r\r(Optional)\rInfrastructureProviderStatus is a raw extension field that contains the provider status that has\rbeen generated by the controller responsible for the Infrastructure resource.\n\r\r\r\rregion\r\rstring\r\r\r\rRegion is the name of the region where the worker pool should be deployed to.\n\r\r\r\rsecretRef\r\rKubernetes core/v1.SecretReference\r\r\r\r\rSecretRef is a reference to a secret that contains the cloud provider specific credentials.\n\r\r\r\rsshPublicKey\r\r[]byte\r\r\r\r(Optional)\rSSHPublicKey is the public SSH key that should be used with these workers.\n\r\r\r\rpools\r\r[]WorkerPool\r\r\r\r\rPools is a list of worker pools.\n\r\r\r\r\r\r\rstatus\r\rWorkerStatus\r\r\r\r\r\r\r\r\rBackupBucketSpec\r\r\r(Appears on:\rBackupBucket)\r\rBackupBucketSpec is the spec for an BackupBucket resource.\n\r\r\rField\rDescription\r\r\r\r\r\rDefaultSpec\r\rDefaultSpec\r\r\r\r\r\r(Members of DefaultSpec are embedded into this type.)\rDefaultSpec is a structure containing common fields used by all extension resources.\n\r\r\r\rregion\r\rstring\r\r\r\rRegion is the region of this bucket.\n\r\r\r\rsecretRef\r\rKubernetes core/v1.SecretReference\r\r\r\r\rSecretRef is a reference to a secret that contains the credentials to access object store.\n\r\r\r\rBackupBucketStatus\r\r\r(Appears on:\rBackupBucket)\r\rBackupBucketStatus is the status for an BackupBucket resource.\n\r\r\rField\rDescription\r\r\r\r\r\rDefaultStatus\r\rDefaultStatus\r\r\r\r\r\r(Members of DefaultStatus are embedded into this type.)\rDefaultStatus is a structure containing common fields used by all extension resources.\n\r\r\r\rgeneratedSecretRef\r\rKubernetes core/v1.SecretReference\r\r\r\r\r(Optional)\rGeneratedSecretRef is reference to the secret generated by backup bucket, which\rwill have object store specific credentials.\n\r\r\r\rBackupEntrySpec\r\r\r(Appears on:\rBackupEntry)\r\rBackupEntrySpec is the spec for an BackupEntry resource.\n\r\r\rField\rDescription\r\r\r\r\r\rDefaultSpec\r\rDefaultSpec\r\r\r\r\r\r(Members of DefaultSpec are embedded into this type.)\rDefaultSpec is a structure containing common fields used by all extension resources.\n\r\r\r\rbackupBucketProviderStatus\r\rk8s.io/apimachinery/pkg/runtime.RawExtension\r\r\r\r\r(Optional)\rBackupBucketProviderStatus contains the provider status that has\rbeen generated by the controller responsible for the BackupBucket resource.\n\r\r\r\rregion\r\rstring\r\r\r\rRegion is the region of this Entry.\n\r\r\r\rbucketName\r\rstring\r\r\r\rBucketName is the name of backup bucket for this Backup Entry.\n\r\r\r\rsecretRef\r\rKubernetes core/v1.SecretReference\r\r\r\r\rSecretRef is a reference to a secret that contains the credentials to access object store.\n\r\r\r\rBackupEntryStatus\r\r\r(Appears on:\rBackupEntry)\r\rBackupEntryStatus is the status for an BackupEntry resource.\n\r\r\rField\rDescription\r\r\r\r\r\rDefaultStatus\r\rDefaultStatus\r\r\r\r\r\r(Members of DefaultStatus are embedded into this type.)\rDefaultStatus is a structure containing common fields used by all extension resources.\n\r\r\r\rCRIConfig\r\r\r(Appears on:\rOperatingSystemConfigSpec)\r\rCRI config is a structure contains configurations of the CRI library\n\r\r\rField\rDescription\r\r\r\r\r\rname\r\rCRIName\r\r\r\r\rName is a mandatory string containing the name of the CRI library.\n\r\r\r\rCRIName\r(string alias)\n\r\r(Appears on:\rCRIConfig)\r\rCRIName is a type alias for the CRI name string.\nCloudConfig\r\r\r(Appears on:\rOperatingSystemConfigStatus)\r\rCloudConfig is a structure for containing the generated output for the given operating system\rconfig spec. It contains a reference to a secret as the result may contain confidential data.\n\r\r\rField\rDescription\r\r\r\r\r\rsecretRef\r\rKubernetes core/v1.SecretReference\r\r\r\r\rSecretRef is a reference to a secret that contains the actual result of the generated cloud config.\n\r\r\r\rClusterSpec\r\r\r(Appears on:\rCluster)\r\rClusterSpec is the spec for a Cluster resource.\n\r\r\rField\rDescription\r\r\r\r\r\rcloudProfile\r\rk8s.io/apimachinery/pkg/runtime.RawExtension\r\r\r\r\rCloudProfile is a raw extension field that contains the cloudprofile resource referenced\rby the shoot that has to be reconciled.\n\r\r\r\rseed\r\rk8s.io/apimachinery/pkg/runtime.RawExtension\r\r\r\r\rSeed is a raw extension field that contains the seed resource referenced by the shoot that\rhas to be reconciled.\n\r\r\r\rshoot\r\rk8s.io/apimachinery/pkg/runtime.RawExtension\r\r\r\r\rShoot is a raw extension field that contains the shoot resource that has to be reconciled.\n\r\r\r\rContainerRuntimeSpec\r\r\r(Appears on:\rContainerRuntime)\r\rContainerRuntimeSpec is the spec for a ContainerRuntime resource.\n\r\r\rField\rDescription\r\r\r\r\r\rbinaryPath\r\rstring\r\r\r\rBinaryPath is the Worker\u0026rsquo;s machine path where container runtime extensions should copy the binaries to.\n\r\r\r\rworkerPool\r\rContainerRuntimeWorkerPool\r\r\r\r\rWorkerPool identifies the worker pool of the Shoot.\rFor each worker pool and type, Gardener deploys a ContainerRuntime CRD.\n\r\r\r\rDefaultSpec\r\rDefaultSpec\r\r\r\r\r\r(Members of DefaultSpec are embedded into this type.)\rDefaultSpec is a structure containing common fields used by all extension resources.\n\r\r\r\rContainerRuntimeStatus\r\r\r(Appears on:\rContainerRuntime)\r\rContainerRuntimeStatus is the status for a ContainerRuntime resource.\n\r\r\rField\rDescription\r\r\r\r\r\rDefaultStatus\r\rDefaultStatus\r\r\r\r\r\r(Members of DefaultStatus are embedded into this type.)\rDefaultStatus is a structure containing common fields used by all extension resources.\n\r\r\r\rContainerRuntimeWorkerPool\r\r\r(Appears on:\rContainerRuntimeSpec)\r\r\r\r\rField\rDescription\r\r\r\r\r\rname\r\rstring\r\r\r\rName specifies the name of the worker pool the container runtime should be available for.\n\r\r\r\rselector\r\rKubernetes meta/v1.LabelSelector\r\r\r\r\rSelector is the label selector used by the extension to match the nodes belonging to the worker pool.\n\r\r\r\rControlPlaneSpec\r\r\r(Appears on:\rControlPlane)\r\rControlPlaneSpec is the spec of a ControlPlane resource.\n\r\r\rField\rDescription\r\r\r\r\r\rDefaultSpec\r\rDefaultSpec\r\r\r\r\r\r(Members of DefaultSpec are embedded into this type.)\rDefaultSpec is a structure containing common fields used by all extension resources.\n\r\r\r\rpurpose\r\rPurpose\r\r\r\r\r(Optional)\rPurpose contains the data if a cloud provider needs additional components in order to expose the control plane.\n\r\r\r\rinfrastructureProviderStatus\r\rk8s.io/apimachinery/pkg/runtime.RawExtension\r\r\r\r\r(Optional)\rInfrastructureProviderStatus contains the provider status that has\rbeen generated by the controller responsible for the Infrastructure resource.\n\r\r\r\rregion\r\rstring\r\r\r\rRegion is the region of this control plane.\n\r\r\r\rsecretRef\r\rKubernetes core/v1.SecretReference\r\r\r\r\rSecretRef is a reference to a secret that contains the cloud provider specific credentials.\n\r\r\r\rControlPlaneStatus\r\r\r(Appears on:\rControlPlane)\r\rControlPlaneStatus is the status of a ControlPlane resource.\n\r\r\rField\rDescription\r\r\r\r\r\rDefaultStatus\r\rDefaultStatus\r\r\r\r\r\r(Members of DefaultStatus are embedded into this type.)\rDefaultStatus is a structure containing common fields used by all extension resources.\n\r\r\r\rDefaultSpec\r\r\r(Appears on:\rBackupBucketSpec, BackupEntrySpec, ContainerRuntimeSpec, ControlPlaneSpec, ExtensionSpec, InfrastructureSpec, NetworkSpec, OperatingSystemConfigSpec, WorkerSpec)\r\rDefaultSpec contains common status fields for every extension resource.\n\r\r\rField\rDescription\r\r\r\r\r\rtype\r\rstring\r\r\r\rType contains the instance of the resource\u0026rsquo;s kind.\n\r\r\r\rproviderConfig\r\rk8s.io/apimachinery/pkg/runtime.RawExtension\r\r\r\r\r\r\r\r\rDefaultStatus\r\r\r(Appears on:\rBackupBucketStatus, BackupEntryStatus, ContainerRuntimeStatus, ControlPlaneStatus, ExtensionStatus, InfrastructureStatus, NetworkStatus, OperatingSystemConfigStatus, WorkerStatus)\r\rDefaultStatus contains common status fields for every extension resource.\n\r\r\rField\rDescription\r\r\r\r\r\rproviderStatus\r\rk8s.io/apimachinery/pkg/runtime.RawExtension\r\r\r\r\r(Optional)\rProviderStatus contains provider-specific status.\n\r\r\r\rconditions\r\r[]github.com/gardener/gardener/pkg/apis/core/v1beta1.Condition\r\r\r\r\r(Optional)\rConditions represents the latest available observations of a Seed\u0026rsquo;s current state.\n\r\r\r\rlastError\r\rgithub.com/gardener/gardener/pkg/apis/core/v1beta1.LastError\r\r\r\r\r(Optional)\rLastError holds information about the last occurred error during an operation.\n\r\r\r\rlastOperation\r\rgithub.com/gardener/gardener/pkg/apis/core/v1beta1.LastOperation\r\r\r\r\r(Optional)\rLastOperation holds information about the last operation on the resource.\n\r\r\r\robservedGeneration\r\rint64\r\r\r\rObservedGeneration is the most recent generation observed for this resource.\n\r\r\r\rstate\r\rk8s.io/apimachinery/pkg/runtime.RawExtension\r\r\r\r\r(Optional)\rState can be filled by the operating controller with what ever data it needs.\n\r\r\r\rDropIn\r\r\r(Appears on:\rUnit)\r\rDropIn is a drop-in configuration for a systemd unit.\n\r\r\rField\rDescription\r\r\r\r\r\rname\r\rstring\r\r\r\rName is the name of the drop-in.\n\r\r\r\rcontent\r\rstring\r\r\r\rContent is the content of the drop-in.\n\r\r\r\rExtensionSpec\r\r\r(Appears on:\rExtension)\r\rExtensionSpec is the spec for a Extension resource.\n\r\r\rField\rDescription\r\r\r\r\r\rDefaultSpec\r\rDefaultSpec\r\r\r\r\r\r(Members of DefaultSpec are embedded into this type.)\rDefaultSpec is a structure containing common fields used by all extension resources.\n\r\r\r\rExtensionStatus\r\r\r(Appears on:\rExtension)\r\rExtensionStatus is the status for a Extension resource.\n\r\r\rField\rDescription\r\r\r\r\r\rDefaultStatus\r\rDefaultStatus\r\r\r\r\r\r(Members of DefaultStatus are embedded into this type.)\rDefaultStatus is a structure containing common fields used by all extension resources.\n\r\r\r\rFile\r\r\r(Appears on:\rOperatingSystemConfigSpec)\r\rFile is a file that should get written to the host\u0026rsquo;s file system. The content can either be inlined or\rreferenced from a secret in the same namespace.\n\r\r\rField\rDescription\r\r\r\r\r\rpath\r\rstring\r\r\r\rPath is the path of the file system where the file should get written to.\n\r\r\r\rpermissions\r\rint32\r\r\r\r(Optional)\rPermissions describes with which permissions the file should get written to the file system.\rShould be defaulted to octal 0644.\n\r\r\r\rcontent\r\rFileContent\r\r\r\r\rContent describe the file\u0026rsquo;s content.\n\r\r\r\rFileContent\r\r\r(Appears on:\rFile)\r\rFileContent can either reference a secret or contain inline configuration.\n\r\r\rField\rDescription\r\r\r\r\r\rsecretRef\r\rFileContentSecretRef\r\r\r\r\r(Optional)\rSecretRef is a struct that contains information about the referenced secret.\n\r\r\r\rinline\r\rFileContentInline\r\r\r\r\r(Optional)\rInline is a struct that contains information about the inlined data.\n\r\r\r\rFileContentInline\r\r\r(Appears on:\rFileContent)\r\rFileContentInline contains keys for inlining a file content\u0026rsquo;s data and encoding.\n\r\r\rField\rDescription\r\r\r\r\r\rencoding\r\rstring\r\r\r\rEncoding is the file\u0026rsquo;s encoding (e.g. base64).\n\r\r\r\rdata\r\rstring\r\r\r\rData is the file\u0026rsquo;s data.\n\r\r\r\rFileContentSecretRef\r\r\r(Appears on:\rFileContent)\r\rFileContentSecretRef contains keys for referencing a file content\u0026rsquo;s data from a secret in the same namespace.\n\r\r\rField\rDescription\r\r\r\r\r\rname\r\rstring\r\r\r\rName is the name of the secret.\n\r\r\r\rdataKey\r\rstring\r\r\r\rDataKey is the key in the secret\u0026rsquo;s .data field that should be read.\n\r\r\r\rInfrastructureSpec\r\r\r(Appears on:\rInfrastructure)\r\rInfrastructureSpec is the spec for an Infrastructure resource.\n\r\r\rField\rDescription\r\r\r\r\r\rDefaultSpec\r\rDefaultSpec\r\r\r\r\r\r(Members of DefaultSpec are embedded into this type.)\rDefaultSpec is a structure containing common fields used by all extension resources.\n\r\r\r\rregion\r\rstring\r\r\r\rRegion is the region of this infrastructure.\n\r\r\r\rsecretRef\r\rKubernetes core/v1.SecretReference\r\r\r\r\rSecretRef is a reference to a secret that contains the actual result of the generated cloud config.\n\r\r\r\rsshPublicKey\r\r[]byte\r\r\r\r(Optional)\rSSHPublicKey is the public SSH key that should be used with this infrastructure.\n\r\r\r\rInfrastructureStatus\r\r\r(Appears on:\rInfrastructure)\r\rInfrastructureStatus is the status for an Infrastructure resource.\n\r\r\rField\rDescription\r\r\r\r\r\rDefaultStatus\r\rDefaultStatus\r\r\r\r\r\r(Members of DefaultStatus are embedded into this type.)\rDefaultStatus is a structure containing common fields used by all extension resources.\n\r\r\r\rnodesCIDR\r\rstring\r\r\r\r(Optional)\rNodesCIDR is the CIDR of the node network that was optionally created by the acting extension controller.\rThis might be needed in environments in which the CIDR for the network for the shoot worker node cannot\rbe statically defined in the Shoot resource but must be computed dynamically.\n\r\r\r\rMachineDeployment\r\r\r(Appears on:\rWorkerStatus)\r\rMachineDeployment is a created machine deployment.\n\r\r\rField\rDescription\r\r\r\r\r\rname\r\rstring\r\r\r\rName is the name of the MachineDeployment resource.\n\r\r\r\rminimum\r\rint32\r\r\r\rMinimum is the minimum number for this machine deployment.\n\r\r\r\rmaximum\r\rint32\r\r\r\rMaximum is the maximum number for this machine deployment.\n\r\r\r\rMachineImage\r\r\r(Appears on:\rWorkerPool)\r\rMachineImage contains logical information about the name and the version of the machie image that\rshould be used. The logical information must be mapped to the provider-specific information (e.g.,\rAMIs, \u0026hellip;) by the provider itself.\n\r\r\rField\rDescription\r\r\r\r\r\rname\r\rstring\r\r\r\rName is the logical name of the machine image.\n\r\r\r\rversion\r\rstring\r\r\r\rVersion is the version of the machine image.\n\r\r\r\rNetworkSpec\r\r\r(Appears on:\rNetwork)\r\rNetworkSpec is the spec for an Network resource.\n\r\r\rField\rDescription\r\r\r\r\r\rDefaultSpec\r\rDefaultSpec\r\r\r\r\r\r(Members of DefaultSpec are embedded into this type.)\rDefaultSpec is a structure containing common fields used by all extension resources.\n\r\r\r\rpodCIDR\r\rstring\r\r\r\rPodCIDR defines the CIDR that will be used for pods.\n\r\r\r\rserviceCIDR\r\rstring\r\r\r\rServiceCIDR defines the CIDR that will be used for services.\n\r\r\r\rNetworkStatus\r\r\r(Appears on:\rNetwork)\r\rNetworkStatus is the status for an Network resource.\n\r\r\rField\rDescription\r\r\r\r\r\rDefaultStatus\r\rDefaultStatus\r\r\r\r\r\r(Members of DefaultStatus are embedded into this type.)\rDefaultStatus is a structure containing common fields used by all extension resources.\n\r\r\r\rObject\r\r\rObject is an extension object resource.\nOperatingSystemConfigPurpose\r(string alias)\n\r\r(Appears on:\rOperatingSystemConfigSpec)\r\rOperatingSystemConfigPurpose is a string alias.\nOperatingSystemConfigSpec\r\r\r(Appears on:\rOperatingSystemConfig)\r\rOperatingSystemConfigSpec is the spec for a OperatingSystemConfig resource.\n\r\r\rField\rDescription\r\r\r\r\r\rcriConfig\r\rCRIConfig\r\r\r\r\r(Optional)\rCRI config is a structure contains configurations of the CRI library\n\r\r\r\rDefaultSpec\r\rDefaultSpec\r\r\r\r\r\r(Members of DefaultSpec are embedded into this type.)\rDefaultSpec is a structure containing common fields used by all extension resources.\n\r\r\r\rpurpose\r\rOperatingSystemConfigPurpose\r\r\r\r\rPurpose describes how the result of this OperatingSystemConfig is used by Gardener. Either it\rgets sent to the Worker extension controller to bootstrap a VM, or it is downloaded by the\rcloud-config-downloader script already running on a bootstrapped VM.\n\r\r\r\rreloadConfigFilePath\r\rstring\r\r\r\r(Optional)\rReloadConfigFilePath is the path to the generated operating system configuration. If set, controllers\rare asked to use it when determining the .status.command of this resource. For example, if for CoreOS\rthe reload-path might be \u0026ldquo;/var/lib/config\u0026rdquo;; then the controller shall set .status.command to\r\u0026ldquo;/usr/bin/coreos-cloudinit \u0026ndash;from-file=/var/lib/config\u0026rdquo;.\n\r\r\r\runits\r\r[]Unit\r\r\r\r\r(Optional)\rUnits is a list of unit for the operating system configuration (usually, a systemd unit).\n\r\r\r\rfiles\r\r[]File\r\r\r\r\r(Optional)\rFiles is a list of files that should get written to the host\u0026rsquo;s file system.\n\r\r\r\rOperatingSystemConfigStatus\r\r\r(Appears on:\rOperatingSystemConfig)\r\rOperatingSystemConfigStatus is the status for a OperatingSystemConfig resource.\n\r\r\rField\rDescription\r\r\r\r\r\rDefaultStatus\r\rDefaultStatus\r\r\r\r\r\r(Members of DefaultStatus are embedded into this type.)\rDefaultStatus is a structure containing common fields used by all extension resources.\n\r\r\r\rcloudConfig\r\rCloudConfig\r\r\r\r\r(Optional)\rCloudConfig is a structure for containing the generated output for the given operating system\rconfig spec. It contains a reference to a secret as the result may contain confidential data.\n\r\r\r\rcommand\r\rstring\r\r\r\r(Optional)\rCommand is the command whose execution renews/reloads the cloud config on an existing VM, e.g.\r\u0026ldquo;/usr/bin/reload-cloud-config -from-file=\u0026rdquo;. The  is optionally provided by Gardener\rin the .spec.reloadConfigFilePath field.\n\r\r\r\runits\r\r[]string\r\r\r\r(Optional)\rUnits is a list of systemd unit names that are part of the generated Cloud Config and shall be\rrestarted when a new version has been downloaded.\n\r\r\r\rPurpose\r(string alias)\n\r\r(Appears on:\rControlPlaneSpec)\r\rPurpose is a string alias.\nSpec\r\r\rSpec is the spec section of an Object.\nStatus\r\r\rStatus is the status of an Object.\nUnit\r\r\r(Appears on:\rOperatingSystemConfigSpec)\r\rUnit is a unit for the operating system configuration (usually, a systemd unit).\n\r\r\rField\rDescription\r\r\r\r\r\rname\r\rstring\r\r\r\rName is the name of a unit.\n\r\r\r\rcommand\r\rstring\r\r\r\r(Optional)\rCommand is the unit\u0026rsquo;s command.\n\r\r\r\renable\r\rbool\r\r\r\r(Optional)\rEnable describes whether the unit is enabled or not.\n\r\r\r\rcontent\r\rstring\r\r\r\r(Optional)\rContent is the unit\u0026rsquo;s content.\n\r\r\r\rdropIns\r\r[]DropIn\r\r\r\r\r(Optional)\rDropIns is a list of drop-ins for this unit.\n\r\r\r\rVolume\r\r\r(Appears on:\rWorkerPool)\r\rVolume contains information about the root disks that should be used for worker pools.\n\r\r\rField\rDescription\r\r\r\r\r\rname\r\rstring\r\r\r\r(Optional)\rName of the volume to make it referencable.\n\r\r\r\rtype\r\rstring\r\r\r\r(Optional)\rType is the type of the volume.\n\r\r\r\rsize\r\rstring\r\r\r\rSize is the of the root volume.\n\r\r\r\rencrypted\r\rbool\r\r\r\r(Optional)\rEncrypted determines if the volume should be encrypted.\n\r\r\r\rWorkerPool\r\r\r(Appears on:\rWorkerSpec)\r\rWorkerPool is the definition of a specific worker pool.\n\r\r\rField\rDescription\r\r\r\r\r\rmachineType\r\rstring\r\r\r\rMachineType contains information about the machine type that should be used for this worker pool.\n\r\r\r\rmaximum\r\rint32\r\r\r\rMaximum is the maximum size of the worker pool.\n\r\r\r\rmaxSurge\r\rk8s.io/apimachinery/pkg/util/intstr.IntOrString\r\r\r\r\rMaxSurge is maximum number of VMs that are created during an update.\n\r\r\r\rmaxUnavailable\r\rk8s.io/apimachinery/pkg/util/intstr.IntOrString\r\r\r\r\rMaxUnavailable is the maximum number of VMs that can be unavailable during an update.\n\r\r\r\rannotations\r\rmap[string]string\r\r\r\r(Optional)\rAnnotations is a map of key/value pairs for annotations for all the Node objects in this worker pool.\n\r\r\r\rlabels\r\rmap[string]string\r\r\r\r(Optional)\rLabels is a map of key/value pairs for labels for all the Node objects in this worker pool.\n\r\r\r\rtaints\r\r[]Kubernetes core/v1.Taint\r\r\r\r\r(Optional)\rTaints is a list of taints for all the Node objects in this worker pool.\n\r\r\r\rmachineImage\r\rMachineImage\r\r\r\r\rMachineImage contains logical information about the name and the version of the machie image that\rshould be used. The logical information must be mapped to the provider-specific information (e.g.,\rAMIs, \u0026hellip;) by the provider itself.\n\r\r\r\rminimum\r\rint32\r\r\r\rMinimum is the minimum size of the worker pool.\n\r\r\r\rname\r\rstring\r\r\r\rName is the name of this worker pool.\n\r\r\r\rproviderConfig\r\rk8s.io/apimachinery/pkg/runtime.RawExtension\r\r\r\r\r(Optional)\rProviderConfig is a provider specific configuration for the worker pool.\n\r\r\r\ruserData\r\r[]byte\r\r\r\rUserData is a base64-encoded string that contains the data that is sent to the provider\u0026rsquo;s APIs\rwhen a new machine/VM that is part of this worker pool shall be spawned.\n\r\r\r\rvolume\r\rVolume\r\r\r\r\r(Optional)\rVolume contains information about the root disks that should be used for this worker pool.\n\r\r\r\rdataVolumes\r\r[]Volume\r\r\r\r\r(Optional)\rDataVolumes contains a list of additional worker volumes.\n\r\r\r\rkubeletDataVolumeName\r\rstring\r\r\r\r(Optional)\rKubeletDataVolumeName contains the name of a dataVolume that should be used for storing kubelet state.\n\r\r\r\rzones\r\r[]string\r\r\r\r(Optional)\rZones contains information about availability zones for this worker pool.\n\r\r\r\rWorkerSpec\r\r\r(Appears on:\rWorker)\r\rWorkerSpec is the spec for a Worker resource.\n\r\r\rField\rDescription\r\r\r\r\r\rDefaultSpec\r\rDefaultSpec\r\r\r\r\r\r(Members of DefaultSpec are embedded into this type.)\rDefaultSpec is a structure containing common fields used by all extension resources.\n\r\r\r\rinfrastructureProviderStatus\r\rk8s.io/apimachinery/pkg/runtime.RawExtension\r\r\r\r\r(Optional)\rInfrastructureProviderStatus is a raw extension field that contains the provider status that has\rbeen generated by the controller responsible for the Infrastructure resource.\n\r\r\r\rregion\r\rstring\r\r\r\rRegion is the name of the region where the worker pool should be deployed to.\n\r\r\r\rsecretRef\r\rKubernetes core/v1.SecretReference\r\r\r\r\rSecretRef is a reference to a secret that contains the cloud provider specific credentials.\n\r\r\r\rsshPublicKey\r\r[]byte\r\r\r\r(Optional)\rSSHPublicKey is the public SSH key that should be used with these workers.\n\r\r\r\rpools\r\r[]WorkerPool\r\r\r\r\rPools is a list of worker pools.\n\r\r\r\rWorkerStatus\r\r\r(Appears on:\rWorker)\r\rWorkerStatus is the status for a Worker resource.\n\r\r\rField\rDescription\r\r\r\r\r\rDefaultStatus\r\rDefaultStatus\r\r\r\r\r\r(Members of DefaultStatus are embedded into this type.)\rDefaultStatus is a structure containing common fields used by all extension resources.\n\r\r\r\rmachineDeployments\r\r[]MachineDeployment\r\r\r\r\rMachineDeployments is a list of created machine deployments. It will be used to e.g. configure\rthe cluster-autoscaler properly.\n\r\r\r\r\r"
},
{
	"uri": "https://gardener.cloud/blog/2019_week_21_2/",
	"title": "Feature Flags in Kubernetes Applications",
	"tags": [],
	"description": "",
	"content": "Feature flags are used to change the behavior of a program at runtime without forcing a restart.\nAlthough they are essential in a native cloud environment, they cannot be implemented without significant effort on some platforms. Kubernetes has made this trivial. Here we will implement them through labels and annotations, but you can also implement them by connecting directly to the Kubernetes API Server.\nPossible Use Cases\n turn on/off a specific instance turn on/off profiling of a specific instance change the logging level, to capture detailed logs during a specific event change caching strategy at runtime change timeouts in production toggle on/off some special verification  ..read some more on Feature Flags for App.\n"
},
{
	"uri": "https://gardener.cloud/documentation/050-tutorials/content/app/featureflag/",
	"title": "Feature Flags in Kubernetes Applications",
	"tags": [],
	"description": "Feature Flags in Kubernetes Applications",
	"content": "Feature Flags in Kubernetes Applications Feature flags are used to change the behavior of a program at runtime without forcing a restart.\nAlthough they are essential in a native cloud environment, they cannot be implemented without significant effort on some platforms. Kubernetes has made this trivial. Here we will implement them through labels and annotations, but you can also implement them by connecting directly to the Kubernetes API Server.\nIn Kubernetes, labels are part of the identity of a resource and can be used through selectors. Annotations are similar, but do not participate in the identity of a resource and cannot be used to select resources. Nevertheless, they can still be used as feature flags to enable/disable application logic.\nPossible Use Cases  turn on/off a specific instance turn on/off profiling of a specific instance change the logging level, to capture detailed logs during a specific event change caching strategy at runtime change timeouts in production toggle on/off some special verification  How does this work We’ll use the Kubernetes downwardAPI ) to expose labels and annotations directly to our application. We’ll end up with two files (labels and annotations) in /etc/podinfo. First we add the downward api to spec.volumes. Note that it is possible to adding both labels and annotations into the same volume.\nHow to update/toggle the feature After the deployment of the demo application is done you can easily switch a feature in the application on or off. This is done very easily with kubectl by changing an annotation in the Pods.\nDeploy demo app/pod kubectl apply -f ./yaml/deployment.yaml\rShow the log kubectl logs featureflag-example -f\rUse business feature 2 kubectl annotate --overwrite pod featureflag-example businessFeature=implementation2\rUse business feature 1 kubectl annotate --overwrite pod featureflag-example businessFeature=implementation1\rConclusion As you can see in the log of the Pod the application switches very fast between the implementations. Everything was controlled by annotations on the deployment or Pod. On the whole a very simple and maintainable solution to configure parts of the application without restarting the whole application.\nWrangling labels and annotations from the shell. # Add a label\r$ kubectl label pod my-pod-name a-label=foo\r# Show labels\r$ kubectl get pods --show-labels\r# If you only want to show specific labels, use -L=\u0026lt;label1\u0026gt;,\u0026lt;label2\u0026gt;\r# Update a label\r$ kubectl label pod my-pod-name a-label=bar --override\r# Delete a label .Remember the \u0026#34;-\u0026#34; at the end of the line. Required to remove a label\r$ kubectl label pod my-pod-name a-label-\r# Add an annotation\r$ kubectl annotatate pod my-pod-name an-annotation=foo\r# Show annotations\r$ kubectl describe pod my-pod-name\r# Update an annotation\r$ kubectl annotation pod my-pod-name an-annotation=foo --override\r# Delete an annotation. Remember the \u0026#34;-\u0026#34; at the end of the line. Required to remove a annotation\r$ kubectl annotation pod my-pod-name an-annotation-\r"
},
{
	"uri": "https://gardener.cloud/blog/2018_week_17/",
	"title": "Frontend HTTPS",
	"tags": [],
	"description": "",
	"content": "\u0026lt;img src=\u0026quot;../blog/2018_week_17/blog-https.png\u0026quot; title=\u0026quot;logo\u0026quot; width=\u0026quot;100%\u0026quot; class=\u0026quot;drop-shadow reveal-fast\u0026quot; style=\u0026quot;\u0026quot;/\u0026gt;\r For encrypted communication between the client to the load balancer, you need to specify a TLS private key and certificate to be used by the ingress controller.\nCreate a secret in the namespace of the ingress containing the TLS private key and certificate. Then configure the secret name in the TLS configuration section of the ingress specification.\n..read on HTTPS - Self Signed Certificates how to configure it.\n"
},
{
	"uri": "https://gardener.cloud/components/gardenctl/",
	"title": "gardenctl",
	"tags": [],
	"description": "",
	"content": "Gardenctl  \nWhat is gardenctl? gardenctl is a command-line client for administrative purposes for the Gardener. It facilitates the administration of one or many garden, seed and shoot clusters, e.g. to check for issues which occured in one of these clusters. Details about the concept behind the Gardener are described in the Gardener wiki.\nInstallation gardenctl is shipped for mac and linux in a binary format.\n Download the latest release:  curl -LO https://github.com/gardener/gardenctl/releases/download/$(curl -s https://raw.githubusercontent.com/gardener/gardenctl/master/LATEST)/gardenctl-darwin-amd64\rTo download a specific version, replace the $(curl -s https://raw.githubusercontent.com/gardener/gardenctl/master/LATEST) portion of the command with the specific version.\nFor example, to download version 0.16.0 on macOS, type:\ncurl -LO https://github.com/gardener/gardenctl/releases/download/v0.16.0/gardenctl-darwin-amd64\rMake the gardenctl binary executable.  chmod +x ./gardenctl-darwin-amd64\rMove the binary in to your PATH.  sudo mv ./gardenctl-darwin-amd64 /usr/local/bin/gardenctl\rHow to build it If no binary builds are available for your platform or architecture, you can build it from source, go get it or build the docker image from Dockerfile. Please keep in mind to use an up to date version of golang.\nPrerequisites To build gardenctl from sources you need to have a running Golang environment. Moreover, since gardenctl allows to execute kubectl as well as a running kubectl installation is recommended, but not required. Please check this description for further details.\nBuild gardenctl From source First, you need to clone the repository and build gardenctl.\ngit clone //documentation/030-architecture/15_gardenctl/\rcd gardenctl\rmake build\rAfter successfully building gardenctl the executables are in the directory ~/go/src/github.com/gardener/gardenctl/bin/. Next, move the executable for your architecture to /usr/local/bin. In this case for darwin-amd64.\nsudo mv bin/darwin-amd64/gardenctl-darwin-amd64 /usr/local/bin/gardenctl\rgardenctl supports auto completion. This recommended feature is bound to gardenctl or the alias g. To configure it you can run:\necho \u0026#34;source \u0026lt;(gardenctl completion bash)\u0026#34; \u0026gt;\u0026gt; ~/.bashrc\rsource ~/.bashrc\rVia Dockerfile First clone the repository as described in the the build step \u0026ldquo;From source\u0026rdquo;. As next step add the garden \u0026ldquo;config\u0026rdquo; file and \u0026ldquo;clusters\u0026rdquo; folder with the corresponding kubeconfig files for the garden cluster. Then build the container image via docker build -t gardener/gardenctl:v1 . in the cloned repository and run a shell in the image with docker run -it gardener/gardenctl:v1 /bin/bash.\nConfigure gardenctl gardenctl requires a configuration file. The default location is in ~/.garden/config, but it can be overwritten with the environment variable GARDENCONFIG.\nHere an example file:\nemail: john.doe@example.com\rgithubURL: https://github.location.company.corp\rgardenClusters:\r- name: dev\rkubeConfig: ~/clusters/dev/kubeconfig.yaml\r- name: prod\rkubeConfig: ~/clusters/prod/kubeconfig.yaml\rThe path to the kubeconfig files of a garden cluster can be relative by using the ~ (tilde) expansion or absolute.\ngardenctl caches some information, e.g. the garden project names. The location of this cache is per default $GARDENCTL_HOME/cache. If GARDENCTL_HOME is not set, ~/.garden is assumed.\ngardenctl supports multiple sessions. The session ID can be set via $GARDEN_SESSION_ID and the sessions are stored under $GARDENCTL_HOME/sessions.\ngardenctl makes it easy to get additional information of your IaaS provider by using the secrets stored in the corresponding projects in the Gardener. To use this functionality, the CLIs of the IaaS providers need to be available.\nPlease check the IaaS provider documentation for more details about their CLIs.\n aliyun aws az gcloud openstack  Moreover, gardenctl offers auto completion. To use it, the command\ngardenctl completion bash\rprint on the standard output a completion script which can be sourced via\nsource \u0026lt;(gardenctl completion bash)\rPlease keep in mind that the auto completion is bound to gardenctl or the alias g.\nUse gardenctl gardenctl requires the definition of a target, e.g. garden, project, seed or shoot. The following commands, e.g. gardenctl ls shoots uses the target definition as a context for getting the information.\nTargets represent a hierarchical structure of resources. On top, there is/are the garden/s. E.g. in case you setup a development and a production garden, you would have two entries in your ~/.garden/config. Via gardenctl ls gardens you get a list of the available gardens.\n gardenctl get target\nDisplays the current target gardenctl target [garden|project|seed|shoot]\nSet the target e.g. to a garden. It is as well possible to set the target directly to a element deeper in the hierarchy, e.g. to a shoot. gardenctl drop target\nDrop the deepest target.  Examples of basic usage:  List all seed cluster\ngardenctl ls seeds List all projects with shoot cluster\ngardenctl ls projects Target a seed cluster\ngardenctl target seed-gce-dev Target a project\ngardenctl target garden-vora Open prometheus ui for a targeted shoot-cluster\ngardenctl show prometheus Execute an aws command on a targeted aws shoot cluster\ngardenctl aws ec2 describe-instances or\ngardenctl aws ec2 describe-instances --no-cache without locally caching credentials Target a shoot directly and get all kube-dns pods in kube-system namespace\ngardenctl target myshoot\ngardenctl kubectl get pods -- -n kube-system -l k8s-app=kube-dns List all cluster with an issue\ngardenctl ls issues Drop an element from target stack\ngardenctl drop Open a shell to a cluster node\ngardenctl shell nodename Show logs from elasticsearch\ngardenctl logs etcd-main --elasticsearch Show last 100 logs from elasticsearch from the last 2 hours\ngardenctl logs etcd-main --elasticsearch --since=2h --tail=100 Show logs from seed nodes\ngardenctl target -g garden-name -s seed-name\ngardenctl logs tf infra shoot-name Show logs from shoot nodes\ngardenctl target -g garden-name -t shoot-name\ngardenctl logs api | scheduler | controller-manager | etcd-main -c etcd |etcd-main -c backup-restore | vpn-seed | vpn-shoot | machine-controller-manager | prometheus |grafana | alertmanager | cluster-autoscaler Show logs from garden nodes\ngardenctl target -g garden-name\ngardenctl logs gardener-apiserver | gardener-controller-manager  Advanced usage based on JsonQuery The following examples are based on jq. The Json Query Playground offers a convenient environment to test the queries.\nBelow a list of examples:\n List the project name, shoot name and the state for all projects with issues  gardenctl ls issues -o json | jq \u0026#39;.issues[] | { project: .project, shoot: .shoot, state: .status.lastOperation.state }\u0026#39;\r Print all issues of a single project e.g. garden-myproject  gardenctl ls issues -o json | jq \u0026#39;.issues[] | if (.project==\u0026#34;garden-myproject\u0026#34;) then . else empty end\u0026#39;  Print all issues with error state \u0026ldquo;Error\u0026rdquo;  gardenctl ls issues -o json | jq \u0026#39;.issues[] | if (.status.lastOperation.state==\u0026#34;Error\u0026#34;) then . else empty end\u0026#39;\r Print all issues with error state not equal \u0026ldquo;Succeded\u0026rdquo;  gardenctl ls issues -o json | jq \u0026#39;.issues[] | if (.status.lastOperation.state!=\u0026#34;Succeeded\u0026#34;) then . else empty end\u0026#39;\r Print createdBy information (typically email addresses) of all shoots  gardenctl k get shoots -- -n garden-core -o json | jq -r \u0026#34;.items[].metadata | {email: .annotations.\\\u0026#34;garden.sapcloud.io/createdBy\\\u0026#34;, name: .name, namespace: .namespace}\u0026#34;\rHere a few on cluster analysis:\n Which states are there and how many clusters are in this state?  gardenctl ls issues -o json | jq \u0026#39;.issues | group_by( .status.lastOperation.state ) | .[] | {state:.[0].status.lastOperation.state, count:length}\u0026#39;\r Get all clusters in state Failed  gardenctl ls issues -o json | jq \u0026#39;.issues[] | if (.status.lastOperation.state==\u0026#34;Failed\u0026#34;) then . else empty end\u0026#39;\r"
},
{
	"uri": "https://gardener.cloud/components/gardener/",
	"title": "Gardener",
	"tags": [],
	"description": "",
	"content": "Gardener     \nGardener implements the automated management and operation of Kubernetes clusters as a service and provides extensibility concepts which allow support for any cloud or infrastructure provider. Its main principle is to leverage Kubernetes concepts for all of its tasks.\nIn essence, Gardener is an extension API server that comes along with a bundle of custom controllers. It introduces new API objects in an existing Kubernetes cluster (which is called garden cluster) in order to use them for the management of end-user Kubernetes clusters (which are called shoot clusters). These shoot clusters are described via declarative cluster specifications which are observed by the controllers. They will bring up the clusters, reconcile their state, perform automated updates and make sure they are always up and running.\nTo accomplish these tasks reliably and to offer a certain quality of service, Gardener controls the main components of a Kubernetes cluster (etcd, API server, controller manager, scheduler). These so-called control plane components are hosted in Kubernetes clusters themselves (which are called seed clusters). This is the main difference compared to many other OSS cluster provisioning tools: The shoot clusters do not have dedicated master VMs. Instead, the control plane is deployed as a native Kubernetes workload into the seeds. This does not only effectively reduce the total cost of ownership but also allows easier implementations for \u0026ldquo;day-2 operations\u0026rdquo; (like cluster updates or robustness) by relying on all the mature Kubernetes features and capabilities.\nPlease find more information regarding the concepts and a detailed description of the architecture in our Gardener Wiki and our blog post on kubernetes.io.\n K8s Conformance Test Coverage Conformance test results of latest stable Gardener release, transparently visible at the CNCF test grid:\n   Provider/K8s v1.18 v1.17 v1.16 v1.15 v1.14 v1.13 v1.12 v1.11 v1.10     AWS            Azure            GCP            OpenStack            Alicloud       N/A N/A N/A   Packet N/A N/A N/A N/A N/A N/A N/A N/A N/A    Besides the conformance tests, over 400 additional e2e tests are executed on a daily basis. Get an overview of the test results at testgrid.\nStart using or developing the Gardener locally See our documentation in the /docs repository, please find the index here.\nSetting up your own Gardener landscape in the Cloud The quickest way to test drive Gardener is to install it virtually onto an existing Kubernetes cluster, just like you would install any other Kubernetes-ready application. Launch your automatic installer here\nWe also have a Gardener Helm Chart. Alternatively you can use our garden setup project to create a fully configured Gardener landscape which also includes our Gardener Dashboard.\nFeedback and Support Feedback and contributions are always welcome!\nAll channels for getting in touch or learning about our project are listed under the community section. We are cordially inviting interested parties to join our weekly meetings.\nPlease report bugs or suggestions about our Kubernetes clusters as such or the Gardener itself as GitHub issues or join our Slack channel #gardener (please invite yourself to the Kubernetes workspace here).\nLearn More! Please find further resources about our project here:\n Our landing page gardener.cloud \u0026ldquo;Gardener, the Kubernetes Botanist\u0026rdquo; blog on kubernetes.io SAP news article about \u0026ldquo;Project Gardener\u0026rdquo; Introduction movie: \u0026ldquo;Gardener - Planting the Seeds of Success in the Cloud\u0026rdquo; \u0026ldquo;Thinking Cloud Native\u0026rdquo; talk at EclipseCon 2018 Blog - \u0026ldquo;Showcase of Gardener at OSCON 2018\u0026rdquo;  "
},
{
	"uri": "https://gardener.cloud/documentation/050-tutorials/content/howto/gardener_certificate_management/",
	"title": "Gardener Certificate Management",
	"tags": [],
	"description": "Configure Certificate Management For Shoot Clusters",
	"content": "Gardener Certificate Management Introduction Gardener comes with an extension that enables shoot owners to request X.509 compliant certificates for shoot domains.\nExtension Installation The Shoot-Cert-Service extension can be deployed and configured via Gardener's native resource ControllerRegistration.\nPrerequisites To let the Shoot-Cert-Service operate properly, you need to have:\n a DNS service in your seed contact details and optionally a private key for a pre-existing Let's Encrypt account  ControllerRegistration An example of a ControllerRegistration for the Shoot-Cert-Service can be found here: https://github.com/gardener/gardener-extensions/blob/master/controllers/extension-shoot-cert-service/example/controller-registration.yaml\nConfiguration The ControllerRegistration contains a Helm chart which eventually deploy the Shoot-Cert-Service to seed clusters. It offers some configuration options, mainly to set up a default issuer for shoot clusters. With a default issuer, pre-existing Let's Encrypt accounts can be used and shared with shoot clusters (See \u0026ldquo;One Account or Many?\u0026rdquo; of the Integration Guide).\n Please keep the Let's Encrypt Rate Limits in mind when using this shared account model. Depending on the amount of shoots and domains it is recommended to use an account with increased rate limits.\n apiVersion: core.gardener.cloud/v1beta1\rkind: ControllerRegistration\r...\rvalues:\rcertificateConfig:\rdefaultIssuer:\racme:\remail: foo@example.com\rprivateKey: |-\r-----BEGIN RSA PRIVATE KEY-----\r ...\r-----END RSA PRIVATE KEY-----\rserver: https://acme-v02.api.letsencrypt.org/directory\r name: default-issuer\rIf the Shoot-Cert-Service should be enabled for every shoot cluster in your Gardener managed environment, you need to globally enable it in the ControllerRegistration:\napiVersion: core.gardener.cloud/v1beta1\rkind: ControllerRegistration\r...\rresources:\r- globallyEnabled: true\rkind: Extension\rtype: shoot-cert-service\rAlternatively, you're given the option to only enable the service for certain shoots:\nkind: Shoot\rapiVersion: core.gardener.cloud/v1beta1\r...\rspec:\rextensions:\r- type: shoot-cert-service\r...\r\r#body-inner blockquote {\rborder: 0;\rpadding: 10px;\rmargin-top: 40px;\rmargin-bottom: 40px;\rborder-radius: 4px;\rbackground-color: rgba(0,0,0,0.05);\rbox-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);\rposition:relative;\rpadding-left:60px;\r}\r#body-inner blockquote:before {\rcontent: \"!\";\rfont-weight: bold;\rposition: absolute;\rtop: 0;\rbottom: 0;\rleft: 0;\rbackground-color: #00a273;\rcolor: white;\rvertical-align: middle;\rmargin: auto;\rwidth: 36px;\rfont-size: 30px;\rtext-align: center;\r}\r\r"
},
{
	"uri": "https://gardener.cloud/documentation/050-tutorials/content/howto/gardener-cookies/",
	"title": "Gardener Cookies",
	"tags": [],
	"description": "Gardener Cookies",
	"content": "Green Tea Matcha Cookies For a team event during the Christmas season we decided to completely reinterpret the topic cookies. :-)\n\r\r\r\r\r\r\r\r\r\r\r\r.sh__item img {\robject-fit: cover !important;\r}\r\rMatcha cookies have the delicate flavor and color of green tea. These soft, pillowy and chewy green tea cookies are perfect with tea. And of course they fit perfectly to our logo\nIngredients  1 stick butter, softened ⅞ cup of granulated sugar 1 cup + 2 tablespoons all-purpose flour 2 eggs 1¼ tablespoons culinary grade matcha powder 1 teaspoon baking powder Pinch of salt  Instructions  Cream together the butter and sugar in a large mixing bowl - it should be creamy colored and airy. A hand blender or stand mixer works well for this. This helps the cookie become fluffy and chewy. Gently incorporate the eggs to the butter mixture one at a time. In a separate bowl, sift together all the dry ingredients. Add the dry ingredients to the wet by adding a little at a time and folding or gently mixing the batter together. Keep going until you've incorporated all the remaining flour mixture. The dough should be a beautiful green color. Chill the dough for at least an hour - up to overnight. The longer the better! Preheat your oven to 325 F. Roll the dough into balls the size of ping pong balls and place them on a non-stick cookie sheet. Bake them for 12-15 minutes until the bottoms just start to become golden brown and the cookie no longer looks wet in the middle. Note: you can always bake them at 350 F for a less moist, fluffy cookie. It will bake faster by about 2-4 minutes 350 F so watch them closely. Remove and let cool on a rack and enjoy!  Note Make sure you get culinary grade matcha powder. You should be able to find this in Asian or natural grocers\n"
},
{
	"uri": "https://gardener.cloud/documentation/050-tutorials/content/howto/gardener_dns_management/",
	"title": "Gardener DNS Management for Shoots",
	"tags": [],
	"description": "Configure DNS Management For Shoot Clusters",
	"content": "Gardener DNS Management for Shoots Introduction Gardener allows Shoot clusters to request DNS names for Ingresses and Services out of the box. To support this the gardener must be installed with the shoot-dns-service extension. This extension uses the seed's dns management infrastructure to maintain DNS names for shoot clusters. So, far only the external DNS domain of a shoot (already used for the kubernetes api server and ingress DNS names) can be used for managed DNS names.\n\r#body-inner blockquote {\rborder: 0;\rpadding: 10px;\rmargin-top: 40px;\rmargin-bottom: 40px;\rborder-radius: 4px;\rbackground-color: rgba(0,0,0,0.05);\rbox-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);\rposition:relative;\rpadding-left:60px;\r}\r#body-inner blockquote:before {\rcontent: \"!\";\rfont-weight: bold;\rposition: absolute;\rtop: 0;\rbottom: 0;\rleft: 0;\rbackground-color: #00a273;\rcolor: white;\rvertical-align: middle;\rmargin: auto;\rwidth: 36px;\rfont-size: 30px;\rtext-align: center;\r}\r\rConfiguration A general description for configuring the DNS management of the gardener can be found here.\nTo generally enable the DNS management for shoot objects the shoot-dns-service extension must be registered by providing an appropriate extension registration in the garden cluster.\nHere it is possible to decide whether the extension should be always available for all shoots or whether the extension must be separately enabled per shoot.\nIf the extension should be used for all shoots the registration must set the globallyEnabled flag to true.\nspec:\rresources:\r- kind: Extension\rtype: shoot-dns-service\rgloballyEnabled: true\rProviding Base Domains usable for a Shoot So, far only the external DNS domain of a shoot already used for the kubernetes api server and ingress DNS names can be used for managed DNS names. This is either the shoot domain as subdomain of the default domain configured for the gardener installation or a dedicated domain with dedicated access credentials configured for a dedicated shoot via the shoot manifest.\nShoot Feature Gate If the shoot DNS feature is not globally enabled by default (depends on the extension registration on the garden cluster), it must be enabled per shoot.\nTo enable the feature for a shoot, the shoot manifest must explicitly add the shoot-dns-service extension.\n...\rspec:\rextensions:\r- type: shoot-dns-service\r...\r"
},
{
	"uri": "https://gardener.cloud/about/",
	"title": "Getting Started",
	"tags": [],
	"description": "",
	"content": "Gardener     \nGardener implements the automated management and operation of Kubernetes clusters as a service and provides extensibility concepts which allow support for any cloud or infrastructure provider. Its main principle is to leverage Kubernetes concepts for all of its tasks.\nIn essence, Gardener is an extension API server that comes along with a bundle of custom controllers. It introduces new API objects in an existing Kubernetes cluster (which is called garden cluster) in order to use them for the management of end-user Kubernetes clusters (which are called shoot clusters). These shoot clusters are described via declarative cluster specifications which are observed by the controllers. They will bring up the clusters, reconcile their state, perform automated updates and make sure they are always up and running.\nTo accomplish these tasks reliably and to offer a certain quality of service, Gardener controls the main components of a Kubernetes cluster (etcd, API server, controller manager, scheduler). These so-called control plane components are hosted in Kubernetes clusters themselves (which are called seed clusters). This is the main difference compared to many other OSS cluster provisioning tools: The shoot clusters do not have dedicated master VMs. Instead, the control plane is deployed as a native Kubernetes workload into the seeds. This does not only effectively reduce the total cost of ownership but also allows easier implementations for \u0026ldquo;day-2 operations\u0026rdquo; (like cluster updates or robustness) by relying on all the mature Kubernetes features and capabilities.\nPlease find more information regarding the concepts and a detailed description of the architecture in our Gardener Wiki and our blog post on kubernetes.io.\n K8s Conformance Test Coverage Conformance test results of latest stable Gardener release, transparently visible at the CNCF test grid:\n   Provider/K8s v1.18 v1.17 v1.16 v1.15 v1.14 v1.13 v1.12 v1.11 v1.10     AWS            Azure            GCP            OpenStack            Alicloud       N/A N/A N/A   Packet N/A N/A N/A N/A N/A N/A N/A N/A N/A    Besides the conformance tests, over 400 additional e2e tests are executed on a daily basis. Get an overview of the test results at testgrid.\nStart using or developing the Gardener locally See our documentation in the /docs repository, please find the index here.\nSetting up your own Gardener landscape in the Cloud The quickest way to test drive Gardener is to install it virtually onto an existing Kubernetes cluster, just like you would install any other Kubernetes-ready application. Launch your automatic installer here\nWe also have a Gardener Helm Chart. Alternatively you can use our garden setup project to create a fully configured Gardener landscape which also includes our Gardener Dashboard.\nFeedback and Support Feedback and contributions are always welcome!\nAll channels for getting in touch or learning about our project are listed under the community section. We are cordially inviting interested parties to join our weekly meetings.\nPlease report bugs or suggestions about our Kubernetes clusters as such or the Gardener itself as GitHub issues or join our Slack channel #gardener (please invite yourself to the Kubernetes workspace here).\nLearn More! Please find further resources about our project here:\n Our landing page gardener.cloud \u0026ldquo;Gardener, the Kubernetes Botanist\u0026rdquo; blog on kubernetes.io SAP news article about \u0026ldquo;Project Gardener\u0026rdquo; Introduction movie: \u0026ldquo;Gardener - Planting the Seeds of Success in the Cloud\u0026rdquo; \u0026ldquo;Thinking Cloud Native\u0026rdquo; talk at EclipseCon 2018 Blog - \u0026ldquo;Showcase of Gardener at OSCON 2018\u0026rdquo;  "
},
{
	"uri": "https://gardener.cloud/documentation/050-tutorials/content/howto/gpu/",
	"title": "GPU Enabled Cluster",
	"tags": [],
	"description": "Setting up a GPU Enabled Cluster for Deep Learning",
	"content": "Intro Be aware, that the following sections might be opinionated. Kubernetes, and the GPU support in particular, are rapidly evolving, which means that this guide is likely to be outdated sometime soon. For this reason, contributions are highly appreciated to update this guide.\nCreate a Cluster First thing first, let’s create a k8s cluster with GPU accelerated nodes. In this example we will use AWS p2.xlarge EC2 instance because it's the cheapest available option at the moment. Use such cheap instances for learning to limit your resource costs. This costs around 1€/hour per GPU\nInstall NVidia Driver as Daemonset apiVersion: apps/v1\rkind: DaemonSet\rmetadata:\rname: nvidia-driver-installer\rnamespace: kube-system\rlabels:\rk8s-app: nvidia-driver-installer\rspec:\rselector:\rmatchLabels:\rname: nvidia-driver-installer\rk8s-app: nvidia-driver-installer\rtemplate:\rmetadata:\rlabels:\rname: nvidia-driver-installer\rk8s-app: nvidia-driver-installer\rspec:\rhostPID: true\rinitContainers:\r- image: squat/modulus:4a1799e7aa0143bcbb70d354bab3e419b1f54972\rname: modulus\rargs:\r- compile\r- nvidia\r- \u0026#34;410.104\u0026#34;\rsecurityContext:\rprivileged: true\renv:\r- name: MODULUS_CHROOT\rvalue: \u0026#34;true\u0026#34;\r- name: MODULUS_INSTALL\rvalue: \u0026#34;true\u0026#34;\r- name: MODULUS_INSTALL_DIR\rvalue: /opt/drivers\r- name: MODULUS_CACHE_DIR\rvalue: /opt/modulus/cache\r- name: MODULUS_LD_ROOT\rvalue: /root\r- name: IGNORE_MISSING_MODULE_SYMVERS\rvalue: \u0026#34;1\u0026#34; volumeMounts:\r- name: etc-coreos\rmountPath: /etc/coreos\rreadOnly: true\r- name: usr-share-coreos\rmountPath: /usr/share/coreos\rreadOnly: true\r- name: ld-root\rmountPath: /root\r- name: module-cache\rmountPath: /opt/modulus/cache\r- name: module-install-dir-base\rmountPath: /opt/drivers\r- name: dev\rmountPath: /dev\rcontainers:\r- image: \u0026#34;gcr.io/google-containers/pause:3.1\u0026#34;\rname: pause\rtolerations:\r- key: \u0026#34;nvidia.com/gpu\u0026#34;\reffect: \u0026#34;NoSchedule\u0026#34;\roperator: \u0026#34;Exists\u0026#34;\rvolumes:\r- name: etc-coreos\rhostPath:\rpath: /etc/coreos\r- name: usr-share-coreos\rhostPath:\rpath: /usr/share/coreos\r- name: ld-root\rhostPath:\rpath: /\r- name: module-cache\rhostPath:\rpath: /opt/modulus/cache\r- name: dev\rhostPath:\rpath: /dev\r- name: module-install-dir-base\rhostPath:\rpath: /opt/drivers\rInstall Device Plugin apiVersion: apps/v1\rkind: DaemonSet\rmetadata:\rname: nvidia-gpu-device-plugin\rnamespace: kube-system\rlabels:\rk8s-app: nvidia-gpu-device-plugin\r#addonmanager.kubernetes.io/mode: Reconcile\r spec:\rselector:\rmatchLabels:\rk8s-app: nvidia-gpu-device-plugin\rtemplate:\rmetadata:\rlabels:\rk8s-app: nvidia-gpu-device-plugin\rannotations:\rscheduler.alpha.kubernetes.io/critical-pod: \u0026#39;\u0026#39;\rspec:\rpriorityClassName: system-node-critical\rvolumes:\r- name: device-plugin\rhostPath:\rpath: /var/lib/kubelet/device-plugins\r- name: dev\rhostPath:\rpath: /dev\rcontainers:\r- image: \u0026#34;k8s.gcr.io/nvidia-gpu-device-plugin@sha256:08509a36233c5096bb273a492251a9a5ca28558ab36d74007ca2a9d3f0b61e1d\u0026#34;\rcommand: [\u0026#34;/usr/bin/nvidia-gpu-device-plugin\u0026#34;, \u0026#34;-logtostderr\u0026#34;, \u0026#34;-host-path=/opt/drivers/nvidia\u0026#34;]\rname: nvidia-gpu-device-plugin\rresources:\rrequests:\rcpu: 50m\rmemory: 10Mi\rlimits:\rcpu: 50m\rmemory: 10Mi\rsecurityContext:\rprivileged: true\rvolumeMounts:\r- name: device-plugin\rmountPath: /device-plugin\r- name: dev\rmountPath: /dev\rupdateStrategy:\rtype: RollingUpdate\rTest To run an example training on a GPU node, start first a base image with Tensorflow with GPU support \u0026amp; Keras\napiVersion: apps/v1\rkind: Deployment\rmetadata:\rname: deeplearning-workbench\rnamespace: default\rspec:\rreplicas: 1\rselector:\rmatchLabels:\rapp: deeplearning-workbench\rtemplate:\rmetadata:\rlabels:\rapp: deeplearning-workbench\rspec:\rcontainers:\r- name: deeplearning-workbench\rimage: afritzler/deeplearning-workbench\rresources:\rlimits:\rnvidia.com/gpu: 1\rtolerations:\r- key: \u0026#34;nvidia.com/gpu\u0026#34;\reffect: \u0026#34;NoSchedule\u0026#34;\roperator: \u0026#34;Exists\u0026#34;\rNote: the tolerations section above is not required if you deploy the ExtendedResourceToleration admission controller to your cluster. You can do this in the kubernetes section of your Gardener cluster shoot.yaml as follows:\n kubernetes:\rkubeAPIServer:\radmissionPlugins:\r- name: ExtendedResourceToleration\rNow exec into the container and start an example Keras training\nkubectl exec -it deeplearning-workbench-8676458f5d-p4d2v -- /bin/bash\rcd /keras/example\rpython imdb_cnn.py\rAcknowledgments \u0026amp; References  Andreas Fritzler from the Gardener Core team for the R\u0026amp;D and providing this setup. Build and install NVIDIA driver on CoreOS Nvidia Device Plugin  "
},
{
	"uri": "https://gardener.cloud/blog/2018_week_50/",
	"title": "Hardening the Gardener Community Setup",
	"tags": [],
	"description": "",
	"content": "The Gardener project team has analyzed the impact of the Gardener CVE-2018-2475 and the Kubernetes CVE-2018-1002105 on the Gardener Community Setup. Following some recommendations it is possible to mitigate both vulnerabilities.\nRead more on Hardening the Gardener Community Setup.\n"
},
{
	"uri": "https://gardener.cloud/documentation/050-tutorials/content/howto/secure-setup/",
	"title": "Hardening the Gardener Community Setup",
	"tags": [],
	"description": "Hardening the Gardener Community Setup",
	"content": "Hardening the Gardener Community Setup Context Gardener stakeholders in the Open Source community usually use the Gardener Setup Scripts, to create a Garden cluster based on Kubernetes v1.9 which then can be used to create Shoot clusters based on Kubernetes v1.10, v1.11 and v1.12. Shoot clusters can play the following roles in a Gardener landscape:\n Seed cluster Shoot cluster  As Alban Crequy from Kinvolk has recommended in his recent Gardener blog Auditing Kubernetes for Secure Setup the Gardener Team at SAP has applied several means to harden the Gardener landscapes at SAP.\nRecommendations Mitigation for Gardener CVE-2018-2475 The following recommendations describe how you can harden your Gardener Community Setup by adding a Seed cluster hardened with network policies.\n Use the Gardener Setup Scripts to create a Garden cluster in a dedicated IaaS account Create a Shoot cluster in a different IaaS account As a precaution you should not deploy the Kubernetes dashboard on this Shoot cluster Register this newly created Shoot cluster as a Seed cluster in the Gardener End user Shoot clusters can then be created using this newly created Seed cluster (which in turn is a Shoot cluster).  A tutorial on how to create a shooted seed cluster can be found here.\nThe rational behind this activity is, that Calico network policies harden this Seed cluster but the community installer uses Flannel which does not offer these features for the Garden cluster.\nWhen you have added a hardened Seed cluster you are expected not be vulnerable to the Gardener CVE-2018-2475 anymore.\nMitigation for Kubernetes CVE-2018-1002105 In addition when you follow the recommendations in the recent Gardener Security Announcement you are expected not be vulnerable to the Kubernetes CVE-2018-1002105 with your hardened Gardener Community Setup.\nAlternative Approach For this alternative approach there is no Gardener blog available, it is not part of the Gardener Setup Scripts, but it was tested by the Gardener Team at SAP. Use GKE to host a Garden cluster based on Kubernetes v1.10, v1.11 and v1.12 (without the Kubernetes dashboard) in a dedicated GCP account. If you do this by your own, please ensure that the network policies are turned on, which might not be the case by default. Then you can apply the security configuration which Alban Crequy from Kinvolk has recommended in his blog directly in the Garden cluster and create Shoot clusters from there in a different IaaS account.\n"
},
{
	"uri": "https://gardener.cloud/documentation/050-tutorials/content/howto/hibernate-cluster/",
	"title": "Hibernate a Cluster",
	"tags": [],
	"description": "Hibernate a Cluster to save money",
	"content": "Problem If you have built a customer scenario for demo purposes, you don't want to run the cluster all the time. The costs would exceed here very fast. You can setup the cluster again for each demo, thanks to Helm this works relatively well, but takes a long time depending on the infrastructure. Furthermore not all 3rd party services are connected yet.\n\nSet a Gardener Cluster in Hibernate Mode Fortunately the Gardener offers the possibility to scale the Worker Nodes down to \u0026ldquo;Zero\u0026rdquo; without much effort. Follow the slide deck below to bring your Gardner Cluster in Hibernate Mode\n The mechanism to hibernate a cluster has changed. It is not necessary to scale down the workergroups to zero. You can now press the hibernate button or add the property hibernation to the shoot YAML.\n \r\r\r\r\r\r\r\r\r\r\r"
},
{
	"uri": "https://gardener.cloud/blog/2018_week_40/",
	"title": "Hibernate a Cluster to save money",
	"tags": [],
	"description": "",
	"content": "You want to experiment with Kubernetes or have set up a customer scenario, but you don't want to run the cluster 24 / 7 for reasons of cost?\n\rThe Gardener gives you the possibility to scale your cluster down to zero nodes.\n..read some more on Hibernate a Cluster.\n"
},
{
	"uri": "https://gardener.cloud/documentation/050-tutorials/content/howto/debug-a-pod/",
	"title": "How to debug a pod",
	"tags": [],
	"description": "Your pod doesn&#39;t run as expected. Are there any log files? Where? How could I debug a pod?",
	"content": "Introduction Kubernetes offers powerful options to get more details about startup or runtime failures of pods as e.g. described in Application Introspection and Debugging or Debug Pods and Replication Controllers.\nIn order to identify pods with potential issus you could e.g. run kubectl get pods --all-namespaces | grep -iv Running  to filter out the pods which are not in the state Running. One of frequent error state is CrashLoopBackOff, which tells that a pod crashes right after the start. Kubernetes then tries to restart the pod, but often the pod startup fails again.\nHere is a short list of possible reasons which might lead to a pod crash:\n error during image pull caused by e.g. wrong/missing secrets or wrong/missing image the app runs in an error state caused e.g. by missing environmental variables (ConfigMaps) or secrets liveness probe failed too high resource consumption (memory and/or CPU) or too strict quota settings persistent volumes can't be created/mounted the container image is not updated  Basically, the commands kubectl logs ... and kubectl describe ... with additional parameters are used to get more detailed information. By calling e.g. kubectl logs --help you get more detailed information about the command and its parameters.\nIn the next sections you'll find some basic approaches to get some ideas what went wrong.\nRemarks:\n Even if the pods seem to be running as the status Running indicates, a high counter of the Restarts shows potential problems There is as well an interactive Tutorial Troubleshooting with Kubectl available which explains basic debugging activities The examples below are deployed into the namespace default. In case you want to change it use the optional parameter --namespace \u0026lt;your-namespace\u0026gt; to select the target namespace. They require Kubernetes release ≥ 1.8.  Prerequisites Your deployment was successful (no logical/syntactical errors in the manifest files), but the pod(s) aren't running.\nError caused by wrong image name You run kubectl describe pod \u0026lt;your-pod\u0026gt; \u0026lt;your-namespace\u0026gt; to get detailed information about the pod startup.\nIn the Events section, you get an error message like Failed to pull image ... and Reason: Failed. The pod is in state ImagePullBackOff.\nThe example below is based on demo in Kubernetes documentation. In all examples the default namespace is used.\nFirst, cleanup with\nkubectl delete pod termination-demo\rNext, create a resource based on the yaml content below\napiVersion: v1\rkind: Pod metadata:\rname: termination-demo\rspec:\rcontainers:\r- name: termination-demo-container\rimage: debiann\rcommand: [\u0026#34;/bin/sh\u0026#34;]\rargs: [\u0026#34;-c\u0026#34;, \u0026#34;sleep 10 \u0026amp;\u0026amp; echo Sleep expired \u0026gt; /dev/termination-log\u0026#34;]\rkubectl describe pod termination-demo lists the following content in the Event section\nEvents:\rFirstSeen\tLastSeen\tCount\tFrom\tSubObjectPath\tType\tReason\tMessage\r---------\t--------\t-----\t----\t-------------\t--------\t------\t-------\r2m\t2m\t1\tdefault-scheduler\tNormal\tScheduled\tSuccessfully assigned termination-demo to ip-10-250-17-112.eu-west-1.compute.internal\r2m\t2m\t1\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tNormal\tSuccessfulMountVolume\tMountVolume.SetUp succeeded for volume \u0026#34;default-token-sgccm\u0026#34; 2m\t1m\t4\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tspec.containers{termination-demo-container}\tNormal\tPulling\tpulling image \u0026#34;debiann\u0026#34;\r2m\t1m\t4\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tspec.containers{termination-demo-container}\tWarning\tFailed\tFailed to pull image \u0026#34;debiann\u0026#34;: rpc error: code = Unknown desc = Error: image library/debiann:latest not found\r2m\t54s\t10\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tWarning\tFailedSync\tError syncing pod\r2m\t54s\t6\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tspec.containers{termination-demo-container}\tNormal\tBackOff\tBack-off pulling image \u0026#34;debiann\u0026#34;\rThe error message with Reason: Failed tells that there is an error during pulling the image. A closer look at the image name indicates a misspelling.\nApp runs in an error state caused by missing ConfigMaps or Secrets This example illustrates the behavior in case of the app expecting environment variables but the corresponding Kubernetes artifacts are missing.\nFirst, cleanup with\nkubectl delete deployment termination-demo\rkubectl delete configmaps app-env\rNext, deploy this manifest\napiVersion: apps/v1\rkind: Deployment\rmetadata:\rname: termination-demo\rlabels:\rapp: termination-demo\rspec:\rreplicas: 1\rselector:\rmatchLabels:\rapp: termination-demo\rtemplate:\rmetadata:\rlabels:\rapp: termination-demo\rspec:\rcontainers:\r- name: termination-demo-container\rimage: debian\rcommand: [\u0026#34;/bin/sh\u0026#34;]\rargs: [\u0026#34;-c\u0026#34;, \u0026#34;sed \\\u0026#34;s/foo/bar/\\\u0026#34; \u0026lt; $MYFILE\u0026#34;]\rNow, the command kubectl get pods lists the pod termination-demo-xxx in the state Error or CrashLoopBackOff. The command kubectl describe pod termination-demo-xxx tells that there is no error during startup but gives no clue about what caused the crash.\nEvents:\rFirstSeen\tLastSeen\tCount\tFrom\tSubObjectPath\tType\tReason\tMessage\r---------\t--------\t-----\t----\t-------------\t--------\t------\t-------\r19m\t19m\t1\tdefault-scheduler\tNormal\tScheduled\tSuccessfully assigned termination-demo-5fb484867d-xz2x9 to ip-10-250-17-112.eu-west-1.compute.internal\r19m\t19m\t1\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tNormal\tSuccessfulMountVolume\tMountVolume.SetUp succeeded for volume \u0026#34;default-token-sgccm\u0026#34; 19m\t19m\t4\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tspec.containers{termination-demo-container}\tNormal\tPulling\tpulling image \u0026#34;debian\u0026#34;\r19m\t19m\t4\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tspec.containers{termination-demo-container}\tNormal\tPulled\tSuccessfully pulled image \u0026#34;debian\u0026#34;\r19m\t19m\t4\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tspec.containers{termination-demo-container}\tNormal\tCreated\tCreated container\r19m\t19m\t4\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tspec.containers{termination-demo-container}\tNormal\tStarted\tStarted container\r19m\t14m\t24\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tspec.containers{termination-demo-container}\tWarning\tBackOff\tBack-off restarting failed container\r19m\t4m\t69\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tWarning\tFailedSync\tError syncing pod\rThe command kubectl get logs termination-demo-xxx gives access to the output, the application writes on stderr and stdout. In this case, you get an output like\n/bin/sh: 1: cannot open : No such file\rSo you need to have a closer look at the application. In this case the environmental variable MYFILEis missing. To fix this issue you could e.g. add a ConfigMap to your deployment as it is shown in the manifest listed below.\napiVersion: v1\rkind: ConfigMap\rmetadata:\rname: app-env\rdata:\rMYFILE: \u0026#34;/etc/profile\u0026#34;\r---\rapiVersion: apps/v1\r kind: Deployment\rmetadata:\rname: termination-demo\rlabels:\rapp: termination-demo\rspec:\rreplicas: 1\rselector:\rmatchLabels:\rapp: termination-demo\rtemplate:\rmetadata:\rlabels:\rapp: termination-demo\rspec:\rcontainers:\r- name: termination-demo-container\rimage: debian\rcommand: [\u0026#34;/bin/sh\u0026#34;]\rargs: [\u0026#34;-c\u0026#34;, \u0026#34;sed \\\u0026#34;s/foo/bar/\\\u0026#34; \u0026lt; $MYFILE\u0026#34;]\renvFrom:\r- configMapRef:\rname: app-env Note that once you fix the error and re-run the scenario, you might still see the pod in CrashLoopBackOff status. It is because the container finishes the command sed ... and runs to completion. In order to keep the container in Running status, a long running task is required, e.g.\napiVersion: v1\rkind: ConfigMap\rmetadata:\rname: app-env\rdata:\rMYFILE: \u0026#34;/etc/profile\u0026#34;\rSLEEP: \u0026#34;5\u0026#34;\r---\rapiVersion: apps/v1\r kind: Deployment\rmetadata:\rname: termination-demo\rlabels:\rapp: termination-demo\rspec:\rreplicas: 1\rselector:\rmatchLabels:\rapp: termination-demo\rtemplate:\rmetadata:\rlabels:\rapp: termination-demo\rspec:\rcontainers:\r- name: termination-demo-container\rimage: debian\rcommand: [\u0026#34;/bin/sh\u0026#34;]\r# args: [\u0026#34;-c\u0026#34;, \u0026#34;sed \\\u0026#34;s/foo/bar/\\\u0026#34; \u0026lt; $MYFILE\u0026#34;]\r args: [\u0026#34;-c\u0026#34;, \u0026#34;while true; do sleep $SLEEP; echo sleeping; done;\u0026#34;]\renvFrom:\r- configMapRef:\rname: app-env\rToo high resource consumption or too strict quota settings You can optionally specify the amount of memory and/or CPU your container gets during runtime. In case these settings are missing, the default requests settings are taken: CPU: 0m (in Milli CPU) and RAM: 0Gi which indicate no other limits than the ones of the node(s) itself. Find more details in Configure Default Memory Requests and Limits for a Namespace,\nIn case your application needs more resources, Kubernetes distinguishes between requests and limit settings: requests specify the guaranteed amount of resource, whereas limit tells Kubernetes the maximum amount of resource the container might need. Mathematically both settings could be described by the relation 0 \u0026lt;= requests \u0026lt;= limit. For both settings you need to consider the total amount of resources the available nodes provide. For a detailed description of the concept see Resource Quality of Service in Kubernetes.\nUse kubectl describe nodes to get a first overview of the resource consumption of your cluster. Of special interest are the figures indicating the amount of CPU and Memory Requests at the bottom of the output.\nThe next example demonstrates what happens in case the CPU request is too high in order to be managed by your cluster.\nFirst, cleanup with\nkubectl delete deployment termination-demo\rkubectl delete configmaps app-env\rNext, adapt the cpu in the yaml below to be slightly higher than the remaining cpu resources in your cluster and deploy this manifest. In this example 600m (milli CPUs) are requested in a Kubernetes system with a single 2 Core worker node which results in an error message.\napiVersion: apps/v1\rkind: Deployment\rmetadata:\rname: termination-demo\rlabels:\rapp: termination-demo\rspec:\rreplicas: 1\rselector:\rmatchLabels:\rapp: termination-demo\rtemplate:\rmetadata:\rlabels:\rapp: termination-demo\rspec:\rcontainers:\r- name: termination-demo-container\rimage: debian\rcommand: [\u0026#34;/bin/sh\u0026#34;]\rargs: [\u0026#34;-c\u0026#34;, \u0026#34;sleep 10 \u0026amp;\u0026amp; echo Sleep expired \u0026gt; /dev/termination-log\u0026#34;]\rresources:\rrequests:\rcpu: \u0026#34;600m\u0026#34; The command kubectl get pods lists the pod termination-demo-xxx in the state Pending. More details on why this happens could be found by using the command kubectl describe pod termination-demo-xxx:\n$ kubectl describe po termination-demo-fdb7bb7d9-mzvfw\rName: termination-demo-fdb7bb7d9-mzvfw\rNamespace: default\r...\rContainers:\rtermination-demo-container:\rImage: debian\rPort: \u0026lt;none\u0026gt;\rHost Port: \u0026lt;none\u0026gt;\rCommand:\r/bin/sh\rArgs:\r-c\rsleep 10 \u0026amp;\u0026amp; echo Sleep expired \u0026gt; /dev/termination-log\rRequests:\rcpu: 6\rEnvironment: \u0026lt;none\u0026gt;\rMounts:\r/var/run/secrets/kubernetes.io/serviceaccount from default-token-t549m (ro)\rConditions:\rType Status\rPodScheduled False\rEvents:\rType Reason Age From Message\r---- ------ ---- ---- -------\rWarning FailedScheduling 9s (x7 over 40s) default-scheduler 0/2 nodes are available: 2 Insufficient cpu.\rMore details in\n Managing Compute Resources for Containters Resource Quality of Service in Kubernetes  Remark:\n This example works similarly when specifying a too high request for memory In case you configured an autoscaler range when creating your Kubernetes cluster another worker node will be started automatically if you didn't reach the maximum number of worker nodes If your app is running out of memory (the memory settings are too small), you typically find OOMKilled (Out Of Memory) message in the Events section fo the kubectl describe pod ... output  Why was the container image not updated? You applied a fix in your app, created a new container image and pushed it into your container repository. After redeploying your Kubernetes manifests you expected to get the updated app, but still the same bug is in the new deployment present.\nThis behavior is related to how Kubernetes decides whether to pull a new docker image or to use the cached one.\nIn case you didn't change the image tag, the default image policy IfNotPresent tells Kubernetes to use the cached image (see Images).\nAs a best practice you should not use the tag latest and change the image tag whenever you changed anything in your image (see Configuration Best Practices).\nFind more details in FAQ Container Image not updating\nLinks  Application Introspection and Debugging Debug Pods and Replication Controllers Logging Architecture Configure Default Memory Requests and Limits for a Namespace Managing Compute Resources for Containters Resource Quality of Service in Kubernetes Interactive Tutorial Troubleshooting with Kubectl Images Kubernetes Best Practises  "
},
{
	"uri": "https://gardener.cloud/documentation/050-tutorials/content/app/",
	"title": "HowTos",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gardener.cloud/documentation/050-tutorials/content/howto/",
	"title": "HowTos",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gardener.cloud/documentation/050-tutorials/content/app/https/",
	"title": "HTTPS with self Signed Certificate",
	"tags": [],
	"description": "HTTPS with self Signed Certificate",
	"content": "Configuring ingress with front-end TLS It is alyways recommended to enable encryption for services to prevent traffic interception and man-in-the-middle attacks - even in DEV environments.\nYou should configure front-end Transport Layer Security (TLS) so that the ingress controller can secure access to a service from the client to the load balancer by using HTTPS.\nWe will use basic procedure here. If your configuration requires advanced security options, please refer to official CloudFlare's cfssl documentation.\nBefore you begin At first, you need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using Gardener\nInstall CFSSL The first step in securing Docker and Kubernetes is to set up a PKI infrastructure for managing TLS certificates.\nInitialize a CA Before we can generate any certs we need to initialize a CA.\nmkdir cfssl\rcd cfssl\rcfssl print-defaults config \u0026gt; ca-config.json\rcfssl print-defaults csr \u0026gt; ca-csr.json\rConfigure CA options Now we can configure signing options inside ca-config.json config file. Default options contain following preconfigured fields:\n profiles: www with server auth (TLS Web Server Authentication) X509 V3 extension and client with client auth (TLS Web Client Authentication) X509 V3 extension. expiry: with 8760h default value (or 365 days)  For compliance let's edit the ca-config.json file and rename www profile into server\nEdit the ca-csr.json to your needs. See example below. Keep in mind that the hosts entries must match all your ingress entries.\nexample ca-csr.json\n{\r\u0026#34;CN\u0026#34;: \u0026#34;Gardener Self Signed CA\u0026#34;,\r\u0026#34;hosts\u0026#34;: [\r\u0026#34;ui.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.com\u0026#34;,\r\u0026#34;api.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.com\u0026#34;\r],\r\u0026#34;key\u0026#34;: {\r\u0026#34;algo\u0026#34;: \u0026#34;rsa\u0026#34;,\r\u0026#34;size\u0026#34;: 2048\r},\r\u0026#34;names\u0026#34;: [\r{\r\u0026#34;C\u0026#34;: \u0026#34;US\u0026#34;,\r\u0026#34;ST\u0026#34;: \u0026#34;CA\u0026#34;,\r\u0026#34;L\u0026#34;: \u0026#34;San Francisco\u0026#34;\r}\r]\r}\rAnd generate CA with defined options:\ncfssl gencert -initca ca-csr.json | cfssljson -bare ca -\rYou'll get following files:\n ca-key.pem ca.csr ca.pem  Note: Please keep ca-key.pem file in safe. This key allows to create any kind of certificates within your CA.\nGenerate server certificate cfssl print-defaults csr \u0026gt; server.json\rMost important values for server certificate are Common Name (CN) and hosts. We have to substitute them, for example:\n{\r\u0026#34;CN\u0026#34;: \u0026#34;Gardener Self Signed CA\u0026#34;,\r\u0026#34;hosts\u0026#34;: [\r\u0026#34;ui.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.com\u0026#34;,\r\u0026#34;api.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.com\u0026#34;\r],\r\u0026#34;key\u0026#34;: {\r\u0026#34;algo\u0026#34;: \u0026#34;rsa\u0026#34;,\r\u0026#34;size\u0026#34;: 2048\r},\r\u0026#34;names\u0026#34;: [\r{\r\u0026#34;C\u0026#34;: \u0026#34;US\u0026#34;,\r\u0026#34;ST\u0026#34;: \u0026#34;CA\u0026#34;,\r\u0026#34;L\u0026#34;: \u0026#34;San Francisco\u0026#34;\r}\r]\r}\rNow we are ready to generate server certificate and private key:\ncfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=server server.json | cfssljson -bare server\rYou'll get following files:\n server-key.pem server.csr server.pem  Configure Kubernetes ingress with TLS To configure front-end TLS, you need to create a TLS certificate (already done above), create a Kubernetes secret, update applicable .yaml files, apply your .yaml file changes, regenerate ingress controllers, and visit the application.\nCreate Kubernetes secret kubectl create secret tls tls-secret --key ./server-key.pem --cert server.pem\rCreate Service / Ingress now you can referenc ethe TLS secret within your ingress definition\nexample ingress definition\napiVersion: v1\rkind: Service\rmetadata:\rlabels:\rapp: node-server\rname: node-svc\rnamespace: default\rspec:\rtype: NodePort\rports:\r- port: 8080\rselector:\rapp: node-server\r---\rapiVersion: extensions/v1beta1\r kind: Ingress\rmetadata:\rname: node-ingress\rspec:\rtls:\r- hosts:\r- ui.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.com\rsecretName: tls-secret\rrules:\r- host: ui.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.com\rhttp:\rpaths:\r- backend:\rserviceName: node-svc\rservicePort: 8080\r"
},
{
	"uri": "https://gardener.cloud/documentation/050-tutorials/content/howto/knative-install/",
	"title": "Install Knative in Gardener clusters",
	"tags": [],
	"description": "A walkthrough the steps for installing Knative in Gardener shoot clusters.",
	"content": "This guide walks you through the installation of the latest version of Knative using pre-built images on a Gardener created cluster environment. To set up your own Gardener, see the documentation or have a look at the landscape-setup-template project. To learn more about this open source project, read the blog on kubernetes.io.\nYou can find guides for other platforms here.\nBefore you begin Knative requires a Kubernetes cluster v1.15 or newer.\nInstall and configure kubectl   If you already have kubectl CLI, run kubectl version --short to check the version. You need v1.10 or newer. If your kubectl is older, follow the next step to install a newer version.\n  Install the kubectl CLI.\n  Access Gardener   Create a project in the Gardener dashboard. This will essentially create a Kubernetes namespace with the name garden-\u0026lt;my-project\u0026gt;.\n  Configure access to your Gardener project using a kubeconfig. If you are not the Gardener Administrator already, you can create a technical user in the Gardener dashboard: go to the \u0026ldquo;Members\u0026rdquo; section and add a service account. You can then download the kubeconfig for your project. You can skip this step if you create your cluster using the user interface; it is only needed for programmatic access, make sure you set export KUBECONFIG=garden-my-project.yaml in your shell.   Creating a Kubernetes cluster You can create your cluster using kubectl cli by providing a cluster specification yaml file. You can find an example for GCP here. Make sure the namespace matches that of your project. Then just apply the prepared so-called \u0026ldquo;shoot\u0026rdquo; cluster crd with kubectl:\nkubectl apply --filename my-cluster.yaml\rThe easier alternative is to create the cluster following the cluster creation wizard in the Gardener dashboard: Configure kubectl for your cluster You can now download the kubeconfig for your freshly created cluster in the Gardener dashboard or via cli as follows:\nkubectl --namespace shoot--my-project--my-cluster get secret kubecfg --output jsonpath={.data.kubeconfig} | base64 --decode \u0026gt; my-cluster.yaml\rThis kubeconfig file has full administrators access to you cluster. For the rest of this guide be sure you have export KUBECONFIG=my-cluster.yaml set.\nInstalling Istio Knative depends on Istio. If your cloud platform offers a managed Istio installation, we recommend installing Istio that way, unless you need the ability to customize your installation.\nOtherwise, see the Installing Istio for Knative guide to install Istio.\nYou must install Istio on your Kubernetes cluster before continuing with these instructions to install Knative.\nInstalling cluster-local-gateway for serving cluster-internal traffic If you installed Istio, you can install a cluster-local-gateway within your Knative cluster so that you can serve cluster-internal traffic. If you want to configure your revisions to use routes that are visible only within your cluster, install and use the cluster-local-gateway.\nInstalling Knative The following commands install all available Knative components as well as the standard set of observability plugins. To customize your Knative installation, see Performing a Custom Knative Installation.\n  If you are upgrading from Knative 0.3.x: Update your domain and static IP address to be associated with the LoadBalancer istio-ingressgateway instead of knative-ingressgateway. Then run the following to clean up leftover resources:\nkubectl delete svc knative-ingressgateway -n istio-system\rkubectl delete deploy knative-ingressgateway -n istio-system\rIf you have the Knative Eventing Sources component installed, you will also need to delete the following resource before upgrading:\nkubectl delete statefulset/controller-manager -n knative-sources\rWhile the deletion of this resource during the upgrade process will not prevent modifications to Eventing Source resources, those changes will not be completed until the upgrade process finishes.\n  To install Knative, first install the CRDs by running the kubectl apply command once with the -l knative.dev/crd-install=true flag. This prevents race conditions during the install, which cause intermittent errors:\nkubectl apply --selector knative.dev/crd-install=true \\\r --filename https://github.com/knative/serving/releases/download/v0.12.1/serving.yaml \\\r --filename https://github.com/knative/eventing/releases/download/v0.12.1/eventing.yaml \\\r --filename https://github.com/knative/serving/releases/download/v0.12.1/monitoring.yaml\r  To complete the install of Knative and its dependencies, run the kubectl apply command again, this time without the --selector flag, to complete the install of Knative and its dependencies:\nkubectl apply --filename https://github.com/knative/serving/releases/download/v0.12.1/serving.yaml \\\r --filename https://github.com/knative/eventing/releases/download/v0.12.1/eventing.yaml \\\r --filename https://github.com/knative/serving/releases/download/v0.12.1/monitoring.yaml\r  Monitor the Knative components until all of the components show a STATUS of Running:\nkubectl get pods --namespace knative-serving\rkubectl get pods --namespace knative-eventing\rkubectl get pods --namespace knative-monitoring\r  Set your custom domain  Fetch the external IP or CNAME of the knative-ingressgateway  kubectl --namespace istio-system get service knative-ingressgateway\rNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE\rknative-ingressgateway LoadBalancer 100.70.219.81 35.233.41.212 80:32380/TCP,443:32390/TCP,32400:32400/TCP 4d\rCreate a wildcard DNS entry in your custom domain to point to above IP or CNAME  *.knative.\u0026lt;my domain\u0026gt; == A 35.233.41.212\r# or CNAME if you are on AWS\r*.knative.\u0026lt;my domain\u0026gt; == CNAME a317a278525d111e89f272a164fd35fb-1510370581.eu-central-1.elb.amazonaws.com\rAdapt your knative config-domain (set your domain in the data field)  kubectl --namespace knative-serving get configmaps config-domain --output yaml\rapiVersion: v1\rdata:\rknative.\u0026lt;my domain\u0026gt;: \u0026quot;\u0026quot;\rkind: ConfigMap\rname: config-domain\rnamespace: knative-serving\rWhat's next Now that your cluster has Knative installed, you can see what Knative has to offer.\nTo deploy your first app with the Getting Started with Knative App Deployment guide.\nGet started with Knative Eventing by walking through one of the Eventing Samples.\nInstall Cert-Manager if you want to use the automatic TLS cert provisioning feature.\nCleaning up Use the Gardener dashboard to delete your cluster, or execute the following with kubectl pointing to your garden-my-project.yaml kubeconfig:\nkubectl --kubeconfig garden-my-project.yaml --namespace garden--my-project annotate shoot my-cluster confirmation.gardener.cloud/deletion=true\rkubectl --kubeconfig garden-my-project.yaml --namespace garden--my-project delete shoot my-cluster\r"
},
{
	"uri": "https://gardener.cloud/documentation/050-tutorials/content/howto/content_trust/",
	"title": "Integrity and Immutability",
	"tags": [],
	"description": "Ensure that you get always the right image",
	"content": "Introduction When transferring data among networked systems, trust is a central concern. In particular, when communicating over an untrusted medium such as the internet, it is critical to ensure the integrity and immutability of all the data a system operates on. Especially if you use Docker Engine to push and pull images (data) to a public registry.\nThis immutability offers me a guarantee that any and all containers that I instantiate will be absolutely identical at inception. Surprise surprise, deterministic operations.\nA Lesson in Deterministic Ops Docker Tags are about as reliable and disposable as this guy down here.\nSeems simple enough. You have probably already deployed hundreds of YAML's or started endless count of Docker container.\ndocker run --name mynginx1 -P -d nginx:1.13.9\ror\napiVersion: apps/v1\rkind: Deployment\rmetadata:\rname: rss-site\rspec:\rreplicas: 1\rselector:\rmatchLabels:\rapp: web\rtemplate:\rmetadata:\rlabels:\rapp: web\rspec:\rcontainers:\r- name: front-end\rimage: nginx:1.13.9\rports:\r- containerPort: 80\rBut Tags are mutable and humans are prone to error. Not a good combination. Here we’ll dig into why the use of tags can be dangerous and how to deploy your containers across a pipeline and across environments, you guessed it, with determinism in mind.\nI want to ensure that whether it’s today or 5 years from now, that specific deployment uses the very same image that I defined. Any updates or newer versions of an image should be executed as a new deployment. The solution: digest\nA digest takes the place of the tag when pulling an image, for example, to pull the above image by digest, run the following command:\ndocker run --name mynginx1 -P -d nginx@sha256:4771d09578c7c6a65299e110b3ee1c0a2592f5ea2618d23e4ffe7a4cab1ce5de\rYou can now make sure that the same image is always loaded at every deployment. It doesn't matter if the TAG of the image has been changed or not. This solves the problem of repeatability.\nContent Trust However, there’s an additionally hidden danger. It is possible for an attacker to replace a server image with another one infected with malware.\nDocker Content trust gives you the ability to verify both the integrity and the publisher of all the data received from a registry over any channel.\nPrior to version 1.8, Docker didn’t have a way to verify the authenticity of a server image. But in v1.8, a new feature called Docker Content Trust was introduced to automatically sign and verify the signature of a publisher.\nSo, as soon as a server image is downloaded, it is cross-checked with the signature of the publisher to see if someone tampered with it in any way. This solves the problem of trust.\nIn addition you should scan all images for known vulnerabilities, this can fill another book\n"
},
{
	"uri": "https://gardener.cloud/documentation/050-tutorials/content/howto/bash_kubeconfig/",
	"title": "Kubeconfig context as bash prompt",
	"tags": [],
	"description": "Expose the active kubeconfig into the bash",
	"content": "Use the Kubernetes command-line tool, kubectl, to deploy and manage applications on Kubernetes. Using kubectl, you can inspect cluster resources; create, delete, and update components\nBy default, the kubectl configuration is located at ~/.kube/config.\nSuppose you have two clusters, one for development work and one for scratch work.\nHow to handle this easily without copying the used configuration always to the right place?\nExport the KUBECONFIG enviroment variable bash$ export KUBECONFIG=\u0026lt;PATH-TO-M\u0026gt;-CONFIG\u0026gt;/kubeconfig-dev.yaml\rHow to determine which cluster is used by the kubectl command?\nDetermine active cluster bash$ kubectl cluster-info\rKubernetes master is running at https://api.dev.garden.shoot.canary.k8s-hana.ondemand.com\rKubeDNS is running at https://api.dev.garden.shoot.canary.k8s-hana.ondemand.com/api/v1/proxy/namespaces/kube-system/services/kube-dns\rTo further debug and diagnose cluster problems, use \u0026#39;kubectl cluster-info dump\u0026#39;.\rbash$ Display cluster in the bash - Linux and alike I found this tip on Stackoverflow and find it worth to be added here. Edit your ~/.bash_profile and add the following code snippet to show the current k8s context in the shell's prompt.\nprompt_k8s(){\rk8s_current_context=$(kubectl config current-context 2\u0026gt; /dev/null)\rif [[ $? -eq 0 ]] ; then echo -e \u0026#34;(${k8s_current_context}) \u0026#34;; fi\r}\rPS1+=\u0026#39;$(prompt_k8s)\u0026#39;\rAfter this your bash command prompt contains the active KUBECONFIG context and you always know which cluster is active - develop or production.\ne.g.\nbash$ export KUBECONFIG=/Users/d023280/Documents/workspace/gardener-ui/kubeconfig_gardendev.yaml bash (garden_dev)$ Note the (garden_dev) prefix in the bash command prompt.\nThis helps immensely to avoid thoughtless mistakes.\nDisplay cluster in the PowerShell - Windows Display current k8s cluster in the title of PowerShell window.\nCreate a profile file for your shell under %UserProfile%\\Documents\\Windows­PowerShell\\Microsoft.PowerShell_profile.ps1\nCopy following code to Microsoft.PowerShell_profile.ps1\nfunction prompt_k8s {\r$k8s_current_context = (kubectl config current-context) | Out-String\rif($?) {\rreturn $k8s_current_context\r}else {\rreturn \u0026#34;No K8S contenxt found\u0026#34;\r}\r}\r$host.ui.rawui.WindowTitle = prompt_k8s\rIf you want to switch to different cluster, you can set KUBECONFIG to new value, and re-run the file Microsoft.PowerShell_profile.ps1\n"
},
{
	"uri": "https://gardener.cloud/documentation/050-tutorials/content/howto/antipattern/",
	"title": "Kubernetes Antipatterns",
	"tags": [],
	"description": "Common Antipatterns for Kubernetes and Docker",
	"content": "This HowTo covers common kubernetes antipatterns that we have seen over the past months.\nRunning as root user. Whenever possible, do not run containers as root user. One could be tempted to say that Kubernetes Pods and Node are well separated. Host and containers running on it share the same kernel. If a container is compromised, the root user in the container has full control over the underlying node.\nWatch the very good presentation by Liz Rice at the KubeCon 2018\n\rUse RUN groupadd -r anygroup \u0026amp;\u0026amp; useradd -r -g anygroup myuser to create a group and add a user to it. Use the USER command to switch to this user. Note that you may also consider to provide an explicit UID/GID if required.\nFor Example:\nARG GF_UID=\u0026quot;500\u0026quot;\rARG GF_GID=\u0026quot;500\u0026quot;\r# add group \u0026amp; user\rRUN groupadd -r -g $GF_GID appgroup \u0026amp;\u0026amp; \\\ruseradd appuser -r -u $GF_UID -g appgroup\rUSER appuser\rStore data or logs in containers Containers are ideal for stateless applications and should be transient. This means that no data or logs should be stored in the container, as they are lost when the container is closed. Use persistence volumes instead to persist data outside of containers. Using an ELK stack is another good option for storing and processing logs.\nUsing pod IP addresses Each pod is assigned an IP address. It is necessary for pods to communicate with each other to build an application, e.g. an application must communicate with a database. Existing pods are terminated and new pods are constantly started. If you would rely on the IP address of a pod or container, you would need to update the application configuration constantly. This makes the application fragile. Create services instead. They provide a logical name that can be assigned independently of the varying number and IP addresses of containers. Services are the basic concept for load balancing within Kubernetes.\nMore than one process in a container A docker file provides a CMD and ENTRYPOINT to start the image. CMD is often used around a script that makes a configuration and then starts the container. Do not try to start multiple processes with this script. It is important to consider the separation of concerns when creating docker images. Running multiple processes in a single pod makes managing your containers, collecting logs and updating each process more difficult. You can split the image into multiple containers and manage them independently - even in one pod. Bear in mind that Kubernetes only monitors the process with PID=1. If more than one process is started within a container, then these no longer fall under the control of Kubernetes.\nCreating images in a running container A new image can be created with the docker commit command. This is useful if changes have been made to the container and you want to persist them for later error analysis. However, images created like this are not reproducible and completely worthless for a CI/CD environment. Furthermore, another developer cannot recognize which components the image contains. Instead, always make changes to the docker file, close existing containers and start a new container with the updated image.\nSaving passwords in docker image 💀 Do not save passwords in a Docker file. They are in plain text and are checked into a repository. That makes them completely vulnerable even if you are using a private repository like the Artifactory. Always use Secrets or ConfigMaps to provision passwords or inject them by mounting a persistent volume.\nUsing the \u0026lsquo;latest\u0026rsquo; tag Starting an image with tomcat is tempting. If no tags are specified, a container is started with the tomcat:latest image. This image may no longer be up to date and refers to an older version instead. Running a production application requires complete control of the environment with exact versions of the image. Make sure you always use a tag or even better the sha256 hash of the image e.g. tomcat@sha256:c34ce3c1fcc0c7431e1392cc3abd0dfe2192ffea1898d5250f199d3ac8d8720f. Why use the sha256 hash? Tags are not immutable and can be overwritten by a developer at any time. In this case you don't have complete control over your image - which is bad.\nDifferent images per environment Don't create different images for development, testing, staging and production environments. The image should be the source of truth and should only be created once and pushed to the repository. This image:tag should be used for different environments in the future.\nDepend on start order of pods Applications often depend on containers being started in a certain order. For example, a database container must be up and running before an application can connect to it. The application should be resilient to such changes, as the db pod can be unreachable or restarted at any time. The application container should be able to handle such situations without terminating or crashing.\nAdditional anti-patterns and patterns\u0026hellip; In the community vast experience have been collected to improve stability and usability of Docker and Kubernetes. Refer to the following link for more information\n Kubernetes Production Patterns  "
},
{
	"uri": "https://gardener.cloud/blog/2018_week_06/",
	"title": "Kubernetes is available in Docker for Mac 17.12 CE",
	"tags": [],
	"description": "",
	"content": "\r\r\u0026lt;img src=\u0026quot;../blog/2018_week_06/blog-kubernetes-enable.png\u0026quot; title=\u0026quot;logo\u0026quot; width=\u0026quot;100%\u0026quot; class=\u0026quot;drop-shadow reveal-fast\u0026quot; style=\u0026quot;\u0026quot;/\u0026gt;\r\u0026lt;/td\u0026gt;\r\u0026lt;td valign=\u0026quot;top\u0026quot;\u0026gt;\r\u0026lt;div\u0026gt;\r\u0026lt;b\u0026gt;Kubernetes is only available in Docker for Mac 17.12 CE and higher\u0026lt;/b\u0026gt; on the Edge channel. Kubernetes support is not included in Docker for Mac Stable releases. To find out more about Stable and Edge channels and how to switch between them, see \u0026lt;a href=\u0026quot;https://docs.docker.com/docker-for-mac/#general\u0026quot;\u0026gt;general configuration\u0026lt;/a\u0026gt;.\r\u0026lt;/div\u0026gt;\r\u0026lt;/td\u0026gt;\r  \rDocker for Mac 17.12 CE (and higher) Edge includes a standalone Kubernetes server that runs on Mac, so that you can test deploying your Docker workloads on Kubernetes.\rThe Kubernetes client command, kubectl, is included and configured to connect to the local Kubernetes server. If you have kubectl already installed and pointing to some other environment, such as minikube or a GKE cluster, be sure to change context so that kubectl is pointing to docker-for-desktop:\n\u0026hellip;see more on Docker.com\nI recommend to setup your shell to see which KUBECONFIG is active.\n"
},
{
	"uri": "https://gardener.cloud/components/kubify/",
	"title": "kubify",
	"tags": [],
	"description": "",
	"content": "Kubify Kubify is a Terraform based provisioning project for setting up production ready Kubernetes clusters on public and private Cloud infrastructures. Kubify currently supports:\n OpenStack AWS Azure  Key features of Kubify are:\n Kubernetes v1.10.12 Etcd v3.3.10 multi master node setup Etcd backup and restore Supports rolling updates   To start using or developing Kubify locally See our documentation in the /docs repository or find the main documentation here.\nFeedback and Support Feedback and contributions are always welcome. Please report bugs or suggestions about our Kubernetes clusters as such or the Kubify itself as GitHub issues or join our Slack channel #gardener (Invite yourself to the Kubernetes Slack workspace here).\n"
},
{
	"uri": "https://gardener.cloud/documentation/050-tutorials/content/howto/landscape-setup/",
	"title": "Landscape Setup",
	"tags": [],
	"description": "",
	"content": "\u0026mdash;DEPRECATED\u0026mdash; This project is outdated and won't be updated anymore. Please use https://github.com/gardener/garden-setup instead!\nGardener Setup Scripts This README is the installation manual for a simple Gardener setup. The installation scripts in this repo are embedded in a configuration template in the landscape-setup-template project. You can find further information there.\nWe do recommend this simplified setup for demonstration purposes only. For productive workloads we do recommend that all components (Gardener/Seed/Shoot) run in their own IaaS accounts and that network policies are enabled and properly tested on the seed clusters. A documentation on how to do this is currently work in progress.\n Gardener Setup Scripts Prerequisites Gardener Installation  TL;DR  Kubectl Aliases   Step 1: Clone the Repositories and get Dependencies  Submodule Management   Step 2: Configure the Landscape  Building the \u0026lsquo;landscape.yaml\u0026rsquo; File The Base Cluster  Kubify Shoot Cluster Using an Arbitrary Base Cluster     Step 3: Build and Run Docker Container Step 4-10: Deploying Components  Undeploying Components The \u0026lsquo;all\u0026rsquo; Component   Step 4-10: Deploying Components (detailed)  Step 4: Kubify / etcd Step 5: Generate Certificates Step 6: Deploy tiller Step 7: Deploy Gardener Step 8: Register Garden Cluster as Seed Cluster  Configuring Additional Seeds Creating a Shoot   Step 9: Install Identity and Dashboard  Create CNAME Entry   Step 10: Apply Valid Certificates Letsencrypt Quota Limits  Accessing the Dashboard       Tearing Down the Landscape Cleanup  Prerequisites Before getting started make sure you have the following at hand:\n You need a cloud account with sufficient quota to set up a Kubernetes cluster with a couple of VMs. The Gardener supports AWS, Azure, GCP, and Openstack, but this simplified setup currently only supports AWS and Openstack. A Linux machine (virtual machine is fine) or a Mac with basic tools such as a git client and the Docker runtime installed.  Gardener Installation Follow these steps to install Gardener. Do not proceed to the next step in case of errors.\nTL;DR If you are already familiar with the installation procedure and just want a short summary of the commands you have to use, here it is:\n# setup\rgit clone --recursive https://github.com/gardener/landscape-setup-template.git landscape\r# fill in landscape/landscape_config.yaml now\rcd landscape/setup\r./docker_run.sh\rdeploy all\r# -------------------------------------------------------------------\r# teardown\rundeploy all\r./cleanup.sh\rOtherwise, follow the detailed guide below.\nKubectl Aliases The following aliases can be used within the docker container:\nk =\u0026gt; kubectl\rks =\u0026gt; kubectl -n kube-system\rkg =\u0026gt; kubectl -n garden\rkn =\u0026gt; kubectl -n\rka =\u0026gt; kubectl get --all-namespaces\rBash completion works for all of them except for ka.\nStep 1: Clone the Repositories and get Dependencies Get the landscape-setup-template from GitHub and initialize the submodules:\ngit clone --recursive https://github.com/gardener/landscape-setup-template.git landscape\rcd landscape\rAfter step 2, this repository will contain all passwords and keys for your landscape. You will be in trouble if you loose them so we recommend that you store this landscape configuration in a private repository. It might be a good idea to change the origin so you do not accidentally publish your secrets to the public template repository.\nSubmodule Management This project needs the Gardener and dashboard as submodules. To avoid conflicts between the checked out versions and the ones specified in the landscape_base.yaml file, automatic version management has been added. As long as the managed field in the chart area of each submodule is set to true, the version specified in the tag field will be checked out before deploying.\nTo check vor the correct version, the VERSION file in the submodule's main folder is read and compared to the tag in the config file. If the VERSION file doesn't exist, the component is added as a submodule. If it exists, but the versions differ, the fitting version will be checked out. If it exists and the versions are identical, nothing is done. The automatic version management will fail if a) the component is already added as a submodule, but the VERSION file is missing or b) the landscape folder is not a git repo.\nIt is also possible to trigger the version update manually: call the manage_submodule function with gardener or dashboard as an argument, or run the manage_submodules.sh script which will update Gardener and dashboard. Both will only work from inside the docker container / with sourced init.sh file.\nStep 2: Configure the Landscape There is a landscape_config.yaml file in the landscape project. This is the only file that you need to modify - all other configuration files will be derived from this and the landscape_base.yaml file. The latter one contains the merging instructions as well as technical configurations and it shouldn't be touched unless you know what you are doing.\nBuilding the \u0026lsquo;landscape.yaml\u0026rsquo; File Both config files - landscape_config.yaml and landscape_base.yaml - are merged into one landscape.yaml file which is then used as configuration for the scripts. Sourcing the init.sh file (which happens automatically when entering the docker image) will perform this merge unless the file already exists. This means if you change something in one of the original config files after the landscape.yaml file has already been created, you need to manually rebuild it in order for the changes to take effect.\n./build_landscape_yaml.sh\rThis script will recreate the landscape.yaml file. It will also source the init.sh file again, as some of the environment variables are extracted from this file.\nThe Base Cluster Gardener extends the Kubernetes apiserver, so in order to deploy it, you need a Kubernetes cluster first. This setup gives you two options for this:\nKubify You can use Kubify to create the initial cluster. Kubify uses Terraform to create the cluster and it is integrated into this project - you don't need to create the cluster yourself, just make sure you fill out all relevant parts of the config file.\nShoot Cluster A shoot cluster is a cluster created by a Gardener instance and it can be used as a base cluster for this project. These flags have to be set for the kube-apiserver (if not set, the Gardener will still work, but the dashboard won't):\n--oidc-issuer-url=https://identity.ingress.\u0026lt;your cluster domain\u0026gt;\r--oidc-client-id=kube-kubectl\r --oidc-username-claim=email\r--oidc-groups-claim=groups\rFor a shoot this can be done by setting issuerUrl, clientID, usernameClaim, and groupsClaim in spec.kubernetes.kubeAPIServer.oidcConfig in the shoot manifest.\nAlso make sure that the CIDRs of your base cluster and the from your Gardener spawned shoots don't overlap - if you want to be able to create shoots from the Gardener dashboard later, then don't use the default CIDRs for this base cluster.\nSome fields in the landscape_config.yaml are marked with # kubify only, they can be ignored when using a shoot as the base cluster. The etcd server address defaults to the address for Kubify and needs to be changed for the shoot setup (see the comments in the config file).\nThe kubeconfig for the base cluster is expected to be named kubeconfig and reside in the directory containing this project's directory (next to the landscape_config.yaml file).\nUsing an Arbitrary Base Cluster While this setup has only been tested for clusters created by Kubify or the Gardener (shoot clusters), it is theoretically possible to use the shoot setup method to deploy the Gardener to an arbitrary kubernetes cluster.\nStep 3: Build and Run Docker Container First, cd into the folder containing this project.\nThen run the container:\n./docker_run.sh\rAfter this,\n you will be connected to the container via an interactive shell the landscape folder will be mounted in that container your current working directory will be setup folder setup/init.sh is sourced, meaning  the environment variables will be set kubectl will be configured to communicate with your cluster landscape.yaml file will have been created if it didn't exist before    The docker_run.sh script searches for the image locally and pulls it from an image repository, if it isn't found. If pulling the image doesn't work for whatever reason, you can use the docker_build.sh script to build the image locally.\nStep 4-10: Deploying Components The Gardener deployment is splitted into components. A single component can be easily deployed using\ndeploy \u0026lt;component name\u0026gt;\rPlease take care that most of the components depend on each other and therefore have to be deployed in the order given below.\nThe deploy command is added to the PATH environment variable and can thus be called from anywhere. Bash auto-completion can be used for the component names.\nUndeploying Components It is also possible to \u0026ldquo;undeploy\u0026rdquo; a component using\nundeploy \u0026lt;component name\u0026gt;\rComponents need to be undeployed in the inverse order. Do not undeploy components without undeploying their successors in the component order first. Take care to delete all shoots before undeploying the gardener or seed-config components (although both undeploy scripts will check for that and trigger a deletion themselves).\nThe \u0026lsquo;all\u0026rsquo; Component The all component is a special component: it serves as a dummy to deploy several components in one go. Usually, manual intervention between deploying components is not necessary and most of them are deployed directly one after the other, so the all component makes the \u0026ldquo;normal\u0026rdquo; use-case easier.\nFor better control which components are deployed, a component range can be given as an argument. The argument should have the form \u0026lt;start component name\u0026gt;:\u0026lt;end component name\u0026gt; and then the start component, the end component, and all components in between will be deployed. The order of the arguments is taken from the environment variables with the $COMPONENT_ORDER_ prefix. The variable with the suffix that matches the clusters.base_cluster entry in the config file will be used.\nIt is also possible to drop one part of the range or to drop the whole argument. For missing parts the defaults will be used, which are the first and the last component of the active component order, respectively.\nThe undeploy command can also be used with the all component, but take care that the component order is inverted.\n# Examples\r# (start and end component are always inclusive)\rdeploy all # deploys all components\rdeploy all gardener:dashboard # deploys \u0026#39;gardener\u0026#39; through \u0026#39;dashboard\u0026#39; deploy all gardener: # deploys all components starting from \u0026#39;gardener\u0026#39;\rdeploy all :gardener # deploys all components up to \u0026#39;gardener\u0026#39;\r# (all undeploy commands use the inverse component order)\rundeploy all # undeploys all components\rundeploy all :helm-tiller # undeploys all components up to \u0026#39;helm-tiller\u0026#39;\rundeploy all dashboard:cert # undeploys \u0026#39;dashboard\u0026#39; through \u0026#39;cert\u0026#39;\rStep 4-10: Deploying Components (detailed) Step 4: Kubify / etcd If you want to create a Kubify cluster, deploy the component:\ndeploy kubify\rThe script will wait some time for the cluster to come up and then partly validate that the cluster is ready.\nIf you get errors during the cluster setup, just try to run the command again.\nOnce completed the following command should show all deployed pods:\nroot@c41327633d6d:/landscape# kubectl get pods --all-namespaces\rNAMESPACE NAME READY STATUS RESTARTS AGE\rkube-system etcd-operator-75dcfcf4f7-xkm4h 1/1 Running 0 6m\rkube-system heapster-c8fb4f746-tvts6 2/2 Running 0 2m\rkube-system kube-apiserver-hcdnc 1/1 Running 0 6m\r[...]\r If you already have a cluster, you don't need Kubify. To deploy an etcd in your cluster, run\ndeploy etcd\rThis component is not meant to be used in combination with Kubify and might require manual steps to make it work.\nIt should also be possible to plug in your own etcd - check the deploy scripts for the etcd and gardener components for information on where to put the certificates, etc.\nStep 5: Generate Certificates This step will generate a self-signed cluster CA and sign some certificates with it.\ndeploy cert\rStep 6: Deploy tiller Tiller is needed to deploy the Helm charts of Gardener and other components.\ndeploy helm-tiller\rStep 7: Deploy Gardener Now we can deploy Gardener. If the previous steps were executed successfully this should be completed in a couple of seconds.\ndeploy gardener\rYou might see a couple of messages like these:\nGardener API server not yet reachable. Waiting...\rwhile the script waits for the Gardener to start. Once Gardener is up when the deployment script finished you can verify the correct setup by running the following command:\nkubectl get shoots\rNo resources found. As we do not have a seed cluster yet we cannot create any shoot clusters. The Gardener itself is installed in the garden namespace:\nkubectl get po -n garden\rNAME READY STATUS RESTARTS AGE\rgardener-apiserver-56cc665667-nvrjl 1/1 Running 0 6m\rgardener-controller-manager-5c9f8db55-hfcts 1/1 Running 0 6m\rStep 8: Register Garden Cluster as Seed Cluster In heterogeneous productive environments one would run Gardener and seed in separate clusters but for simplicity and resource consumption reasons we will register the Gardener cluster that we have just created also as the seed cluster. Make sure that the seed_config in the landscape file is correct and matches the region that you are using. Keep in mind that image ids differ between regions as well. Also, valid credentials for the seed provider have to be specified in the authentication part of the landscape_config.yaml file (the etcd backups of the shoot clusters are stored on the seed).\ndeploy seed-config\rConfiguring Additional Seeds By default, this step will create a seed for the cloud provider the Gardener has been deployed on and thus creating shoots on this provider will be possible. If you want to create shoots on other cloud providers, you will have to configure additional seeds. There are two options for that:\nIf the seed-config deploy script is called without any arguments (as shown above), it will create seeds for all providers specified in the seed_config.seeds section in the landscape_config.yaml file. By default, the only entry in that list is the cloud provider chosen for the Gardener cluster, but you can extend the list.\nIt is also possible to provide the seed-config deploy script with additional arguments specifying which seeds should be created. Multiple arguments can be given and the script will ignore the list in the landscape_config.yaml file when called with arguments. Only the specified seeds will be created, already existing seeds are not affected. If a given seed already exists, it will be updated to the current configuration.\nIn both cases, the corresponding variant nodes in authentication and seed_config have to be filled out in the config file.\nValid values for seeds are aws, az (for Azure), gcp, and openstack. Please note, that while it is possible to create seeds for any cloud provider on any cloud provider, shoot creation may not work across cloud providers for every combination. It should always work if seed (Gardener cluster in this setup) and shoot are on the same provider, though.\nCreating a Shoot That's it! If everything went fine you should now be able to create shoot clusters. You can start with a sample manifest and create a shoot cluster by standard Kubernetes means:\nkubectl apply -f shoot-aws.yaml\rStep 9: Install Identity and Dashboard Creating clusters based on a shoot manifest is quite nice but also a little complex. While almost all aspects of a shoot cluster can be configured, it can be quite difficult for beginners, so go on and install the dashboard:\ndeploy identity\r[...]\rdeploy dashboard\r[...]\rCreate CNAME Entry Dashboard and identity need a CNAME entry pointing the domain *.ingress.\u0026lt;your cluster domain\u0026gt; to your cluster's nginx ingress ip/hostname. Kubify creates this entry automatically. If you are not using kubify to create your base cluster, you can create the CNAME entry with the corresponding component:\ndeploy cname\rThe script uses the AWS CLI to create the entry, so it will only work for route53.\nStep 10: Apply Valid Certificates The following command will install the cert-manager and request valid letsencrypt certificates for both the identity and dashboard ingresses:\ndeploy certmanager\rAfter a few minutes valid certificates should be installed.\nLetsencrypt Quota Limits Letsencrypt limits how many certificates you can get for the same host within a short time. To avoid hitting these limits, you can use the letsencrypt staging server for testing, which has a significantly higher rate limit but produces untrusted certificates.\nThe charts.[certmanager].live field in the config file allows to switch between live and staging server (remember to rebuild the landscape.yaml file after you changed something in the landscape_config.yaml file).\nAccessing the Dashboard After step 9 you will be able to access the Gardener dashboard. There is a difference in how you access it depending on whether you used the letsencrypt live server or the staging one (and thus have untrusted credentials in the latter case).\nThe print_dashboard_urls.sh script constructs two URLs from the domain name given in the landscape.yaml file and prints them.\nIf you have trusted certificates, just use the second one (the one for the dashboard) and everything will be fine.\nIf you used the letsencrypt staging server, you will need to visit the first link first. Your browser will show a warning regarding untrusted certificates, you need to ignore that warning. You will then see a nearly blank page with some 404 message. After that, you can open the dashboard link, ignore the certificate warning again and should be able to login. If you skip the first link, you will still be able to see the dashboard, but the login button probably won't work. While you will be able to login and create projects with the untrusted certificates from the letsencrypt staging server, creating secrets or shoots won't be possible. You'll need trusted certificates for that.\nTo log into the dashboard, use the options you have specified in the identity chart part of the landscape_config.yaml.\nTearing Down the Landscape Make sure that you delete all shoot clusters prior to tearing down the cluster. Not deleting project resources before deleting the Gardener can also cause troubles, because the namespaces associated with the projects have a finalizer which can't be handled anymore when the Gardener is gone. Both, shoots and projects, can be deleted using the delete_all.sh script (give \u0026lsquo;shoots\u0026rsquo; or \u0026lsquo;projects\u0026rsquo; as an argument). To delete a single shoot/project, use this script.\nThe following command should not return any shoot clusters:\nkubectl get shoots --all-namespaces\rNo resources found.\rIf you created your base cluster with the Kubify component, you can destroy it using the undeploy command:\nundeploy kubify\rCleanup After destroying the Kubify cluster, there will be some files left that prevent you from simply starting the project up again.\nATTENTION: Only do this if you are sure the cluster has been completely destroyed! Since this removes the terraform state, an automated deletion of resources won't be possible anymore - you will have to clean up any leftovers manually.\n./cleanup.sh\rThis will reset your landscape folder to its initial state (including the deletion of landscape.yaml).\nThe script takes an optional \u0026ldquo;-y\u0026rdquo; argument to skip the confirmation.\n"
},
{
	"uri": "https://gardener.cloud/components/mcm/",
	"title": "Machine Controller Manager",
	"tags": [],
	"description": "",
	"content": "machine-controller-manager  \nMachine Controller Manager (MCM) manages VMs as another kubernetes custom resource. It provides a declarative way to manage VMs. The current implementation supports AWS, GCP, Azure, Alicloud, Packet and Openstack. It can easily be extended to support other cloud providers as well.\nExample of managing machine:\nkubectl create/get/delete machine vm1\rKey terminologies Nodes/Machines/VMs are different terminologies used to represent similar things. We use these terms in the following way\n VM: A virtual machine running on any cloud provider. Node: Native kubernetes node objects. The objects you get to see when you do a \u0026ldquo;kubectl get nodes\u0026rdquo;. Although nodes can be either physical/virtual machines, for the purposes of our discussions it refers to a VM. Machine: A VM that is provisioned/managed by the Machine Controller Manager.  Design of Machine Controller Manager See the design documentation in the /docs/design repository, please find the design doc here.\nTo start using or developing the Machine Controller Manager See the documentation in the /docs repository, please find the index here.\nCluster-api Implementation  cluster-api branch of machine-controller-manager implements the machine-api aspect of the cluster-api project. Link: https://github.com/gardener/machine-controller-manager/tree/cluster-api Once cluster-api project gets stable, we may make master branch of MCM as well cluster-api compliant, with well-defined migration notes.  "
},
{
	"uri": "https://gardener.cloud/documentation/050-tutorials/content/howto/maintain-shoot/",
	"title": "Maintain a Shoot cluster",
	"tags": [],
	"description": "Maintain a Shoot cluster",
	"content": "Maintain Shoot Cluster Day two operations like updating Kubernetes patch version (if the auto-update is enabled) and updating Operating system version happen in the maintenance time window of the Shoot cluster. The maintenance time window is part of the shoot spec (.spec.maintenance.timeWindow). If it is not specified during Shoot creation, Gardener will default to a randomized time window (to spread the load). The time interval cannot be less than 30 minutes and more than 6 hours.\nTo trigger the maintenance operation, you can annotate the Shoot with shoot.gardener.cloud/operation: maintain.\nKubernetes Patch Version If a Shoot has .spec.maintenance.autoUpdate.kubernetesVersion: true in the manifest, and you update the .spec.kubernetes.versions field in the CloudProfile used in the Shoot, then the maintenance controller will apply Kubernetes patch releases updates automatically during the maintenance time window.\nSince Kubernetes follows Semantic Versioning, if indicated so, Gardener will automatically apply the patch release updates. But it will never auto update the Major or Minor releases since there is no effort to keep backward compatibility in those releases.\nMajor or Minor updates must be handled by updating the .spec.kubernetes.version field manually, these updates will be executed immediately and will not wait for maintenance time window. Before applying such update on Minor or Major releases, operators should check for all the breaking changes introduced in the target release Changelog.\nE.g. If you have a Shoot cluster with the field values below (only related fields are shown):\nspec:\rkubernetes:\rversion: 1.10.0\rmaintenance:\rtimeWindow:\rbegin: 220000+0000\rend: 230000+0000\rautoUpdate:\rkubernetesVersion: true\rIf you update the CloudProfile used in the Shoot and add 1.10.5 and 1.11.0 to the .spec.kubernetes.versions list, the Shoot will be updated to 1.10.5 between 22:00-23:00 UTC. Your Shoot won't be updated to 1.11.0 even though its the highest Kubernetes version in the CloudProfile. This is because that wouldn't be a patch release update but a minor release update, and potentially have breaking changes that could impact your deployed resources.\nIn this example if the operator wants to update the Kubernetes version to 1.11.0, he/she must update the Shoot's .spec.kubernetes.version to 1.11.0 manually.\nKubernetes Version Expiration Date Gardener administrators can also specify expiration dates for the Kubernetes versions in the CloudProfile. Kubernetes version expiration dates allow smoother transitions for Shoot owners giving them time for testing before the actual Kubernetes version update happens. Expiration date for the latest Kubernetes version in the CloudProfile is not allowed.\nWe can check the following scenarios for better understanding on Kubernetes version expiration dates:\n Automatic patch update from expired Kubernetes version.  Let's assume the following CloudProfile spec (only related fields are shown):\nspec:\rkubernetes:\rversions:\r- version: 1.12.8\r- version: 1.11.10\r- version: 1.10.13\r- version: 1.10.12\rexpirationDate: \u0026#34;2019-04-13T08:00:00Z\u0026#34;\rAnd let's the Shoot has the following spec:\nspec:\rkubernetes:\rversion: 1.10.12\rmaintenance:\rtimeWindow:\rbegin: 220000+0100\rend: 230000+0100\rautoUpdate:\rkubernetesVersion: false\rThe Shoot refers a Kubernetes version that has an expirationDate. In the maintenance window on 2019-04-12 the Kubernetes version will stay the same as it is still not expired. But in the maintenance window on 2019-04-14 the Kubernetes version of the Shoot will be updated to 1.10.13 (no matter the value of .spec.maintenance.autoUpdate.kubernetesVersion).\n Automatic patch update from dropped Kubernetes version:  Let's assume the following CloudProfile spec (only related fields are shown):\nspec:\rkubernetes:\rversions:\r- version: 1.12.8\r- version: 1.11.10\r- version: 1.10.13\rAnd let's the Shoot has the following spec:\nspec:\rkubernetes:\rversion: 1.10.12\rmaintenance:\rtimeWindow:\rbegin: 220000+0100\rend: 230000+0100\rautoUpdate:\rkubernetesVersion: true\rThe Shoot refers a Kubernetes version that was dropped from the CloudProfile. In the upcoming maintenance window the Kubernetes version of the Shoot will be updated to the next patch version - 1.10.13. .spec.maintenance.autoUpdate.kubernetesVersion needs to be true, otherwise no version update will happen.\nOperating System Version If a Shoot has .spec.maintenance.autoUpdate.machineImageVersion: true in the manifest, and you update the .spec.machineImages field in the CloudProfile used in the Shoot, then the maintenance controller will apply the new machine image to the Shoot spec (and will mark the Shoot to be reconciled) during the maintenance time window. During the reconciliation the corresponding \u0026lt;Provider\u0026gt;MachineClass resource in the Shoot namespace in the Seed will be updated and the machine controller manager will take care of the actual state to match the desired one.\nMachine Image Expiration Date  Automatic update from expired machine image version.  Let's assume the following CloudProfile spec (only related fields are shown):\nspec:\rmachineImages:\r- name: coreos\rversions:\r- version: 2191.5.0\r- version: 2191.4.1\r- version: 2135.6.0\rexpirationDate: \u0026#34;2019-04-13T08:00:00Z\u0026#34;\rAnd let's the Shoot has the following spec:\nspec:\rprovider:\rtype: aws\rworkers:\r- name: name\rmaximum: 1\rminimum: 1\rmaxSurge: 1\rmaxUnavailable: 0\rimage:\rname: coreos\rversion: 2135.6.0\rtype: m5.large\rvolume:\rtype: gp2\rsize: 20Gi\rmaintenance:\rtimeWindow:\rbegin: 220000+0100\rend: 230000+0100\rautoUpdate:\rmachineImageVersion: false\rThe Shoot refers a machine image version that has an expirationDate. In the maintenance window on 2019-04-12 the machine image version will stay the same as it is still not expired. But in the maintenance window on 2019-04-14 the machine image version of the Shoot will be updated to 2191.5.0 (no matter the value of .spec.maintenance.autoUpdate.machineImageVersion) as version 2135.6.0 will be expired.\n"
},
{
	"uri": "https://gardener.cloud/blog/2019_week_06/",
	"title": "Manually adding a node to an existing cluster",
	"tags": [],
	"description": "",
	"content": "Gardener has an excellent ability to automatically scale machines for the cluster. From the point of view of scalability, there is no need for manual intervention.\n\rThis tutorial is useful for those end-users who need specifically configured nodes, which are not yet supported by Gardener. For example: an end-user who wants some workload that requires runnc instead of runc as container runtime.\n..read some more on Adding Nodes to a Cluster.\n"
},
{
	"uri": "https://gardener.cloud/documentation/050-tutorials/content/howto/add-node-to-cluster/",
	"title": "Manually adding a node to an existing cluster",
	"tags": [],
	"description": "This document describes steps on how to add a node to an existing cluster without the support of Gardener",
	"content": "Manually adding a node to an existing cluster Gardener has an excellent ability to automatically scale machines for the cluster. From the point of view of scalability, there is no need for manual intervention.\nThis tutorial is useful for those end-users who need specifically configured nodes, which are not yet supported by Gardener. For example: an end-user who wants some workload that requires runnc instead of runc as container runtime.\nDisclaimer  Here we will look at the steps on how to add a node to an existing cluster without the support of Gardener. Such a node will not be managed by Gardener, and if it goes down for any reason, Gardener will not be responsible to replace it.\n How   Create a new instance in the same VPC/network as other machines in the cluster. You should be able to ssh into the machine. So save its private key, and assign a public IP to it. If adding a public IP is not preferred, then ssh into any other machine in the cluster, and then ssh from there into the new machine using its private key.\nTo ssh into a machine which is already in the cluster, use the steps defined here.\nAttach the same IAM role to the new machine which is attached to the existing machines in the cluster. This is required by kubelet in the new machine so that it can contact the cloud provider to query the node's name.\n  On the new machine, create file /var/lib/kubelet/kubeconfig-bootstrap with the following content:\n  apiVersion: v1\rkind: Config\rcurrent-context: kubelet-bootstrap@default\rclusters:\r- cluster:\rcertificate-authority-data: \u0026lt;CA Certificate\u0026gt;\rserver: \u0026lt;Server\u0026gt;\r name: default\rcontexts:\r- context:\rcluster: default\ruser: kubelet-bootstrap\rname: kubelet-bootstrap@default\rusers:\r- name: kubelet-bootstrap\ruser:\ras-user-extra: {}\rtoken: \u0026lt;Token\u0026gt;\rssh into an existing node, and run these commands to get the values of and  to be replaced in above file:   \u0026lt;Servr\u0026gt;  /opt/bin/hyperkube kubectl \\\r --kubeconfig /var/lib/kubelet/kubeconfig-real \\\r config view \\\r -o go-template=\u0026#39;{{index .clusters 0 \u0026#34;cluster\u0026#34; \u0026#34;server\u0026#34;}}\u0026#39; \\\r --raw\r \u0026lt;CA Certificate\u0026gt;  /opt/bin/hyperkube kubectl \\\r --kubeconfig /var/lib/kubelet/kubeconfig-real \\\r config view \\\r -o go-template=\u0026#39;{{index .clusters 0 \u0026#34;cluster\u0026#34; \u0026#34;certificate-authority-data\u0026#34;}}\u0026#39; \\\r --raw\r \u0026lt;Token\u0026gt;\nThe kubelet on the new machine needs a bootstrap token to authenticate with the kube-apiserver when adding itself to the cluster. Kube-apiserver uses a secret in the kube-system namespace to authenticate this token. This token is valid for 90 minutes from the time of creation, and the corresponding secret captures this detail in its .data.expiration field. The name of this secret is of the format bootstrap-token-*. Gardener takes care of creating new bootstrap tokens, and the corresponding secrets. To get an unexpired token, find the secrets with the name format bootstrap-token-* in the kube-system namespace in the cluster, and pick the one with minimum age. Eg. bootstrap-token-abcdef.\nRun these commands to get the token:\ntokenid=$(kubectl get secret bootstrap-token-abcdef -n kube-system -o go-template=\u0026#39;{{index .data \u0026#34;token-id\u0026#34;}}\u0026#39; | base64 --decode)\rtokensecret=$(kubectl get secret bootstrap-token-abcdef -n kube-system -o go-template=\u0026#39;{{index .data \u0026#34;token-secret\u0026#34;}}\u0026#39; | base64 --decode)\recho $tokenid.$tokensecret\rThe value of $TOKEN will be tokenid.tokensecret. Replace $TOKEN in above file with this value\n  Copy contents of the files - /var/lib/kubelet/config/kubelet, /var/lib/kubelet/ca.crt and /etc/systemd/system/kubelet.service - from an existing node to the new node\n  Run the following command in the new node to start the kubelet:\n  systemctl enable kubelet \u0026amp;\u0026amp; systemctl start kubelet\rThe new node should be added to the existing cluster within a couple of minutes.\n"
},
{
	"uri": "https://gardener.cloud/documentation/045_contribute/20_documentation/25_markup/",
	"title": "Markdown",
	"tags": [],
	"description": "",
	"content": "Hugo uses Markdown for its simple content format. However, there are a lot of things that Markdown doesn’t support well. You could use pure HTML to expand possibilities.\nBut this happens to be a bad idea. Everyone uses Markdown because it's pure and simple to read even non-rendered. You should avoid HTML to keep it as simple as possible.\nTo avoid this limitations, Hugo created shortcodes. A shortcode is a simple snippet inside a page.\nGardener provides multiple shortcodes on top of existing ones.\n\rAttachments\r\rThe Attachments shortcode displays a list of files attached to a page.\n\rButton\r\rNice buttons on your page.\n\rExpand\r\rDisplays an expandable/collapsible section of text on your page\n\rMermaid\r\rGeneration of diagram and flowchart from text in a similar manner as markdown\n\rNotice\r\rDisclaimers to help you structure your page\n\r"
},
{
	"uri": "https://gardener.cloud/documentation/045_contribute/20_documentation/25_markup/mermaid/",
	"title": "Mermaid",
	"tags": [],
	"description": "Generation of diagram and flowchart from text in a similar manner as markdown",
	"content": "Mermaid is a library helping you to generate diagram and flowcharts from text, in a similar manner as Markdown.\nJust insert your mermaid code in the mermaid shortcode and that's it.\nFlowchart example {{\u0026lt;mermaid align=\u0026quot;left\u0026quot;\u0026gt;}}\rgraph LR;\rA[Hard edge] --\u0026gt;|Link text| B(Round edge)\rB --\u0026gt; C{Decision}\rC --\u0026gt;|One| D[Result one]\rC --\u0026gt;|Two| E[Result two]\r{{\u0026lt; /mermaid \u0026gt;}}\r renders as\ngraph LR;\rA[Hard edge] --|Link text| B(Round edge)\rB -- C{Decision}\rC --|One| D[Result one]\rC --|Two| E[Result two]\r\rSequence example {{\u0026lt;mermaid\u0026gt;}}\rsequenceDiagram\rparticipant Alice\rparticipant Bob\rAlice-\u0026gt;\u0026gt;John: Hello John, how are you?\rloop Healthcheck\rJohn-\u0026gt;John: Fight against hypochondria\rend\rNote right of John: Rational thoughts \u0026lt;br/\u0026gt;prevail...\rJohn--\u0026gt;Alice: Great!\rJohn-\u0026gt;Bob: How about you?\rBob--\u0026gt;John: Jolly good!\r{{\u0026lt; /mermaid \u0026gt;}}\r renders as\nsequenceDiagram\rparticipant Alice\rparticipant Bob\rAlice-John: Hello John, how are you?\rloop Healthcheck\rJohn-John: Fight against hypochondria\rend\rNote right of John: Rational thoughts prevail...\rJohn--Alice: Great!\rJohn-Bob: How about you?\rBob--John: Jolly good!\r\rGANTT Example {{\u0026lt;mermaid\u0026gt;}}\rgantt\rdateFormat YYYY-MM-DD\rtitle Adding GANTT diagram functionality to mermaid\rsection A section\rCompleted task :done, des1, 2014-01-06,2014-01-08\rActive task :active, des2, 2014-01-09, 3d\rFuture task : des3, after des2, 5d\rFuture task2 : des4, after des3, 5d\rsection Critical tasks\rCompleted task in the critical line :crit, done, 2014-01-06,24h\rImplement parser and jison :crit, done, after des1, 2d\rCreate tests for parser :crit, active, 3d\rFuture task in critical line :crit, 5d\rCreate tests for renderer :2d\rAdd to mermaid :1d\r{{\u0026lt; /mermaid \u0026gt;}}\r render as\ngantt\rdateFormat YYYY-MM-DD\rtitle Adding GANTT diagram functionality to mermaid\rsection A section\rCompleted task :done, des1, 2014-01-06,2014-01-08\rActive task :active, des2, 2014-01-09, 3d\rFuture task : des3, after des2, 5d\rFuture task2 : des4, after des3, 5d\rsection Critical tasks\rCompleted task in the critical line :crit, done, 2014-01-06,24h\rImplement parser and jison :crit, done, after des1, 2d\rCreate tests for parser :crit, active, 3d\rFuture task in critical line :crit, 5d\rCreate tests for renderer :2d\rAdd to mermaid :1d\r\r"
},
{
	"uri": "https://gardener.cloud/blog/2018_week_09/",
	"title": "Namespace Isolation",
	"tags": [],
	"description": "",
	"content": "\u0026hellip;or DENY all traffic from other namespaces\nYou can configure a NetworkPolicy to deny all traffic from other namespaces while allowing all traffic coming from the same namespace the pod is deployed to. There are many reasons why you may chose to configure Kubernetes network policies:\n  Isolate multi-tenant deployments\n  Regulatory compliance\n  Ensure containers assigned to different environments (e.g. dev/staging/prod) cannot interfere with each another\n  ..read on Namespace Isolation how to configure it.\n"
},
{
	"uri": "https://gardener.cloud/documentation/050-tutorials/content/howto/network-isolation/",
	"title": "Namespace Isolation",
	"tags": [],
	"description": "",
	"content": "\u0026hellip;or DENY all traffic from other namespaces\nYou can configure a NetworkPolicy to deny all the traffic from other namespaces while allowing all the traffic coming from the same namespace the pod was deployed into.\nThere are many reasons why you may chose to employ Kubernetes network policies:\n Isolate multi-tenant deployments Regulatory compliance Ensure containers assigned to different environments (e.g. dev/staging/prod) cannot interfere with each other  Kubernetes network policies are application centric compared to infrastructure/network centric standard firewalls. There are no explicit CIDRs or IP addresses used for matching source or destination IP’s. Network policies build up on labels and selectors which are key concepts of Kubernetes that are used to organize (for e.g all DB tier pods of an app) and select subsets of objects.\nExample We create two nginx HTTP-Servers in two namespaces and block all traffic between the two namespaces. E.g. you are unable to get content from namespace1 if you are sitting in namespace2.\nSetup the namespaces # create two namespaces for test purpose\rkubectl create ns customer1\rkubectl create ns customer2\r# create a standard HTTP web server\rkubectl run nginx --image=nginx --replicas=1 --port=80 -n=customer1\rkubectl run nginx --image=nginx --replicas=1 --port=80 -n=customer2\r# expose the port 80 for external access\rkubectl expose deployment nginx --port=80 --type=NodePort -n=customer1\rkubectl expose deployment nginx --port=80 --type=NodePort -n=customer2\r Test without NP Create a pod with curl preinstalled inside the namespace customer1\n# create a \u0026#34;bash\u0026#34; pod in one namespace\rkubectl run -i --tty client --image=tutum/curl -n=customer1\rtry to curl the exposed nginx server to get the default index.html page. Execute this in the bash prompt of the pod created above.\n# get the index.html from the nginx of the namespace \u0026#34;customer1\u0026#34; =\u0026gt; success\rcurl http://nginx.customer1\r# get the index.html from the nginx of the namespace \u0026#34;customer2\u0026#34; =\u0026gt; success\rcurl http://nginx.customer2\rBoth calls are done in a pod within namespace customer1 and both nginx servers are always reachable, no matter in what namespace.\n Test with NP Install the NetworkPolicy from your shell\napiVersion: networking.k8s.io/v1\rkind: NetworkPolicy\rmetadata:\rname: deny-from-other-namespaces\rspec:\rpodSelector:\rmatchLabels:\ringress:\r- from:\r- podSelector: {}\r it applies the policy to ALL pods in the named namespace as the spec.podSelector.matchLabels is empty and therefore selects all pods. it allows traffic from ALL pods in the named namespace, as spec.ingress.from.podSelector is empty and therefore selects all pods.  kubectl apply -f ./network-policy.yaml -n=customer1\rkubectl apply -f ./network-policy.yaml -n=customer2\rafter this curl http://nginx.customer2 shouldn't work anymore if you are a service inside the namespace customer1 and vice versa\nNote: This policy, once applied, will also disable all external traffic to these pods. For example you can create a service of type LoadBalancer in namespace customer1 that match the nginx pod. When you request the service by its \u0026lt;EXTERNAL_IP\u0026gt;:\u0026lt;PORT\u0026gt;, then the network policy will deny the ingress traffic from the service and the request will time out.\nMore You can get more information how to configure the NetworkPolicies on:\n Calico WebSite Kubernetes NP Recipes  "
},
{
	"uri": "https://gardener.cloud/blog/2018_week_08_2/",
	"title": "Namespace Scope",
	"tags": [],
	"description": "",
	"content": "Should I use:\n❌ one namespace per user/developer?\r❌ one namespace per team?\r❌ one per service type?\r❌ one namespace per application type?\r😄 one namespace per running instance of your application?\r\rApply the Principle of Least Privilege\nAll user accounts should run at all times as few privileges as possible, and also launch applications with as few privileges as possible. If you share a cluster for different user separated by a namespace, all user has access to all namespaces and services per default. It can happen that a user accidentally uses and destroys the namespace of a productive application or the namespace of another developer.\nKeep in mind: By default namespaces don't provide:\n Network isolation Access Control Audit Logging on user level  "
},
{
	"uri": "https://gardener.cloud/blog/2020_week_20/00/",
	"title": "New Website, Same Green Power",
	"tags": [],
	"description": "",
	"content": "The Gardener project website just received a serious facelift. Here are some of the highlights:\n A completely new landing page, emphasizing both on Gardener's value proposition and the open community behind it. The Community page was reconstructed for quick access to the various community channels and will soon merge the Adopters page. It will provide a better insight into success stories from the communty. A completely new News section and content type available at /documentation/news. Use metadata such as publishdate and archivedate to schedule for news publish and archive automatically, regardless of when you contributed them. You can now track what's happening from the landing page or in the dedicated News section on the website and share. Improved blogs layout. One-click sharing options are closely following up.  Website builds also got to a new level with:\n Containerization. The whole build environment is containerized now, eliminating differences between local and CI/CD setup and reducing content developers focus only to the /documentation repository. Running a local server for live preview of changes as you make them when developing content for the website, is now as easy as runing make serve in your local /documentation clone. Numerous improvements to the buld scripts. More configuration options, authenticated requests, fault tollerance and performance. Good news for Windows WSL users who will now nejoy a significantly support. See the updated README for details on that. A number of improvements in layouts styles, site assets and hugo site-building techniques.  But hey, THAT'S NOT ALL!\nStay tuned for more improvements around the corner. The biggest ones are aligning the documentation with the new theme and restructuring it along, more emphasis on community success stories all around, more sharing options and more than a handful of shortcodes for content development and \u0026hellip; let's cut the spoilers here.\nI hope you will like it. Let us know what you think about it. Feel free to leave comments and discuss on Twitter and Slack, or in case of issues - on GitHub.\nGo ahead and help us spread the word: https://gardener.cloud\nTweet\n\r\r"
},
{
	"uri": "https://gardener.cloud/documentation/045_contribute/20_documentation/25_markup/notice/",
	"title": "Notice",
	"tags": [],
	"description": "Disclaimers to help you structure your page",
	"content": "The notice shortcode shows 4 types of disclaimers to help you structure your page.\nNote {{% notice note %}}\rA notice disclaimer\r{{% /notice %}}\rrenders as\nA notice disclaimer\r\rInfo {{% notice info %}}\rAn information disclaimer\r{{% /notice %}}\rrenders as\nAn information disclaimer\r\rTip {{% notice tip %}}\rA tip disclaimer\r{{% /notice %}}\rrenders as\nA tip disclaimer\r\rWarning {{% notice warning %}}\rAn warning disclaimer\r{{% /notice %}}\rrenders as\nA warning disclaimer\r\r"
},
{
	"uri": "https://gardener.cloud/documentation/050-tutorials/content/howto/container-startup/",
	"title": "Orchestration of container startup",
	"tags": [],
	"description": "How to orchestrate startup sequence of multiple containers",
	"content": "Disclaimer If an application depends on other services deployed separately do not rely on a certain start sequence of containers but ensure that the application can cope with unavailability of the services it depends on.\nIntroduction Kubernetes offers a feature called InitContainers to perform some tasks during a pod's initialization. In this tutorial we demonstrate how to use it orchestrate starting sequence of multiple containers. The tutorial uses the example app url-shortener which consists of two components:\n postgresql database webapp which depends on postgresql database and provides two endpoints: create a short url from a given location, and redirect from a given short URL to the corresponding target location.  This app represents the minimal example where an application relies on another service or database. In this example, if the application starts before database is ready, the application will fail as shown below:\n$ kubectl logs webapp-958cf5567-h247n\rtime=\u0026#34;2018-06-12T11:02:42Z\u0026#34; level=info msg=\u0026#34;Connecting to Postgres database using: host=`postgres:5432` dbname=`url_shortener_db` username=`user`\\n\u0026#34;\rtime=\u0026#34;2018-06-12T11:02:42Z\u0026#34; level=fatal msg=\u0026#34;failed to start: failed to open connection to database: dial tcp: lookup postgres on 100.64.0.10:53: no such host\\n\u0026#34;\r$ kubectl get po -w\rNAME READY STATUS RESTARTS AGE\rwebapp-958cf5567-h247n 0/1 Pending 0 0s\rwebapp-958cf5567-h247n 0/1 Pending 0 0s\rwebapp-958cf5567-h247n 0/1 ContainerCreating 0 0s\rwebapp-958cf5567-h247n 0/1 ContainerCreating 0 1s\rwebapp-958cf5567-h247n 0/1 Error 0 2s\rwebapp-958cf5567-h247n 0/1 Error 1 3s\rwebapp-958cf5567-h247n 0/1 CrashLoopBackOff 1 4s\rwebapp-958cf5567-h247n 0/1 Error 2 18s\rwebapp-958cf5567-h247n 0/1 CrashLoopBackOff 2 29s\rwebapp-958cf5567-h247n 0/1 Error 3 43s\rwebapp-958cf5567-h247n 0/1 CrashLoopBackOff 3 56s\rIf the restartPolicy is set to Always (default) in yaml, the application will continue to restart the pod with an exponential back-off delay in case of failure.\nUsing InitContaniner To avoid such situation, InitContainers  can be defined which are executed prior to the application container. If one InitContainers fails, the application container won't be triggered.\napiVersion: apps/v1\rkind: Deployment\rmetadata:\rname: webapp\rspec:\rselector:\rmatchLabels:\rapp: webapp\rtemplate:\rmetadata:\rlabels:\rapp: webapp\rspec:\rinitContainers: # check if DB is ready, and only continue when true\r - name: check-db-ready\rimage: postgres:9.6.5\rcommand: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;until pg_isready -h postgres -p 5432; do echo waiting for database; sleep 2; done;\u0026#39;]\rcontainers:\r- image: xcoulon/go-url-shortener:0.1.0\rname: go-url-shortener\renv:\r- name: POSTGRES_HOST\rvalue: postgres\r- name: POSTGRES_PORT\rvalue: \u0026#34;5432\u0026#34;\r- name: POSTGRES_DATABASE\rvalue: url_shortener_db\r- name: POSTGRES_USER\rvalue: user\r- name: POSTGRES_PASSWORD\rvalue: mysecretpassword\rports:\r- containerPort: 8080\rIn above example, the InitContainers uses docker image postgres:9.6.5 which is different from the application container. This also brings the advantage of not having to include unnecessary tools (e.g. pg_isready) in the application container.\nWith introduction of InitContainers, the pod startup will look like following in case database is not available yet:\n$ kubectl get po -w\rNAME READY STATUS RESTARTS AGE\rnginx-deployment-5cc79d6bfd-t9n8h 1/1 Running 0 5d\rprivileged-pod 1/1 Running 0 4d\rwebapp-fdcb49cbc-4gs4n 0/1 Pending 0 0s\rwebapp-fdcb49cbc-4gs4n 0/1 Pending 0 0s\rwebapp-fdcb49cbc-4gs4n 0/1 Init:0/1 0 0s\rwebapp-fdcb49cbc-4gs4n 0/1 Init:0/1 0 1s\r$ kubectl logs webapp-fdcb49cbc-4gs4n\rError from server (BadRequest): container \u0026#34;go-url-shortener\u0026#34; in pod \u0026#34;webapp-fdcb49cbc-4gs4n\u0026#34; is waiting to start: PodInitializing\r"
},
{
	"uri": "https://gardener.cloud/documentation/045_contribute/20_documentation/10_organisation/",
	"title": "Organisation",
	"tags": [],
	"description": "",
	"content": "Content Organisation This site uses Hugo. In Hugo, content organization is a core concept.\n**Hugo Tip:** Start Hugo with `hugo server --navigateToChanged` for content edit-sessions.\r\rPage Lists Page Order The documentation side menu, the documentation page browser etc. are listed using Hugo's default sort order, which sorts by weight (from 1), date (newest first) and finally by the link title.\nGiven that, if you want to move a page or a section up, set a weight in the page's front matter:\ntitle: My Page\rweight: 10\rFor page weights, it can be smart not to use 1, 2, 3 ..., but some other interval, say 10, 20, 30... This allows you to insert pages where you want later.\r\r"
},
{
	"uri": "https://gardener.cloud/blog/2019_week_02/",
	"title": "Organizing Access Using kubeconfig Files",
	"tags": [],
	"description": "",
	"content": "The kubectl command-line tool uses kubeconfig files to find the information it needs to choose a cluster and communicate with the API server of a cluster.\n\r What happens if your kubeconfig file of your production cluster is leaked or published by accident?\n Since there is no possibility to rotate or revoke the initial kubeconfig, there is only one way to protect your infrastructure or application if it is has leaked - delete the cluster.\n..learn more on Work with kubeconfig files.\n"
},
{
	"uri": "https://gardener.cloud/documentation/050-tutorials/content/howto/working-with-kubeconfig/",
	"title": "Organizing Access Using kubeconfig Files",
	"tags": [],
	"description": " Organizing Access Using kubeconfig Files",
	"content": "Organizing Access Using kubeconfig Files The kubectl command-line tool uses kubeconfig files to find the information it needs to choose a cluster and communicate with the API server of a cluster.\nProblem If you've become aware of a security breach that affects you, you may want to revoke or cycle credentials in case anything was leaked. However, this is not possible with the initial or master kubeconfig from your cluster.\nPitfall Never distribute the kubeconfig, which you can download directly within the Gardener dashboard, for a productive cluster.\nCreate custom kubeconfig file for each user Create a separate kubeconfig for each user. One of the big advantages is, that you can revoke them and control the permissions better. A limitation to single namespaces is also possible here.\nThe script creates a new ServiceAccount with read privileges in the whole cluster (Secretes are excluded). To run the script jq, a lightweight and flexible command-line JSON processor, must be installed.\n#!/bin/bash\r\rif [[ -z \u0026#34;$1\u0026#34; ]] ;then\recho \u0026#34;usage: $0\u0026lt;username\u0026gt;\u0026#34;\rexit 1\rfi\ruser=$1\rkubectl create sa ${user}\rsecret=$(kubectl get sa ${user} -o json | jq -r .secrets[].name)\rkubectl get secret ${secret} -o json | jq -r \u0026#39;.data[\u0026#34;ca.crt\u0026#34;]\u0026#39; | base64 -D \u0026gt; ca.crt\ruser_token=$(kubectl get secret ${secret} -o json | jq -r \u0026#39;.data[\u0026#34;token\u0026#34;]\u0026#39; | base64 -D)\rc=`kubectl config current-context`\rcluster_name=`kubectl config get-contexts $c | awk \u0026#39;{print $3}\u0026#39; | tail -n 1`\rendpoint=`kubectl config view -o jsonpath=\u0026#34;{.clusters[?(@.name == \\\u0026#34;${cluster_name}\\\u0026#34;)].cluster.server}\u0026#34;`\r# Set up the config\rKUBECONFIG=k8s-${user}-conf kubectl config set-cluster ${cluster_name} \\\r --embed-certs=true \\\r --server=${endpoint} \\\r --certificate-authority=./ca.crt\rKUBECONFIG=k8s-${user}-conf kubectl config set-credentials ${user}-${cluster_name#cluster-} --token=${user_token}\rKUBECONFIG=k8s-${user}-conf kubectl config set-context ${user}-${cluster_name#cluster-} \\\r --cluster=${cluster_name} \\\r --user=${user}-${cluster_name#cluster-}\rKUBECONFIG=k8s-${user}-conf kubectl config use-context ${user}-${cluster_name#cluster-}\rcat \u0026lt;\u0026lt;EOF | kubectl create -f -\rapiVersion: rbac.authorization.k8s.io/v1\rkind: ClusterRoleBinding\rmetadata:\rname: view-${user}-global\rsubjects:\r- kind: ServiceAccount\rname: ${user}\rnamespace: default\rroleRef:\rkind: ClusterRole\rname: view\rapiGroup: rbac.authorization.k8s.io\rEOF\recho \u0026#34;done! Test with: \u0026#34;\recho \u0026#34;export KUBECONFIG=k8s-${user}-conf\u0026#34;\recho \u0026#34;kubectl get pods\u0026#34;\rIf edit or admin rights are to be assigned, the ClusterRoleBinding must be adapted in the roleRef section with the roles listed below.\nFurthermore, you can restrict this to a single namespace by not creating a ClusterRoleBinding but only a RoleBinding within the desired namespace.\n   Default ClusterRole Default ClusterRoleBinding Description     cluster-admin system:masters group Allows super-user access to perform any action on any resource. When used in a ClusterRoleBinding, it gives full control over every resource in the cluster and in all namespaces. When used in a RoleBinding, it gives full control over every resource in the rolebinding's namespace, including the namespace itself.   admin None Allows admin access, intended to be granted within a namespace using a RoleBinding. If used in a RoleBinding, allows read/write access to most resources in a namespace, including the ability to create roles and rolebindings within the namespace. It does not allow write access to resource quota or to the namespace itself.   edit None Allows read/write access to most objects in a namespace. It does not allow viewing or modifying roles or rolebindings.   view None Allows read-only access to see most objects in a namespace. It does not allow viewing roles or rolebindings. It does not allow viewing secrets, since those are escalating.    "
},
{
	"uri": "https://gardener.cloud/documentation/050-tutorials/content/howto/service-cache-control/",
	"title": "Out-Dated HTML and JS files delivered",
	"tags": [],
	"description": "Why is my application always outdated?",
	"content": "Problem After updating your HTML and JavaScript sources in your web application, the kubernetes cluster delivers outdated versions - why?\nPreamble By default, Kubernetes service pods are not accessible from the external network, but only from other pods within the same Kubernetes cluster.\nThe Gardener cluster has a built-in configuration for HTTP load balancing called Ingress, defining rules for external connectivity to Kubernetes services. Users who want external access to their Kubernetes services create an ingress resource that defines rules, including the URI path, backing service name, and other information. The Ingress controller can then automatically program a frontend load balancer to enable Ingress configuration.\nExample Ingress Configuration apiVersion: networking.k8s.io/v1beta1\rkind: Ingress\rmetadata:\rname: vuejs-ingress\rspec:\rrules:\r- host: test.ingress.\u0026lt;GARDENER-CLUSTER\u0026gt;.\u0026lt;GARDENER-PROJECT\u0026gt;.shoot.canary.k8s-hana.ondemand.com\rhttp:\rpaths:\r- backend:\rserviceName: vuejs-svc\rservicePort: 8080\rwhere:\n \u0026lt;GARDENER-CLUSTER\u0026gt;: The cluster name in the Gardener \u0026lt;GARDENER-PROJECT\u0026gt;: You project name in the Gardener  What is the underlying problem? The ingress controller we are using is NGINX.\n NGINX is a software load balancer, web server, and content cache built on top of open source NGINX.\n NGINX caches the content as specified in the HTTP header. If the HTTP header is missing, it is assumed that the cache is forever and NGINX never updates the content in the stupidest case.\nSolution In general you can avoid this pitfall with one of the solutions below:\n use a cache buster + HTTP-Cache-Control(prefered) use HTTP-Cache-Control with a lower retention period disable the caching in the ingress (just for dev purpose)  Learning how to set the HTTP header or setup a cache buster is left to the read as an exercise for your web framework (e.g. Express/NodeJS, SpringBoot,\u0026hellip;)\nHere an example how to disable the cache control for your ingress done with an annotation in your ingress YAML (during development).\n---\rapiVersion: networking.k8s.io/v1beta1\r kind: Ingress\rmetadata:\rannotations:\ringress.kubernetes.io/cache-enable: \u0026#34;false\u0026#34;\rname: vuejs-ingress\rspec:\rrules:\r- host: test.ingress.\u0026lt;GARDENER-CLUSTER\u0026gt;.\u0026lt;GARDENER-PROJECT\u0026gt;.shoot.canary.k8s-hana.ondemand.com\rhttp:\rpaths:\r- backend:\rserviceName: vuejs-svc\rservicePort: 8080\r"
},
{
	"uri": "https://gardener.cloud/documentation/045_contribute/10_code/37_process/",
	"title": "Process",
	"tags": [],
	"description": "",
	"content": "Creating a new Feature If you want to contribute to the Gardener, please do that always on a dedicated branch on your own fork named after the purpose of the code changes, for example feature/helm-integration. Please do not forget to rebase your branch regularly.\nIf you have finished your work, please create a pull request based on master. It will be reviewed and merged if no further changes are requested from you.\n:warning: Please ensure that your modifications pass the lint checks, formatting checks, static code checks, and unit tests by executing\nmake verify\r:rotating_light: Please run make generate whenever you modify the any API within pkg/apis.\nPlease do not file your pull request unless you receive a successful response from here!\nCreating a new Release Please refer to the Gardener contributor guide.\n"
},
{
	"uri": "https://gardener.cloud/blog/2018_week_27/",
	"title": "ReadWriteMany - Dynamically Provisioned Persistent Volumes Using Amazon EFS",
	"tags": [],
	"description": "",
	"content": "The efs-provisioner allows you to mount EFS storage as PersistentVolumes in kubernetes. It consists of a container that has access to an AWS EFS resource. The container reads a configmap containing the EFS filesystem ID, the AWS region and the name identifying the efs-provisioner. This name will be used later when you create a storage class.\nWhy EFS  When you have application running on multiple nodes which require shared access to a file system When you have an application that requires multiple virtual machines to access the same file system at the same time, AWS EFS is a tool that you can use. EFS supports encryption. EFS is SSD based storage and its storage capacity and pricing will scale in or out as needed, so there is no need for the system administrator to do additional operations. It can grow to a petabyte scale. EFS now supports NFSv4 lock upgrading and downgrading, so yes, you can use sqlite with EFS… even if it was possible before. Easy to setup  Why Not EFS  Sometimes when you think about using a service like EFS, you may also think about vendor lock-in and its negative sides Making an EFS backup may decrease your production FS performance; the throughput used by backup counts towards your total file system throughput. EFS is expensive compared to EBS (roughly twice the price of EBS storage) EFS is not the magical solution for all your distributed FS problems, it can be slow in many cases. Test, benchmark and measure to ensure your if EFS is a good solution for your use case. EFS distributed architecture results in a latency overhead for each file read/write operation. If you have the possibility to use a CDN, don’t use EFS, use it for the files which can't be stored in a CDN. Don’t use EFS as a caching system, sometimes you could be doing this unintentionally. Last but not least, even if EFS is a fully managed NFS, you will face performance problems in many cases, resolving them takes time and needs effort.  ..read some more on ReadWriteMany.\n"
},
{
	"uri": "https://gardener.cloud/documentation/050-tutorials/content/app/read-write-many/",
	"title": "ReadWriteMany with AWS",
	"tags": [],
	"description": "Dynamically Provisioned PV’s Using Amazon EFS",
	"content": ""
},
{
	"uri": "https://gardener.cloud/documentation/045_contribute/10_code/40_repositories/",
	"title": "Repositories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gardener.cloud/documentation/050-tutorials/content/howto/dns_names/",
	"title": "Request DNS Names",
	"tags": [],
	"description": "Requesting DNS Names for Ingresses and Services in Shoot Clusters",
	"content": "Request DNS Names in Shoot Clusters Introduction Gardener allows Shoot clusters to request DNS names for Ingresses and Services out of the box. Therefore the gardener must be installed with the shoot-dns-service extension. This extension uses the seed's dns management infrastructure to maintain DNS names for shoot clusters. So, far only the external DNS domain of a shoot (already used for the kubernetes api server and ingress DNS names) can be used for managed DNS names.\nShoot Feature Gate The shoot DNS feature is not globally enabled by default (depends on the extension registration on the garden cluster). Therefore it must be enabled per shoot.\nTo enable the feature for a shoot, the shoot manifest must add the shoot-dns-service extension.\n...\rspec:\rextensions:\r- type: shoot-dns-service\r...\rConfiguration In Shoot Cluster To request a DNS name for an Ingress or Service object in the shoot cluster it must be annotated with the DNS class garden and an annotation denoting the desired DNS names.\nFor a Service (it must have the type LoadBalancer) this looks like this:\napiVersion: v1\rkind: Service\rmetadata:\rannotations:\rdns.gardener.cloud/class: garden\rdns.gardener.cloud/dnsnames: my.subdomain.for.shootsomain.cloud\rname: my-service\rnamespace: default\rspec:\rports:\r- port: 80\rprotocol: TCP\rtargetPort: 80\rtype: LoadBalancer\rThe dnsnames annotation accepts a comma-separated list of DNS names, if multiple names are required.\nFor an Ingress, the dns names are already declared in the specification. Nevertheless the dnsnames annotation must be present. Here a subset of the dns names of the ingress can be specified. If DNS names for all names are desired, the value all can be used.\nIf one of the accepted dns names is a direct subname of the shoot's ingress domain, this is already handled by the standard wildcard entry for the ingress domain. Therefore this name should be excluded from the dnsnames list in the annotation. If only this dns name is configured in the ingress, no explicit dns entry is required, and the dns annotations should be omitted at all.\n"
},
{
	"uri": "https://gardener.cloud/documentation/050-tutorials/content/howto/x509_certificates/",
	"title": "Request X.509 Certificates",
	"tags": [],
	"description": "X.509 Certificates For TLS Communication",
	"content": "Request X.509 Certificates Introduction Dealing with applications on Kubernetes which offer service endpoints (e.g. HTTP) may also require you to enable a secured communication via SSL/TLS. Gardener let's you request a commonly trusted X.509 certificate for your application endpoint. Furthermore, Gardener takes care about the renewal process for your requested certificate.\nLet's get the basics straight first. If this is too long for you, you can read below how to get certificates by\n Certificate Resources Ingress Service  Restrictions Domains Certificates may be received for any subdomain of your shoot's domain (see .spec.dns.domain of your shoot resource) with the default issuer.\nCharacter Restrictions Due to the ACME protocol specification, at least one domain of the domains you request a certificate for must not exceed a character limit of 64 (CN - Common Name).\nFor example, the following request is invalid:\napiVersion: cert.gardener.cloud/v1alpha1\rkind: Certificate\rmetadata:\rname: cert-invalid\rnamespace: default\rspec:\rcommonName: morethan64characters.ingress.shoot.project.default-domain.gardener.cloud\rBut it is valid to request a certificate for this domain if you have at least one domain which does not exceed the mentioned limit:\napiVersion: cert.gardener.cloud/v1alpha1\rkind: Certificate\rmetadata:\rname: cert-example\rnamespace: default\rspec:\rcommonName: short.ingress.shoot.project.default-domain.gardener.cloud\rdnsNames:\r- morethan64characters.ingress.shoot.project.default-domain.gardener.cloud\rCertificate Resources Every X.509 certificate is represented by a Kubernetes custom resource certificate.cert.gardener.cloud in your cluster. A Certificate resource may be used to initiate a new certificate request as well as to manage its lifecycle. Gardener's certificate service regularly checks the expiration timestamp of Certificates, triggers a renewal process if necessary and replaces the existing X.509 certificate with a new one.\n Your application should be able to reload replaced certificates in a timely manner to avoid service disruptions.\n Certificates can either be requested by creating Certificate resources in the Kubernetes cluster or by configuring Ingress or Service (type LoadBalancer) resources. If the latter is used, a Certificate resource will automatically be created by Gardener's certificate service.\nIf you're interested in the current progress of your request, you're advised to consult the Certificate's status subresource. You'll also find error descriptions in the status in case the issuance failed.\nCertificate status example:\napiVersion: cert.gardener.cloud/v1alpha1\rkind: Certificate\r...\rstatus:\rcommonName: short.ingress.shoot.project.default-domain.gardener.cloud\rexpirationDate: \u0026#34;2020-02-27T15:39:10Z\u0026#34;\rissuerRef:\rname: garden\rnamespace: shoot--foo--bar\rlastPendingTimestamp: \u0026#34;2019-11-29T16:38:40Z\u0026#34;\robservedGeneration: 11\rstate: Ready\rCustom Domains If you want to request certificates for domains other then any subdomain of shoot.spec.dns.domain, the following configuration is required:\nDNS provider In order to issue certificates for a custom domain you need to specify a DNS provider which is permitted to create DNS records for subdomains of your requested domain in the certificate. For example, if you request a certificate for host.example.com your DNS provider must be capable of managing subdomains of host.example.com.\nDNS providers are specified in the shoot manifest:\nkind: Shoot\r...\rspec:\rdns:\rproviders:\r- type: aws-route53 # consult the DNS provisioning controllers group (dnscontrollers) in https://github.com/gardener/external-dns-management#using-the-dns-controller-manager for possible values\r secretName: provider-example-com # contains credentials for service account, see any 20-secret-\u0026lt;provider\u0026gt;-credentials.yaml in https://github.com/gardener/external-dns-management/tree/master/examples\r The secret referenced by secretName can also be conveniently created via the Gardener dashboard.\nIssuer Another prerequisite to request certificates for custom domains is a dedicated issuer.\nkind: Shoot\r...\rspec:\rextensions:\r- type: shoot-cert-service\rproviderConfig:\rapiVersion: service.cert.extensions.gardener.cloud/v1alpha1\rkind: CertConfig\rissuers:\r- email: your-email@example.com\rname: custom-issuer # issuer name must be specified in every custom issuer request, must not be \u0026#34;garden\u0026#34;\r server: \u0026#39;https://acme-v02.api.letsencrypt.org/directory\u0026#39;\rExamples Request a certificate via Certificate apiVersion: cert.gardener.cloud/v1alpha1\rkind: Certificate\rmetadata:\rname: cert-example\rnamespace: default\rspec:\rcommonName: short.ingress.shoot.project.default-domain.gardener.cloud\rdnsNames:\r- morethan64characters.ingress.shoot.project.default-domain.gardener.cloud\rsecretRef:\rname: cert-example\rnamespace: default\r# issuerRef:\r # name: custom-issuer\r  spec.commonName (required) specifies for which domain the certificate request will be created. This entry must comply with the 64 character limit.\n  spec.dnsName additional domains the certificate should be valid for. Entries in this list can be longer than 64 characters.\n  spec.secretRef specifies the secret which contains the certificate/key pair. If the secret is not available yet, it'll be created automatically as soon as the X.509 certificate has been issued.\n  spec.issuerRef (optional) specifies the issuer you want to use. Only necessary if you request certificates for custom domains.\n Request a wildcard certificate via Certificate apiVersion: cert.gardener.cloud/v1alpha1\rkind: Certificate\rmetadata:\rname: cert-wildcard\rnamespace: default\rspec:\rcommonName: \u0026#39;*.ingress.shoot.project.default-domain.gardener.cloud\u0026#39;\rsecretRef:\rname: cert-wildcard\rnamespace: default\r# issuerRef:\r # name: custom-issuer\r  spec.commonName (required) specifies for which domain the certificate request will be created. This entry must comply with the 64 character limit.\n  Please note that verifications for wildcard domain certificates only succeed if the subdomain and wildcard domain are on the same level. For example: A certificate for *.example.com works for foo.example.com but not for foo.bar.example.com.\n  spec.secretRef specifies the secret which contains the certificate/key pair. If the secret is not available yet, it'll be created automatically as soon as the X.509 certificate has been issued.\n  spec.issuerRef (optional) specifies the issuer you want to use. Only necessary if you request certificates for custom domains.\n Request a certificate via Ingress apiVersion: networking.k8s.io/v1beta1\rkind: Ingress\rmetadata:\rname: vuejs-ingress\rannotations:\rcert.gardener.cloud/purpose: managed\r# cert.gardener.cloud/issuer: custom-issuer\r spec:\rtls:\r# Must not exceed 64 characters.\r - hosts:\r- short.ingress.shoot.project.default-domain.gardener.cloud\r- morethan64characters.ingress.shoot.project.default-domain.gardener.cloud\r# Certificate and private key reside in this secret.\r secretName: testsecret-tls\rrules:\r- host: morethan64characters.ingress.shoot.project.default-domain.gardener.cloud\rhttp:\rpaths:\r- backend:\rserviceName: vuejs-svc\rservicePort: 8080\r metadata.annotations must contain cert.gardener.cloud/purpose: managed to activate the certificate service on this resource. cert.gardener.cloud/issuer: \u0026lt;name\u0026gt; is optional and may be specified if the certificate is request for a custom domains.\n  spec.tls[].hosts specifies for which domains the certificate request will be created. The first entry is always taken to fill the Common Name field and must therefore comply with the 64 character limit.\n  spec.tls[].secretName specifies the secret which contains the certificate/key pair to be used by this Ingress. If the secret is not available yet, it'll be created automatically as soon as the certificate has been issued. Once configured, you're not advised to change the name while the Ingress is still managed by the certificate service.\n Request a wildcard certificate via Ingress apiVersion: networking.k8s.io/v1beta1\rkind: Ingress\rmetadata:\rname: vuejs-ingress\rannotations:\rcert.gardener.cloud/purpose: managed\r# cert.gardener.cloud/issuer: custom-issuer\r spec:\rtls:\r# Must not exceed 64 characters.\r - hosts:\r- \u0026#34;*.ingress.shoot.project.default-domain.gardener.cloud\u0026#34;\r# Certificate and private key reside in this secret.\r secretName: testsecret-tls\rrules:\r- host: morethan64characters.ingress.shoot.project.default-domain.gardener.cloud\rhttp:\rpaths:\r- backend:\rserviceName: vuejs-svc\rservicePort: 8080\r metadata.annotations must contain cert.gardener.cloud/purpose: managed to activate the certificate service on this resource. cert.gardener.cloud/issuer: \u0026lt;name\u0026gt; is optional and may be specified if the certificate is request for a custom domains.\n  spec.tls[].hosts please make sure the wildcard domain complies with the 64 character limit.\n  Please note that verifications for wildcard domain certificates only succeed if the subdomain and wildcard domain are on the same level. For example: A certificate for *.example.com works for foo.example.com but not for foo.bar.example.com.\n Request a certificate via Service apiVersion: v1\rkind: Service\rmetadata:\rannotations:\rcert.gardener.cloud/secretname: test-service-secret\r# cert.gardener.cloud/issuer: custom-issuer\r dns.gardener.cloud/dnsnames: \u0026#34;service.shoot.project.default-domain.gardener.cloud, morethan64characters.svc.shoot.project.default-domain.gardener.cloud\u0026#34;\rdns.gardener.cloud/ttl: \u0026#34;600\u0026#34;\rname: test-service\rnamespace: default\rspec:\rports:\r- name: http\rport: 80\rprotocol: TCP\rtargetPort: 8080\rtype: LoadBalancer\r metadata.annotations[cert.gardener.cloud/secretname] specifies the secret which contains the certificate/key pair. If the secret is not available yet, it'll be created automatically as soon as the certificate has been issued.\n  metadata.annotations[cert.gardener.cloud/issuer] is optional and may be specified if the certificate is request for a custom domains.\n  metadata.annotations[dns.gardener.cloud/dnsnames] specifies for which domains the certificate request will be created. The first entry is always taken to fill the Common Name field and must therefore comply with the 64 character limit.\n Request a wildcard certificate via Service apiVersion: v1\rkind: Service\rmetadata:\rannotations:\rcert.gardener.cloud/secretname: test-service-secret\r# cert.gardener.cloud/issuer: custom-issuer\r dns.gardener.cloud/dnsnames: \u0026#34;*.service.shoot.project.default-domain.gardener.cloud\u0026#34;\rdns.gardener.cloud/ttl: \u0026#34;600\u0026#34;\rname: test-service\rnamespace: default\rspec:\rports:\r- name: http\rport: 80\rprotocol: TCP\rtargetPort: 8080\rtype: LoadBalancer\r metadata.annotations[cert.gardener.cloud/secretname] specifies the secret which contains the certificate/key pair. If the secret is not available yet, it'll be created automatically as soon as the certificate has been issued.\n  metadata.annotations[cert.gardener.cloud/issuer] is optional and may be specified if the certificate is request for a custom domains.\n  metadata.annotations[dns.gardener.cloud/dnsnames] please make sure the wildcard domain complies with the 64 character limit.\n  Please note that verifications for wildcard domain certificates only succeed if the subdomain and wildcard domain are on the same level. For example: A certificate for *.example.com works for foo.example.com but not for foo.bar.example.com.\n \r#body-inner blockquote {\rborder: 0;\rpadding: 10px;\rmargin-top: 40px;\rmargin-bottom: 40px;\rborder-radius: 4px;\rbackground-color: rgba(0,0,0,0.05);\rbox-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);\rposition:relative;\rpadding-left:60px;\r}\r#body-inner blockquote:before {\rcontent: \"!\";\rfont-weight: bold;\rposition: absolute;\rtop: 0;\rbottom: 0;\rleft: 0;\rbackground-color: #00a273;\rcolor: white;\rvertical-align: middle;\rmargin: auto;\rwidth: 36px;\rfont-size: 30px;\rtext-align: center;\r}\r\r"
},
{
	"uri": "https://gardener.cloud/documentation/045_contribute/10_code/12-security_guide/",
	"title": "Security Release Process",
	"tags": [],
	"description": "",
	"content": "Gardener Security Release Process Gardener is a growing community of volunteers and users. The Gardener community has adopted this security disclosure and response policy to ensure we responsibly handle critical issues.\nGardener Security Team Security vulnerabilities should be handled quickly and sometimes privately. The primary goal of this process is to reduce the total time users are vulnerable to publicly known exploits. The Gardener Security Team is responsible for organizing the entire response including internal communication and external disclosure but will need help from relevant developers and release managers to successfully run this process. The initial Gardener Security Team will consist of the following volunteers:\n Olaf Beier, (@olafbeier) Vasu Chandrasekhara, (@vasu1124) Alban Crequy, (@alban) Norbert Hamann, (@norberthamann) Claudia Hölters, (@hoeltcl) Oliver Kling, (@oliverkling) Vedran Lerenc, (@vlerenc) Dirk Marwinski, (@marwinski) Michael Schubert, (@schu) Matthias Sohn, (@msohn) Frederik Thormaehlen, (@ThormaehlenFred) Christian Cwienk (@ccwienk)  Disclosures Private Disclosure Processes The Gardener community asks that all suspected vulnerabilities be privately and responsibly disclosed. If you've found a vulnerability or a potential vulnerability in Gardener please let us know by writing an e-mail to secure@sap.com. We'll send a confirmation e-mail to acknowledge your report, and we'll send an additional e-mail when we've identified the issue positively or negatively.\nPublic Disclosure Processes If you know of a publicly disclosed vulnerability please IMMEDIATELY e-mail to secure@sap.com to inform the Gardener Security Team about the vulnerability so they may start the patch, release, and communication process.\nIf possible the Gardener Security Team will ask the person making the public report if the issue can be handled via a private disclosure process (for example if the full exploit details have not yet been published). If the reporter denies the request for private disclosure, the Gardener Security Team will move swiftly with the fix and release process. In extreme cases GitHub can be asked to delete the issue but this generally isn't necessary and is unlikely to make a public disclosure less damaging.\nPatch, Release, and Public Communication For each vulnerability a member of the Gardener Security Team will volunteer to lead coordination with the \u0026ldquo;Fix Team\u0026rdquo; and is responsible for sending disclosure e-mails to the rest of the community. This lead will be referred to as the \u0026ldquo;Fix Lead.\u0026rdquo; The role of the Fix Lead should rotate round-robin across the Gardener Security Team. Note that given the current size of the Gardener community it is likely that the Gardener Security Team is the same as the \u0026ldquo;Fix team.\u0026rdquo; (I.e., all maintainers). The Gardener Security Team may decide to bring in additional contributors for added expertise depending on the area of the code that contains the vulnerability. All of the time lines below are suggestions and assume a private disclosure. The Fix Lead drives the schedule using his best judgment based on severity and development time. If the Fix Lead is dealing with a public disclosure all time lines become ASAP (assuming the vulnerability has a CVSS score \u0026gt;= 7; see below). If the fix relies on another upstream project's disclosure time line, that will adjust the process as well. We will work with the upstream project to fit their time line and best protect our users.\nFix Team Organization The Fix Lead will work quickly to identify relevant engineers from the affected projects and packages and CC those engineers into the disclosure thread. These selected developers are the Fix Team. The Fix Lead will give the Fix Team access to a private security repository to develop the fix.\nFix Development Process The Fix Lead and the Fix Team will create a CVSS using the CVSS Calculator. The Fix Lead makes the final call on the calculated CVSS; it is better to move quickly than make the CVSS perfect. The Fix Team will notify the Fix Lead that work on the fix branch is complete once there are LGTMs on all commits in the private repository from one or more maintainers. If the CVSS score is under 7.0 (a medium severity score) the Fix Team can decide to slow the release process down in the face of holidays, developer bandwidth, etc. These decisions must be discussed on the private Gardener Security mailing list.\nFix Disclosure Process With the fix development underway, the Fix Lead needs to come up with an overall communication plan for the wider community. This Disclosure process should begin after the Fix Team has developed a Fix or mitigation so that a realistic time line can be communicated to users. The Fix Lead will inform the Gardener mailing list that a security vulnerability has been disclosed and that a fix will be made available in the future on a certain release date. The Fix Lead will include any mitigating steps users can take until a fix is available. The communication to Gardener users should be actionable. They should know when to block time to apply patches, understand exact mitigation steps, etc.\nFix Release Day The Release Managers will ensure all the binaries are built, publicly available, and functional before the Release Date. The Release Managers will create a new patch release branch from the latest patch release tag + the fix from the security branch. As a practical example if v0.12.0 is the latest patch release in gardener.git a new branch will be created called v0.12.1 which includes only patches required to fix the issue. The Fix Lead will cherry-pick the patches onto the master branch and all relevant release branches. The Fix Team will LGTM and merge. The Release Managers will merge these PRs as quickly as possible. Changes shouldn't be made to the commits even for a typo in the CHANGELOG as this will change the git sha of the already built and commits leading to confusion and potentially conflicts as the fix is cherry-picked around branches. The Fix Lead will request a CVE from the SAP Product Security Response Team via email to cna@sap.com with all the relevant information (description, potential impact, affected version, fixed version, CVSS v3 base score and supporting documentation for the CVSS score) for every vulnerability. The Fix Lead will inform the Gardener mailing list and announce the new releases, the CVE number (if available), the location of the binaries, and the relevant merged PRs to get wide distribution and user action.\nAs much as possible this e-mail should be actionable and include links how to apply the fix to users environments; this can include links to external distributor documentation. The recommended target time is 4pm UTC on a non-Friday weekday. This means the announcement will be seen morning Pacific, early evening Europe, and late evening Asia. The Fix Lead will remove the Fix Team from the private security repository.\nRetrospective These steps should be completed after the Release Date. The retrospective process should be blameless.\nThe Fix Lead will send a retrospective of the process to the Gardener mailing list including details on everyone involved, the time line of the process, links to relevant PRs that introduced the issue, if relevant, and any critiques of the response and release process. The Release Managers and Fix Team are also encouraged to send their own feedback on the process to the Gardener mailing list. Honest critique is the only way we are going to get good at this as a community.\nCommunication Channel The private or public disclosure process should be triggered exclusively by writing an e-mail to secure@sap.com.\nGardener security announcements will be communicated by the Fix Lead sending an e-mail to the Gardener mailing list (reachable via gardener@googlegroups.com) as well as posting a link in the Gardener Slack channel. Public discussions about Gardener security announcements and retrospectives, will primarily happen in the Gardener mailing list. Thus Gardener community members who are interested in participating in discussions related to the Gardener Security Release Process are encouraged to join the Gardener mailing list (how to find and join a group)\nThe members of the Gardener Security Team are subscribed to the private Gardener Security mailing list (reachable via gardener-security@googlegroups.com).\n"
},
{
	"uri": "https://gardener.cloud/documentation/050-tutorials/content/howto/setup-seed/",
	"title": "Setting up a Seed Cluster",
	"tags": [],
	"description": "How to configure a Kubernetes cluster as a Gardener seed",
	"content": "The Seed Cluster The landscape-setup-template is meant to provide an as-simple-as-possible Gardener installation. Therefore it just registers the cluster where the Gardener is deployed on as a seed cluster. While this is easy, it might be insecure. Clusters created with Kubify don't have network policies, for example. See Hardening the Gardener Community Setup for more information.\nTo have network policies on the seed cluster and avoid having the seed on the same cluster as the Gardener, the easiest option is probably to simply create a shoot and then register that shoot as seed. This way you can also leverage other advantages of shooted clusters for your seed, e.g. autoscaling.\nSetting up the Shoot The first step is to create a shoot cluster. Unfortunately, the Gardener dashboard currently does not allow to change the CIDRs for the created shoot clusters, and your shoots won't work if they have overlapping CIDR ranges with their corresponding seed cluster. So either your seed cluster is deployed with different CIDRs - not using the dashboard, but kubectl apply and a yaml file - or all of your shoots on that seed need to be created this way. In order to be able to use the dashboard for the shoots, it makes sense to create the seed with different CIDRs.\nSo, create yourself a shoot with modified CIDRs. You can find templates for the shoot manifest here. You could, for example, change the CIDRs to this:\n...\rnetworks:\rinternal:\r- 10.254.112.0/22\rnodes: 10.254.0.0/19\rpods: 10.255.0.0/17\rpublic:\r- 10.254.96.0/22\rservices: 10.255.128.0/17\rvpc:\rcidr: 10.254.0.0/16\rworkers:\r- 10.254.0.0/19\r...\rAlso make sure that your new seed cluster has enough resources for the expected number of shoots.\nRegistering the Shoot as Seed The seed itself is a Kubernetes resource that can be deployed via a yaml file, but it has some dependencies. You can find templated versions of these files in the seed-config component of the landscape-setup-template project. If you have set up your Gardener using this project, there should also be rendered versions of these files in the state/seed-config/ directory of your landscape folder (they are probably easier to work with). Examples for all these files can also be found in the aforementioned example folder in the Gardener repo.\n1. Seed Namespace First, you should create a namespace for your new seed and everything that belongs to it. This is not necessary, but it will keep your cluster organized. For this example, the namespace will be called seed-test.\n2. Cloud Provider Secret The Gardener needs to create resources on the seed and thus needs a kubeconfig for it. It is provided with the cloud provider secret (below is an example for AWS).\napiVersion: v1\rkind: Secret\rmetadata:\rname: test-seed-secret\rnamespace: seed-test\rlabels:\rcloudprofile.garden.sapcloud.io/name: aws type: Opaque\rdata:\raccessKeyID: \u0026lt;base64-encoded AWS access key\u0026gt;\rsecretAccessKey: \u0026lt;base64-encoded AWS secret key\u0026gt;\r kubeconfig: \u0026lt;base64-encoded kubeconfig\u0026gt;\rDeploy the secret into your seed namespace. Apart from the kubeconfig, also infrastructure credentials are required. They will only be used for the etcd backup, so in case for AWS, S3 privileges should be sufficient.\n3. Secretbinding for Cloud Provider Secret Create a secretbinding for your cloud provider secret:\napiVersion: core.gardener.cloud/v1beta1\rkind: SecretBinding\rmetadata:\rname: test-seed-secret\rnamespace: seed-test\rlabels:\rcloudprofile.garden.sapcloud.io/name: aws\rsecretRef:\rname: test-seed-secret\r# namespace: only required if in different namespace than referenced secret\r quotas: []\rYou can give it the same name as the referenced secret.\n4. Cloudprofile The cloudprofile contains the information which shoots can be created with this seed. You could create a new cloudprofile, but you can also just reference the existing cloudprofile if you don't want to change anything.\n5. Seed Now the seed resource can be created. Choose a name, reference cloudprofile and secretbinding, fill in your ingress domain, and set the CIDRs to the same values as in the underlying shoot cluster.\napiVersion: core.gardener.cloud/v1beta1\rkind: Seed\rmetadata:\rname: aws-secure\rspec:\rprovider:\rtype: aws\rregion: eu-west-1\rsecretRef:\rname: test-seed-secret\rnamespace: seed-test\rdns:\ringressDomain: ingress.\u0026lt;your cluster domain\u0026gt;\rnetworks:\r nodes: 10.254.0.0/19\rpods: 10.255.0.0/17\rservices: 10.255.128.0/17\r6. Hide Original Seed In the dashboard, it is not possible to select the seed for a shoot (it is possible when deploying the shoot using a yaml file, however). Since both seeds probably reference the same cloudprofile, the Gardener will try to distribute the shoots equally among both seeds.\nTo solve this problem, edit the original seed and set its spec.visible field to false. This will prevent the Gardener from choosing this seed, so now all shoots created via the dashboard should have their control plane on the new, more secure seed.\n"
},
{
	"uri": "https://gardener.cloud/api-reference/settings/",
	"title": "Settings",
	"tags": [],
	"description": "",
	"content": "Packages:\n\r\rsettings.gardener.cloud/v1alpha1\r\r\rsettings.gardener.cloud/v1alpha1\r\rPackage v1alpha1 is a version of the API.\nResource Types:\r\rClusterOpenIDConnectPreset\r\rOpenIDConnectPreset\r\rClusterOpenIDConnectPreset\r\r\rClusterOpenIDConnectPreset is a OpenID Connect configuration that is applied\rto a Shoot objects cluster-wide.\n\r\r\rField\rDescription\r\r\r\r\r\rapiVersion\rstring\r\r\rsettings.gardener.cloud/v1alpha1\r\r\r\r\r\rkind\rstring\r\rClusterOpenIDConnectPreset\r\r\r\rmetadata\r\rKubernetes meta/v1.ObjectMeta\r\r\r\r\rStandard object metadata.\nRefer to the Kubernetes API documentation for the fields of the\rmetadata field.\r\r\r\r\rspec\r\rClusterOpenIDConnectPresetSpec\r\r\r\r\rSpec is the specification of this OpenIDConnect preset.\n\r\r\r\r\rOpenIDConnectPresetSpec\r\rOpenIDConnectPresetSpec\r\r\r\r\r\r(Members of OpenIDConnectPresetSpec are embedded into this type.)\r\r\r\r\rprojectSelector\r\rKubernetes meta/v1.LabelSelector\r\r\r\r\r(Optional)\rProject decides whether to apply the configuration if the\rShoot is in a specific Project mathching the label selector.\rUse the selector only if the OIDC Preset is opt-in, because end\rusers may skip the admission by setting the labels.\rDefault to the empty LabelSelector, which matches everything.\n\r\r\r\r\r\r\rOpenIDConnectPreset\r\r\rOpenIDConnectPreset is a OpenID Connect configuration that is applied\rto a Shoot in a namespace.\n\r\r\rField\rDescription\r\r\r\r\r\rapiVersion\rstring\r\r\rsettings.gardener.cloud/v1alpha1\r\r\r\r\r\rkind\rstring\r\rOpenIDConnectPreset\r\r\r\rmetadata\r\rKubernetes meta/v1.ObjectMeta\r\r\r\r\rStandard object metadata.\nRefer to the Kubernetes API documentation for the fields of the\rmetadata field.\r\r\r\r\rspec\r\rOpenIDConnectPresetSpec\r\r\r\r\rSpec is the specification of this OpenIDConnect preset.\n\r\r\r\r\rserver\r\rKubeAPIServerOpenIDConnect\r\r\r\r\rServer contains the kube-apiserver\u0026rsquo;s OpenID Connect configuration.\rThis configuration is not overwritting any existing OpenID Connect\rconfiguration already set on the Shoot object.\n\r\r\r\rclient\r\rOpenIDConnectClientAuthentication\r\r\r\r\r(Optional)\rClient contains the configuration used for client OIDC authentication\rof Shoot clusters.\rThis configuration is not overwritting any existing OpenID Connect\rclient authentication already set on the Shoot object.\n\r\r\r\rshootSelector\r\rKubernetes meta/v1.LabelSelector\r\r\r\r\r(Optional)\rShootSelector decides whether to apply the configuration if the\rShoot has matching labels.\rUse the selector only if the OIDC Preset is opt-in, because end\rusers may skip the admission by setting the labels.\rDefault to the empty LabelSelector, which matches everything.\n\r\r\r\rweight\r\rint32\r\r\r\rWeight associated with matching the corresponding preset,\rin the range 1-100.\rRequired.\n\r\r\r\r\r\r\rClusterOpenIDConnectPresetSpec\r\r\r(Appears on:\rClusterOpenIDConnectPreset)\r\rClusterOpenIDConnectPresetSpec contains the OpenIDConnect specification and\rproject selector matching Shoots in Projects.\n\r\r\rField\rDescription\r\r\r\r\r\rOpenIDConnectPresetSpec\r\rOpenIDConnectPresetSpec\r\r\r\r\r\r(Members of OpenIDConnectPresetSpec are embedded into this type.)\r\r\r\r\rprojectSelector\r\rKubernetes meta/v1.LabelSelector\r\r\r\r\r(Optional)\rProject decides whether to apply the configuration if the\rShoot is in a specific Project mathching the label selector.\rUse the selector only if the OIDC Preset is opt-in, because end\rusers may skip the admission by setting the labels.\rDefault to the empty LabelSelector, which matches everything.\n\r\r\r\rKubeAPIServerOpenIDConnect\r\r\r(Appears on:\rOpenIDConnectPresetSpec)\r\rKubeAPIServerOpenIDConnect contains configuration settings for the OIDC provider.\rNote: Descriptions were taken from the Kubernetes documentation.\n\r\r\rField\rDescription\r\r\r\r\r\rcaBundle\r\rstring\r\r\r\r(Optional)\rIf set, the OpenID server\u0026rsquo;s certificate will be verified by one of the authorities in the oidc-ca-file, otherwise the host\u0026rsquo;s root CA set will be used.\n\r\r\r\rclientID\r\rstring\r\r\r\rThe client ID for the OpenID Connect client.\rRequired.\n\r\r\r\rgroupsClaim\r\rstring\r\r\r\r(Optional)\rIf provided, the name of a custom OpenID Connect claim for specifying user groups. The claim value is expected to be a string or array of strings. This field is experimental, please see the authentication documentation for further details.\n\r\r\r\rgroupsPrefix\r\rstring\r\r\r\r(Optional)\rIf provided, all groups will be prefixed with this value to prevent conflicts with other authentication strategies.\n\r\r\r\rissuerURL\r\rstring\r\r\r\rThe URL of the OpenID issuer, only HTTPS scheme will be accepted. If set, it will be used to verify the OIDC JSON Web Token (JWT).\rRequired.\n\r\r\r\rrequiredClaims\r\rmap[string]string\r\r\r\r(Optional)\rkey=value pairs that describes a required claim in the ID Token. If set, the claim is verified to be present in the ID Token with a matching value.\rOnly applied when the Kubernetes version of the Shoot is \u0026gt;= 1.11\n\r\r\r\rsigningAlgs\r\r[]string\r\r\r\r(Optional)\rList of allowed JOSE asymmetric signing algorithms. JWTs with a \u0026lsquo;alg\u0026rsquo; header value not in this list will be rejected. Values are defined by RFC 7518 https://tools.ietf.org/html/rfc7518#section-3.1\rDefaults to [RS256]\n\r\r\r\rusernameClaim\r\rstring\r\r\r\r(Optional)\rThe OpenID claim to use as the user name. Note that claims other than the default (\u0026lsquo;sub\u0026rsquo;) is not guaranteed to be unique and immutable. This field is experimental, please see the authentication documentation for further details.\rDefaults to \u0026ldquo;sub\u0026rdquo;.\n\r\r\r\rusernamePrefix\r\rstring\r\r\r\r(Optional)\rIf provided, all usernames will be prefixed with this value. If not provided, username claims other than \u0026lsquo;email\u0026rsquo; are prefixed by the issuer URL to avoid clashes. To skip any prefixing, provide the value \u0026lsquo;-\u0026rsquo;.\n\r\r\r\rOpenIDConnectClientAuthentication\r\r\r(Appears on:\rOpenIDConnectPresetSpec)\r\rOpenIDConnectClientAuthentication contains configuration for OIDC clients.\n\r\r\rField\rDescription\r\r\r\r\r\rsecret\r\rstring\r\r\r\r(Optional)\rThe client Secret for the OpenID Connect client.\n\r\r\r\rextraConfig\r\rmap[string]string\r\r\r\r(Optional)\rExtra configuration added to kubeconfig\u0026rsquo;s auth-provider.\rMust not be any of idp-issuer-url, client-id, client-secret, idp-certificate-authority, idp-certificate-authority-data, id-token or refresh-token\n\r\r\r\rOpenIDConnectPresetSpec\r\r\r(Appears on:\rOpenIDConnectPreset, ClusterOpenIDConnectPresetSpec)\r\rOpenIDConnectPresetSpec contains the Shoot selector for which\ra specific OpenID Connect configuration is applied.\n\r\r\rField\rDescription\r\r\r\r\r\rserver\r\rKubeAPIServerOpenIDConnect\r\r\r\r\rServer contains the kube-apiserver\u0026rsquo;s OpenID Connect configuration.\rThis configuration is not overwritting any existing OpenID Connect\rconfiguration already set on the Shoot object.\n\r\r\r\rclient\r\rOpenIDConnectClientAuthentication\r\r\r\r\r(Optional)\rClient contains the configuration used for client OIDC authentication\rof Shoot clusters.\rThis configuration is not overwritting any existing OpenID Connect\rclient authentication already set on the Shoot object.\n\r\r\r\rshootSelector\r\rKubernetes meta/v1.LabelSelector\r\r\r\r\r(Optional)\rShootSelector decides whether to apply the configuration if the\rShoot has matching labels.\rUse the selector only if the OIDC Preset is opt-in, because end\rusers may skip the admission by setting the labels.\rDefault to the empty LabelSelector, which matches everything.\n\r\r\r\rweight\r\rint32\r\r\r\rWeight associated with matching the corresponding preset,\rin the range 1-100.\rRequired.\n\r\r\r\r\r"
},
{
	"uri": "https://gardener.cloud/blog/2018_week_10/",
	"title": "Shared storage with S3 backend",
	"tags": [],
	"description": "",
	"content": "The storage is definitely the most complex and important part of an application setup, once this part is completed, one of the most problematic parts could be solved.\nMounting a S3 bucket into a pod using FUSE allows to access data stored in S3 via the filesystem. The mount is a pointer to an S3 location, so the data is never synced locally. Once mounted, any pod can read or even write from that directory without the need for explicit keys.\n\u0026lt;img src=\u0026quot;../blog/2018_week_10/blog-s3-shared-storage.png\u0026quot; title=\u0026quot;logo\u0026quot; width=\u0026quot;100%\u0026quot; class=\u0026quot;drop-shadow reveal-fast\u0026quot; style=\u0026quot;\u0026quot;/\u0026gt;\r However, it can be used to import and parse large amounts of data into a database.\n..read on Shared S3 Storage how to configure it.\n"
},
{
	"uri": "https://gardener.cloud/documentation/050-tutorials/content/app/s3/",
	"title": "Shared storage with S3 backend",
	"tags": [],
	"description": "Shared storage with S3 backend",
	"content": "Shared storage with S3 backend The storage is definitely the most complex and important part of an application setup, once this part is completed, 80% of the tasks are completed.\nMounting an S3 bucket into a pod using FUSE allows you to access the data as if it were on the local disk. The mount is a pointer to an S3 location, so the data is never synced locally. Once mounted, any pod can read or even write from that directory without the need for explicit keys.\nHowever, it can be used to import and parse large amounts of data into a database.\nOverview Limitations Generally S3 cannot offer the same performance or semantics as a local file system. More specifically:\n random writes or appends to files require rewriting the entire file metadata operations such as listing directories have poor performance due to network latency eventual consistency can temporarily yield stale data(Amazon S3 Data Consistency Model) no atomic renames of files or directories no coordination between multiple clients mounting the same bucket no hard links  Before you Begin You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using the Gardener.\nEnsure that you have create the \u0026ldquo;imagePullSecret\u0026rdquo; in your cluster.\nkubectl create secret docker-registry artifactory --docker-server=\u0026lt;YOUR-REGISTRY\u0026gt;.docker.repositories.sap.ondemand.com --docker-username=\u0026lt;USERNAME\u0026gt; --docker-password=\u0026lt;PASSWORD\u0026gt; --docker-email=\u0026lt;EMAIL\u0026gt; -n \u0026lt;NAMESPACE\u0026gt;\rSetup The first step is to clone this repository. Next is the Secret for the AWS API credentials of the user that has full access to our S3 bucket. Copy the configmap_secrets_template.yaml to configmap_secrets.yaml and place your secrets at the right place\napiVersion: v1\rkind: ConfigMap\rmetadata:\rname: s3-config\rdata:\rS3_BUCKET: \u0026lt;YOUR-S3-BUCKET-NAME\u0026gt;\rAWS_KEY: \u0026lt;YOUR-AWS-TECH-USER-ACCESS-KEY\u0026gt;\r AWS_SECRET_KEY: \u0026lt;YOUR-AWS-TECH-USER-SECRET\u0026gt;\rBuild and deploy Change the settings in the build.sh file with your docker registry settings.\n#!/usr/bin/env bash\r\r########################################################################################################################\r# PREREQUISTITS\r########################################################################################################################\r#\r# - ensure that you have a valid Artifactory or other Docker registry account\r# - Create your image pull secret in your namespace\r# kubectl create secret docker-registry artifactory --docker-server=\u0026lt;YOUR-REGISTRY\u0026gt;.docker.repositories.sap.ondemand.com --docker-username=\u0026lt;USERNAME\u0026gt; --docker-password=\u0026lt;PASSWORD\u0026gt; --docker-email=\u0026lt;EMAIL\u0026gt; -n \u0026lt;NAMESPACE\u0026gt;\r# - change the settings below arcording your settings\r#\r# usage:\r# Call this script with the version to build and push to the registry. After build/push the\r# yaml/* files are deployed into your cluster\r#\r# ./build.sh 1.0\r#\rVERSION=$1\rPROJECT=kube-s3\rREPOSITORY=cp-enablement.docker.repositories.sap.ondemand.com\r# causes the shell to exit if any subcommand or pipeline returns a non-zero status.\rset -e\r# set debug mode\r#set -x\r.\r.\r.\r.\rCreate the S3Fuse Pod and check the status:\n# build and push the image to your docker registry\r./build.sh 1.0 # check that the pods are up and running\rkubectl get pods\rCheck success Create a demo Pod and check the status:\nkubectl apply -f ./yaml/example_pod.yaml\r# wait some second to get the pod up and running...\rkubectl get pods\r# go into the pd and check that the /var/s3 is mounted with your S3 bucket content inside\rkubectl exec -ti test-pd sh\r# inside the pod\rls -la /var/s3\rWhy does this work? Docker engine 1.10 added a new feature which allows containers to share the host mount namespace. This feature makes it possible to mount a s3fs container file system to a host file system through a shared mount, providing a persistent network storage with S3 backend.\nThe key part is mountPath: /var/s3:shared which enables the volume to be mounted as shared inside the pod. When the container starts it will mount the S3 bucket onto /var/s3 and consequently the data will be available under /mnt/data-s3fs on the host and thus to any other container/pod running on it (and has /mnt/data-s3fs mounted too).\n"
},
{
	"uri": "https://gardener.cloud/documentation/050-tutorials/content/howto/commit_secret_fail/",
	"title": "Storing secrets in git 💀",
	"tags": [],
	"description": "Never ever commit a kubeconfig.yaml into github",
	"content": "Problem If you commit sensitive data, such as a kubeconfig.yaml or SSH key into a Git repository, you can remove it from the history. To entirely remove unwanted files from a repository's history you can use the git filter-branch command.\nThe git filter-branch command rewrite your repository's history, which changes the SHAs for existing commits that you alter and any dependent commits. Changed commit SHAs may affect open pull requests in your repository. I recommend merging or closing all open pull requests before removing files from your repository.\n Warning: - if someone has already checked out the repository, then of course he has the secret on his computer. So ALWAYS revoke the OAuthToken/Password or whatever it was imediately.\n Purging a file from your repository's history  Warning: If you run git filter-branch after stashing changes, you won't be able to retrieve your changes with other stash commands. Before running git filter-branch, we recommend unstashing any changes you've made. To unstash the last set of changes you've stashed, run git stash show -p | git apply -R. For more information, see Git Tools Stashing.\n To illustrate how git filter-branch works, we'll show you how to remove your file with sensitive data from the history of your repository and add it to .gitignore to ensure that it is not accidentally re-committed.\nNavigate into the repository's working directory.\ncd YOUR-REPOSITORY\rRun the following command, replacing PATH-TO-YOUR-FILE-WITH-SENSITIVE-DATA with the path to the file you want to remove, not just its filename.\nThese arguments will:\n Force Git to process, but not check out, the entire history of every branch and tag Remove the specified file, as well as any empty commits generated as a result Overwrite your existing tags  git filter-branch --force --index-filter \\\r \u0026#39;git rm --cached --ignore-unmatch PATH-TO-YOUR-FILE-WITH-SENSITIVE-DATA\u0026#39; \\\r --prune-empty --tag-name-filter cat -- --all\rAdd your file with sensitive data to .gitignore to ensure that you don't accidentally commit it again.\necho \u0026#34;YOUR-FILE-WITH-SENSITIVE-DATA\u0026#34; \u0026gt;\u0026gt; .gitignore\rDouble-check that you've removed everything you wanted to from your repository's history, and that all of your branches are checked out.\nOnce you're happy with the state of your repository, force-push your local changes to overwrite your GitHub repository, as well as all the branches you've pushed up:\ngit push origin --force --all\rIn order to remove the sensitive file from your tagged releases, you'll also need to force-push against your Git tags:\ngit push origin --force --tags\r Warning: Tell your collaborators to rebase, not merge, any branches they created off of your old (tainted) repository history. One merge commit could reintroduce some or all of the tainted history that you just went to the trouble of purging.\n References:\n https://help.github.com/articles/removing-sensitive-data-from-a-repository/  \rblockquote {\rborder:1px solid red;\rpadding:10px;\rmargin-top:40px;\rmargin-bottom:40px;\r}\rblockquote p {\rfont-size: 1.5rem;\rcolor: black;\r}\r\r"
},
{
	"uri": "https://gardener.cloud/documentation/045_contribute/20_documentation/20_style/",
	"title": "Style Guide",
	"tags": [],
	"description": "",
	"content": "This page gives writing style guidelines for the Gardener documentation. These are guidelines, not rules. Use your best judgment, and feel free to propose changes to this document in a pull request.\nLanguage Gardener documentation uses US English.\nDocumentation formatting standards Use camel case for API objects When you refer to an API object, use the same uppercase and lowercase letters that are used in the actual object name. Typically, the names of API objects use camel case.\nDon't split the API object name into separate words. For example, use PodTemplateList, not Pod Template List.\nRefer to API objects without saying \u0026ldquo;object,\u0026rdquo; unless omitting \u0026ldquo;object\u0026rdquo; leads to an awkward construction.\n\rDoDon't\rThe Pod has two containers.The pod has two containers.\rThe Deployment is responsible for ...The Deployment object is responsible for ...\rA PodList is a list of Pods.A Pod List is a list of pods.\rThe two ContainerPorts ...The two ContainerPort objects ...\rThe two ContainerStateTerminated objects ...The two ContainerStateTerminateds ...\r\rUse angle brackets for placeholders Use angle brackets for placeholders. Tell the reader what a placeholder represents.\n  Display information about a pod:\nkubectl describe pod \u0026lt;pod-name\u0026gt;\r where \u0026lt;pod-name\u0026gt; is the name of one of your pods.\n  Use bold for user interface elements \rDoDon't\rClick Fork.Click \"Fork\".\rSelect Other.Select 'Other'.\r\rUse italics to define or introduce new terms \rDoDon't\rA cluster is a set of nodes ...A \"cluster\" is a set of nodes ...\rThese components form the control plane.These components form the control plane.\r\rUse code style for filenames, directories, and paths \rDoDon't\rOpen the envars.yaml file.Open the envars.yaml file.\rGo to the /docs/tutorials directory.Go to the /docs/tutorials directory.\rOpen the /_data/concepts.yaml file.Open the /_data/concepts.yaml file.\r\rUse the international standard for punctuation inside quotes \rDoDon't\revents are recorded with an associated \"stage\".events are recorded with an associated \"stage.\"\rThe copy is called a \"fork\".The copy is called a \"fork.\"\r\rInline code formatting Use code style for inline code and commands For inline code in an HTML document, use the \u0026lt;code\u0026gt; tag. In a Markdown document, use the backtick (`).\n\rDoDon't\rThe kubectl run command creates a Deployment.The \"kubectl run\" command creates a Deployment.\rFor declarative management, use kubectl apply.For declarative management, use \"kubectl apply\".\r\rUse code style for object field names \rDoDon't\rSet the value of the replicas field in the configuration file.Set the value of the \"replicas\" field in the configuration file.\rThe value of the exec field is an ExecAction object.The value of the \"exec\" field is an ExecAction object.\r\rUse normal style for string and integer field values For field values of type string or integer, use normal style without quotation marks.\n\rDoDon't\rSet the value of imagePullPolicy to Always.Set the value of imagePullPolicy to \"Always\".\rSet the value of image to nginx:1.8.Set the value of image to nginx:1.8.\rSet the value of the replicas field to 2.Set the value of the replicas field to 2.\r\rCode snippet formatting Don't include the command prompt \rDoDon't\rkubectl get pods$ kubectl get pods\r\rSeparate commands from output Verify that the pod is running on your chosen node:\nkubectl get pods --output=wide\r The output is similar to this:\nNAME READY STATUS RESTARTS AGE IP NODE\rnginx 1/1 Running 0 13s 10.200.0.4 worker0\r Versioning Kubernetes examples Code examples and configuration examples that include version information should be consistent with the accompanying text. Identify the Kubernetes version in the Before you begin section.\nTo specify the Kubernetes version for a task or tutorial page, include min-kubernetes-server-version in the front matter of the page.\nIf the example YAML is in a standalone file, find and review the topics that include it as a reference. Verify that any topics using the standalone YAML have the appropriate version information defined. If a stand-alone YAML file is not referenced from any topics, consider deleting it instead of updating it.\nFor example, if you are writing a tutorial that is relevant to Kubernetes version 1.8, the front-matter of your markdown file should look something like:\n---\rtitle: \u0026lt;your tutorial title here\u0026gt;\r min-kubernetes-server-version: v1.8\r---\rIn code and configuration examples, do not include comments about alternative versions. Be careful to not include incorrect statements in your examples as comments, such as:\napiVersion: v1 # earlier versions use...\r kind: Pod\r...\r"
},
{
	"uri": "https://gardener.cloud/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gardener.cloud/documentation/050-tutorials/content/howto/tail-logfile/",
	"title": "tail -f /var/log/my-application.log",
	"tags": [],
	"description": "Aggregate log files from different pods",
	"content": "Problem One thing that always bothered me was that I couldn't get logs of several pods at once with kubectl. A simple tail -f \u0026lt;path-to-logfile\u0026gt; isn't possible at all. Certainly you can use kubectl logs -f \u0026lt;pod-id\u0026gt;, but it doesn't help if you want to monitor more than one pod at a time.\nThis is something you really need a lot, at least if you run several instances of a pod behind a deployment. This is even more so if you don't have a Kibana setup or similar.\nSolution Luckily, there are smart developers out there who always come up with solutions. The finding of the week is a small bash script that allows you to aggregate log files of several pods at the same time in a simple way. The script is called kubetail and is available at https://github.com/johanhaleby/kubetail.\n"
},
{
	"uri": "https://gardener.cloud/documentation/050-tutorials/content/howto/trigger-shoot-operations/",
	"title": "Trigger Shoot operations",
	"tags": [],
	"description": "Trigger Shoot operations",
	"content": "Trigger shoot operations You can trigger a few explicit operations by annotating the Shoot with an operation annotation. This might allow you to induct certain behavior without the need to change the Shoot specification. Some of the operations can also not be caused by changing something in the shoot specification because they can't properly be reflected here.\nPlease note: If .spec.maintenance.confineSpecUpdateRollout=true then the only way to trigger a shoot reconciliation is by setting the reconcile operation, see below.\nImmediate reconciliation Annotate the shoot with gardener.cloud/operation=reconcile to make the gardenlet start a reconciliation operation without changing the shoot spec and possibly without being in its maintenance time window:\nkubectl -n garden-\u0026lt;project-name\u0026gt; annotate shoot \u0026lt;shoot-name\u0026gt; gardener.cloud/operation=reconcile\rImmediate maintenance Annotate the shoot with gardener.cloud/operation=maintain to make the gardener-controller-manager start maintaining your shoot immediately (possibly without being in its maintenance time window). If no reconciliation starts then nothing needed to be maintained:\nkubectl -n garden-\u0026lt;project-name\u0026gt; annotate shoot \u0026lt;shoot-name\u0026gt; gardener.cloud/operation=maintain\rRetry failed operation Annotate the shoot with gardener.cloud/operation=retry to make the gardenlet start a new reconciliation loop on a failed shoot. Failed shoots are only reconciled again if a new Gardener version is deployed, the shoot specification is changed or this annotation is set\nkubectl -n garden-\u0026lt;project-name\u0026gt; annotate shoot \u0026lt;shoot-name\u0026gt; gardener.cloud/operation=retry\rRotate kubeconfig credentials Annotate the shoot with gardener.cloud/operation=rotate-kubeconfig-credentials to make the gardenlet exchange the credentials in your shoot cluster's kubeconfig. Please note that only the token (and basic auth password, if enabled) are exchanged. The cluster CAs remain the same.\nkubectl -n garden-\u0026lt;project-name\u0026gt; annotate shoot \u0026lt;shoot-name\u0026gt; gardener.cloud/operation=rotate-kubeconfig-credentials\r"
},
{
	"uri": "https://gardener.cloud/tutorials/",
	"title": "Tutorials",
	"tags": [],
	"description": "",
	"content": "Initial Consideration\rThere is a big difference between installing Kubernetes and using Kubernetes as a developer\r\r\r\r\rAdministrator\rThe admin section is for anyone setting up or administering a\rGardener Landscape. It assumes some familiarity with concepts of IaaS\r\r\rDeveloper\rYou don’t have to understand all the internals of Kubernetes; however, basic knowledge of the architecture is helpful for understanding how to deploy and debug your applications. In this section we offer best practices for service and application development on Kubernetes in the context of Gardener.\r\r\r\r\r\r"
},
{
	"uri": "https://gardener.cloud/documentation/050-tutorials/content/howto/helm/",
	"title": "Use a Helm chart to deploy some application or service",
	"tags": [],
	"description": "Use a Helm chart to deploy some application or service",
	"content": "Basically, Helm Charts can be installed as described e.g. in the Helm QuickStart Guide. However, our clusters come with RBAC enabled by default hence Helm must be installed as follows:\nCreate a Service Account Create a service account via the following command:\ncat \u0026lt;\u0026lt;EOF | kubectl create -f -\rapiVersion: v1\rkind: ServiceAccount\rmetadata:\rname: helm\rnamespace: kube-system\r---\rapiVersion: rbac.authorization.k8s.io/v1beta1\rkind: ClusterRoleBinding\rmetadata:\rname: helm\rroleRef:\rapiGroup: rbac.authorization.k8s.io\rkind: ClusterRole\rname: cluster-admin\rsubjects:\r- kind: ServiceAccount\rname: helm\rnamespace: kube-system\rEOF\rInitialize Helm Initialise Helm via helm init --service-account helm. You can now use helm.\nIn case of failure In case you have already executed helm init, but without the above service account, you will get the following error: Error: User \u0026quot;system:serviceaccount:kube-system:default\u0026quot; cannot list configmaps in the namespace \u0026quot;kube-system\u0026quot;. (get configmaps) (e.g. when you run helm list). You will now need to delete the Tiller deployment (Helm backend implicitly deployed to the Kubernetes cluster when you call helm init) as well as the local Helm files (usually $HELM_HOME is set to ~/.helm):\nkubectl delete deployment tiller-deploy --namespace=kube-system\rkubectl delete service tiller-deploy --namespace=kube-system rm -rf ~/.helm/\rNow follow the instructions above. For more details see this Kubernetes Helm issue #2687.\n"
},
{
	"uri": "https://gardener.cloud/documentation/050-tutorials/content/howto/prometheus/",
	"title": "Using Prometheus and Grafana to monitor K8s",
	"tags": [],
	"description": "How to deploy and configure Prometheus and Grafana to collect and monitor kubelet container metrics",
	"content": "Disclaimer This post is meant to give a basic end-to-end description for deploying and using Prometheus and Grafana. Both applications offer a wide range of flexibility which needs to be considered in case you have specific requirenments. Such advanced details are not in the scope of this post.\nIntroduction Prometheus is an open-source systems monitoring and alerting toolkit for recording numeric time series. It fits both machine-centric monitoring as well as monitoring of highly dynamic service-oriented architectures. In a world of microservices, its support for multi-dimensional data collection and querying is a particular strength.\nPrometheus graduates within CNCF second hosted project.\nThe following characteristics make Prometheus a good match for monitoring Kubernetes clusters:\n  Pull-based monitoring\nPrometheus is a pull-based monitoring system, which means that the Prometheus server dynamically discovers and pulls metrics from your services running in Kubernetes.\n  Labels Prometheus and Kubernetes share the same label (key-value) concept that can be used to select objects in the system.\nLabels are used to identify time series and sets of label matchers can be used in the query language ( PromQL ) to select the time series to be aggregated..\n  Exporters\nThere are many exporters available which enable integration of databases or even other monitoring systems not already providing a way to export metrics to Prometheus. One prominent exporter is the so called node-exporter, which allows to monitor hardware and OS related metrics of Unix systems.\n  Powerful query language\nThe Prometheus query language PromQL lets the user select and aggregate time series data in real time. Results can either be shown as a graph, viewed as tabular data in the Prometheus expression browser, or consumed by external systems via the HTTP API.\n  Find query examples on Prometheus Query Examples.\nOne very popular open-source visualization tool not only for Prometheus is Grafana. Grafana is a metric analytics and visualization suite. It is popular for for visualizing time series data for infrastructure and application analytics but many use it in other domains including industrial sensors, home automation, weather, and process control [see Grafana Documentation].\nGrafana accesses data via Data Sources. The continuously growing list of supported backends includes Prometheus.\nDashboards are created by combining panels, e.g. Graph and Dashlist.\nIn this example we describe an End-To-End scenario including the deployment of Prometheus and a basic monitoring configuration as the one provided for Kubernetes clusters created by Gardener.\nIf you miss elements on the Prometheus web page when accessing it via its service URL https://\u0026lt;your K8s FQN\u0026gt;/api/v1/namespaces/\u0026lt;your-prometheus-namespace\u0026gt;/services/prometheus-prometheus-server:80/proxy this is probably caused by Prometheus issue #1583 To workaround this issue setup a port forward kubectl port-forward -n \u0026lt;your-prometheus-namespace\u0026gt; \u0026lt;prometheus-pod\u0026gt; 9090:9090 on your client and access the Prometheus UI from there with your locally installed web browser. This issue is not relevant in case you use the service type LoadBalancer.\nPreparation The deployment of Prometheus and Grafana is based on Helm charts.\nMake sure to implement the Helm settings before deploying the Helm charts.\nThe Kubernetes clusters provided by Gardener use role based access control (RBAC). To authorize the Prometheus node-exporter to access hardware and OS relevant metrics of your cluster's worker nodes specific artifacts need to be deployed.\nBind the prometheus service account to the garden.sapcloud.io:monitoring:prometheus cluster role by running the command kubectl apply -f crbinding.yaml.\nContent of crbinding.yaml\napiVersion: rbac.authorization.k8s.io/v1beta1\rkind: ClusterRoleBinding\rmetadata:\rname: \u0026lt;your-prometheus-name\u0026gt;-server\rroleRef:\rapiGroup: rbac.authorization.k8s.io\rkind: ClusterRole\rname: garden.sapcloud.io:monitoring:prometheus\rsubjects:\r- kind: ServiceAccount\rname: \u0026lt;your-prometheus-name\u0026gt;-server\rnamespace: \u0026lt;your-prometheus-namespace\u0026gt;\rDeployment of Prometheus and Grafana Only minor changes are needed to deploy Prometheus and Grafana based on Helm charts.\nCopy the following configuration into a file called values.yaml and deploy Prometheus: helm install --name \u0026lt;your-prometheus-name\u0026gt; --namespace \u0026lt;your-prometheus-namespace\u0026gt; stable/prometheus -f values.yaml\nTypically, Prometheus and Grafana are deployed into the same namespace. There is no technical reason behind this so feel free to choose different namespaces.\nContent of values.yaml for Prometheus:\nrbac:\rcreate: false # Already created in Preparation step\r nodeExporter:\renabled: false # The node-exporter is already deployed by default\r server:\rglobal:\rscrape_interval: 30s\rscrape_timeout: 30s\rserverFiles:\rprometheus.yml:\rrule_files:\r- /etc/config/rules\r- /etc/config/alerts scrape_configs:\r- job_name: \u0026#39;kube-kubelet\u0026#39;\rhonor_labels: false\rscheme: https\rtls_config:\r# This is needed because the kubelets\u0026#39; certificates are not generated\r # for a specific pod IP\r insecure_skip_verify: true\rbearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\rkubernetes_sd_configs:\r- role: node\rrelabel_configs:\r- target_label: __metrics_path__\rreplacement: /metrics\r- source_labels: [__meta_kubernetes_node_address_InternalIP]\rtarget_label: instance\r- action: labelmap\rregex: __meta_kubernetes_node_label_(.+)\r- job_name: \u0026#39;kube-kubelet-cadvisor\u0026#39;\rhonor_labels: false\rscheme: https\rtls_config:\r# This is needed because the kubelets\u0026#39; certificates are not generated\r # for a specific pod IP\r insecure_skip_verify: true\rbearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\rkubernetes_sd_configs:\r- role: node\rrelabel_configs:\r- target_label: __metrics_path__\rreplacement: /metrics/cadvisor\r- source_labels: [__meta_kubernetes_node_address_InternalIP]\rtarget_label: instance\r- action: labelmap\rregex: __meta_kubernetes_node_label_(.+)\r# Example scrape config for probing services via the Blackbox Exporter.\r #\r # Relabelling allows to configure the actual service scrape endpoint using the following annotations:\r #\r # * `prometheus.io/probe`: Only probe services that have a value of `true`\r - job_name: \u0026#39;kubernetes-services\u0026#39;\rmetrics_path: /probe\rparams:\rmodule: [http_2xx]\rkubernetes_sd_configs:\r- role: service\rrelabel_configs:\r- source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]\raction: keep\rregex: true\r- source_labels: [__address__]\rtarget_label: __param_target\r- target_label: __address__\rreplacement: blackbox\r- source_labels: [__param_target]\rtarget_label: instance\r- action: labelmap\rregex: __meta_kubernetes_service_label_(.+)\r- source_labels: [__meta_kubernetes_namespace]\rtarget_label: kubernetes_namespace\r- source_labels: [__meta_kubernetes_service_name]\rtarget_label: kubernetes_name\r# Example scrape config for pods\r #\r # Relabelling allows to configure the actual service scrape endpoint using the following annotations:\r #\r # * `prometheus.io/scrape`: Only scrape pods that have a value of `true`\r # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.\r # * `prometheus.io/port`: Scrape the pod on the indicated port instead of the default of `9102`.\r - job_name: \u0026#39;kubernetes-pods\u0026#39;\rkubernetes_sd_configs:\r- role: pod\rrelabel_configs:\r- source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]\raction: keep\rregex: true\r- source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]\raction: replace\rtarget_label: __metrics_path__\rregex: (.+)\r- source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]\raction: replace\rregex: (.+):(?:\\d+);(\\d+)\rreplacement: ${1}:${2}\rtarget_label: __address__\r- action: labelmap\rregex: __meta_kubernetes_pod_label_(.+)\r- source_labels: [__meta_kubernetes_namespace]\raction: replace\rtarget_label: kubernetes_namespace\r- source_labels: [__meta_kubernetes_pod_name]\raction: replace\rtarget_label: kubernetes_pod_name\r# Scrape config for service endpoints.\r #\r # The relabeling allows the actual service scrape endpoint to be configured\r # via the following annotations:\r #\r # * `prometheus.io/scrape`: Only scrape services that have a value of `true`\r # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need\r # to set this to `https` \u0026amp; most likely set the `tls_config` of the scrape config.\r # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.\r # * `prometheus.io/port`: If the metrics are exposed on a different port to the\r # service then set this appropriately.\r - job_name: \u0026#39;kubernetes-service-endpoints\u0026#39;\rkubernetes_sd_configs:\r- role: endpoints\rrelabel_configs:\r- source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]\raction: keep\rregex: true\r- source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]\raction: replace\rtarget_label: __scheme__\rregex: (https?)\r- source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]\raction: replace\rtarget_label: __metrics_path__\rregex: (.+)\r- source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]\raction: replace\rtarget_label: __address__\rregex: (.+)(?::\\d+);(\\d+)\rreplacement: $1:$2\r- action: labelmap\rregex: __meta_kubernetes_service_label_(.+)\r- source_labels: [__meta_kubernetes_namespace]\raction: replace\rtarget_label: kubernetes_namespace\r- source_labels: [__meta_kubernetes_service_name]\raction: replace\rtarget_label: kubernetes_name # Add your additional configuration here...\r Next, deploy Grafana. Since the deployment in this post is based on the Helm default values, the settings below are set explicitly in case the default changed. Deploy Grafana via helm install --name grafana --namespace \u0026lt;your-prometheus-namespace\u0026gt; stable/grafana -f values.yaml. Here, the same namespace is chosen for Prometheus and for Grafana.\nContent of values.yaml for Grafana:\nserver:\ringress:\renabled: false\rservice:\rtype: ClusterIP\rCheck the running state of the pods on the Kubernetes Dashboard or by running kubectl get pods -n \u0026lt;your-prometheus-namespace\u0026gt;. In case of errors check the log files of the pod(s) in question.\nThe text output of Helm after the deployment of Prometheus and Grafana contains very useful information, e.g. the user and password of the Grafana Admin user. The credentials are stored as secrets in the namespace \u0026lt;your-prometheus-namespace\u0026gt; and could be decoded via kubectl get secret --namespace \u0026lt;my-grafana-namespace\u0026gt; grafana -o jsonpath=\u0026quot;{.data.admin-password}\u0026quot; | base64 --decode ; echo.\nBasic functional tests To access the web UI of both applications use port forwarding of port 9090.\nSetup port forwarding for port 9090:\nkubectl port-forward -n \u0026lt;your-prometheus-namespace\u0026gt; \u0026lt;your-prometheus-server-pod\u0026gt; 9090:9090\rOpen http://localhost:9090 in your web browser. Select Graph from the top tab and enter the following expressing to show the overall CPU usage for a server (see Prometheus Query Examples)\n100 * (1 - avg by(instance)(irate(node_cpu{mode='idle'}[5m])))\rThis should show some data in a graph.\nTo show the same data in Grafana setup port forwarding for port 3000 for the Grafana pod and open the Grafana Web UI by opening http://localhost:3000 in a browser. Enter the credentials of the admin user.\nNext, you need to enter the server name of your Prometheus deployment. This name is shown directly after the installation via helm.\nRun\nhelm status \u0026lt;your-prometheus-name\u0026gt;\rto find this name. Below this server name is referenced by \u0026lt;your-prometheus-server-name\u0026gt;.\nFirst, you need to add your Prometheus server as data source.\n select Dashboards → Data Sources select Add data source enter Name: \u0026lt;your-prometheus-datasource-name\u0026gt;\nType: Prometheus\nURL: http://\u0026lt;your-prometheus-server-name\u0026gt;\n_Access: proxy select Save \u0026amp; Test  In case of failure check the Prometheus URL in the Kubernetes Dashboard.\nTo add a Graph follow these steps:\n in the left corner, select Dashboards → New to create a new dashboard select Graph to create a new graph next, select the Panel Title → Edit select your Prometheus Data Source in the drop down list enter the expression 100 * (1 - avg by(instance)(irate(node_cpu{mode='idle'}[5m]))) in the entry field A select the floppy disk symbol (Save) on top  Now you should have a very basic Prometheus and Grafana setup for your Kubernetes cluster.\nAs a next step you can implement monitoring for your applications by implementing the Prometheus client API.\nLinks  Prometheus Prometheus Helm Chart Prometheus and Kubernetes: A Perfect Match Grafana Grafana Helm Chart  "
},
{
	"uri": "https://gardener.cloud/blog/2018_week_08/",
	"title": "Watching logs of several pods",
	"tags": [],
	"description": "",
	"content": "One thing that always bothered me was that I couldn't get logs of several pods at once with kubectl. A simple tail -f \u0026lt;path-to-logfile\u0026gt; isn't possible. Certainly you can use kubectl logs -f \u0026lt;pod-id\u0026gt;, but it doesn't help if you want to monitor more than one pod at a time.\nThis is something you really need a lot, at least if you run several instances of a pod behind a deployment and you don't have setup a log viewer service like Kibana.\n\u0026lt;img src=\u0026quot;../blog/2018_week_08/blog-kubetail.png\u0026quot; title=\u0026quot;logo\u0026quot; width=\u0026quot;100%\u0026quot; class=\u0026quot;drop-shadow reveal-fast\u0026quot; style=\u0026quot;\u0026quot;/\u0026gt;\r kubetail comes to the rescue, it is a small bash script that allows you to aggregate log files of several pods at the same time in a simple way. The script is called kubetail and is available at https://github.com/johanhaleby/kubetail.\n"
}]